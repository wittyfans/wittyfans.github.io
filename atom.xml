<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Scott Zhang</title>
  
  <subtitle>学则不固,知则不惑</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wittyfans.com/"/>
  <updated>2021-03-02T13:28:51.138Z</updated>
  <id>http://wittyfans.com/</id>
  
  <author>
    <name>Scott Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark 笔记</title>
    <link href="http://wittyfans.com/coding/Spark-%E7%AC%94%E8%AE%B0.html"/>
    <id>http://wittyfans.com/coding/Spark-笔记.html</id>
    <published>2021-03-02T13:27:00.000Z</published>
    <updated>2021-03-02T13:28:51.138Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>A platform for cluster computing.</p></blockquote><a id="more"></a><p>Spark lets you spread data and computations over <em>clusters</em> with multiple <em>nodes</em> (think of each node as a separate computer). Spark is a platform for cluster computing. Spark lets you spread data and computations over <em>clusters</em> with multiple <em>nodes</em> (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.</p><p>As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed <em>in parallel</em> over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.</p><h1 id="连接集群"><a href="#连接集群" class="headerlink" title="连接集群"></a>连接集群</h1><p>The first step in using Spark is connecting to a cluster.</p><p>In practice, the cluster will be hosted on a remote machine that’s connected to all other nodes. There will be one computer, called the <em>master</em> that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called <em>worker</em>. The master sends the workers data and calculations to run, and they send their results back to the master.</p><p>Creating the connection is as simple as creating an instance of the <code>SparkContext</code> class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you’re connecting to.</p><p>An object holding all these attributes can be created with the <code>SparkConf()</code> constructor. Take a look at the <a href="http://spark.apache.org/docs/2.1.0/api/python/pyspark.html" target="_blank" rel="noopener">documentation</a> for all the details!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建连接</span></span><br><span class="line">SparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 sc 对象</span></span><br><span class="line">print(sc)</span><br><span class="line">&lt;SparkContext master=local[*] appName=pyspark-shell&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">print(sc.version)</span><br></pre></td></tr></table></figure><h1 id="Spark-中的数据"><a href="#Spark-中的数据" class="headerlink" title="Spark 中的数据"></a>Spark 中的数据</h1><p>Spark’s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly</p><p>The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.</p><p>When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it’s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!</p><p>To start working with Spark DataFrames, you first have to create a <code>SparkSession</code> object from your <code>SparkContext</code>. You can think of the <code>SparkContext</code> as your connection to the cluster and the <code>SparkSession</code> as your interface with that connection.</p><p>we will have a <code>SparkSession</code> called <code>spark</code> .</p><p>Creating multiple <code>SparkSession</code>s and <code>SparkContext</code>s can cause issues, so it’s best practice to use the <code>SparkSession.builder.getOrCreate()</code> method. This returns an existing <code>SparkSession</code> if there’s already one in the environment, or creates a new one if necessary!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import SparkSession from pyspark.sql</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create my_spark</span></span><br><span class="line">my_spark = SparkSession.builder.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print my_spark</span></span><br><span class="line">print(my_spark)</span><br></pre></td></tr></table></figure><p>Once you’ve created a <code>SparkSession</code>, you can start poking around to see what data is in your cluster!</p><p>Your <code>SparkSession</code> has an attribute called <code>catalog</code> which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.</p><p>One of the most useful is the <code>.listTables()</code> method, which returns the names of all the tables in your cluster as a list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the tables in the catalog</span></span><br><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running a query on this table</span></span><br><span class="line">query = <span class="string">"FROM flights SELECT * LIMIT 10"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the first 10 rows of flights</span></span><br><span class="line">flights10 = spark.sql(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the results</span></span><br><span class="line">flights10.show()</span><br></pre></td></tr></table></figure><h2 id="读取-Spark-数据到-Pandas"><a href="#读取-Spark-数据到-Pandas" class="headerlink" title="读取 Spark 数据到 Pandas"></a>读取 Spark 数据到 Pandas</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Don't change this query</span></span><br><span class="line">query = <span class="string">"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the query</span></span><br><span class="line">flight_counts = spark.sql(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the results to a pandas DataFrame</span></span><br><span class="line">pd_counts = flight_counts.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the head of pd_counts</span></span><br><span class="line">print(pd_counts.head())</span><br></pre></td></tr></table></figure><h2 id="Pandas-Data-Frame-写入到-Spark"><a href="#Pandas-Data-Frame-写入到-Spark" class="headerlink" title="Pandas Data Frame 写入到 Spark"></a>Pandas Data Frame 写入到 Spark</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create pd_temp</span></span><br><span class="line">pd_temp = pd.DataFrame(np.random.random(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create spark_temp from pd_temp</span></span><br><span class="line">spark_temp = spark.createDataFrame(pd_temp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Examine the tables in the catalog</span></span><br><span class="line">print(spark.catalog.listTables())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add spark_temp to the catalog</span></span><br><span class="line">spark_temp.createOrReplaceTempView(<span class="string">"temp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Examine the tables in the catalog again</span></span><br><span class="line">print(spark.catalog.listTables())</span><br></pre></td></tr></table></figure><p>The <code>.createDataFrame()</code> method takes a <code>pandas</code> DataFrame and returns a Spark DataFrame.</p><p>The output of this method is stored locally, not in the <code>SparkSession</code> catalog. This means that you can use all the Spark DataFrame methods on it, but you can’t access the data in other contexts.</p><p>For example, a SQL query (using the <code>.sql()</code> method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a <em>temporary table</em>.</p><p>You can do this using the <code>.createTempView()</code> Spark DataFrame method, which takes as its only argument the name of the temporary table you’d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific <code>SparkSession</code> used to create the Spark DataFrame.</p><p>There is also the method <code>.createOrReplaceTempView()</code>. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You’ll use this method to avoid running into problems with duplicate tables.</p><p>Check out the diagram to see all the different ways your Spark data structures interact with each other.</p><p><img src="https://i.loli.net/2021/03/01/OsewxPRf3JFbGd4.png" alt="image.png"></p><h2 id="Spark-读取文本文件"><a href="#Spark-读取文本文件" class="headerlink" title="Spark 读取文本文件"></a>Spark 读取文本文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Don't change this file path</span></span><br><span class="line">file_path = <span class="string">"/usr/local/share/datasets/airports.csv"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read in the airports data</span></span><br><span class="line">airports = spark.read.csv(file_path,header=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the data</span></span><br><span class="line">airports.show()</span><br></pre></td></tr></table></figure><h1 id="操作数据"><a href="#操作数据" class="headerlink" title="操作数据"></a>操作数据</h1><p>In Spark you can do this using the <code>.withColumn()</code> method, which takes two arguments. First, a string with the name of your new column, and second the new column itself.</p><p>The new column must be an object of class <code>Column</code>. Creating one of these is as easy as extracting a column from your DataFrame using <code>df.colName</code>.</p><p>Updating a Spark DataFrame is somewhat different than working in <code>pandas</code> because the Spark DataFrame is <em>immutable</em>. This means that it can’t be changed, and so columns can’t be updated in place.</p><p>Thus, all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.withColumn(<span class="string">"newCol"</span>, df.oldCol + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>The above code creates a DataFrame with the same columns as <code>df</code> plus a new column, <code>newCol</code>, where every entry is equal to the corresponding entry from <code>oldCol</code>, plus one.</p><p>To overwrite an existing column, just pass the name of the column as the first argument!</p><h2 id="Filter-Data"><a href="#Filter-Data" class="headerlink" title="Filter Data"></a>Filter Data</h2><p>The <code>.filter()</code> method takes either an expression that would follow the <code>WHERE</code> clause of a SQL expression as a string, or a Spark Column of boolean (<code>True</code>/<code>False</code>) values.</p><p>For example, the following two expressions will produce the same output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flights.filter(<span class="string">"air_time &gt; 120"</span>).show()</span><br><span class="line">flights.filter(flights.air_time &gt; <span class="number">120</span>).show()</span><br></pre></td></tr></table></figure><h2 id="Select-Data"><a href="#Select-Data" class="headerlink" title="Select Data"></a>Select Data</h2><p>The Spark variant of SQL’s <code>SELECT</code> is the <code>.select()</code> method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the <code>df.colName</code> syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside <code>.withColumn()</code>.</p><p>The difference between <code>.select()</code> and <code>.withColumn()</code> methods is that <code>.select()</code> returns only the columns you specify, while <code>.withColumn()</code> returns all the columns of the DataFrame in addition to the one you defined. It’s often a good idea to drop columns you don’t need at the beginning of an operation so that you’re not dragging around extra data as you’re wrangling. In this case, you would use <code>.select()</code> and not <code>.withColumn()</code>.</p><p>Similar to SQL, you can also use the <code>.select()</code> method to perform column-wise operations. When you’re selecting a column using the <code>df.colName</code> notation, you can perform any column operation and the <code>.select()</code> method will return the transformed column. For example,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flights.select(flights.air_time/<span class="number">60</span>)</span><br></pre></td></tr></table></figure><p>returns a column of flight durations in hours instead of minutes. You can also use the <code>.alias()</code> method to rename a column you’re selecting. So if you wanted to <code>.select()</code> the column <code>duration_hrs</code> (which isn’t in your DataFrame) you could do</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flights.select((flights.air_time/<span class="number">60</span>).alias(<span class="string">"duration_hrs"</span>))</span><br></pre></td></tr></table></figure><p>with the SQL <code>as</code> keyword being equivalent to the <code>.alias()</code> method. To select multiple columns, you can pass multiple strings.</p><h2 id="Aggregating"><a href="#Aggregating" class="headerlink" title="Aggregating"></a>Aggregating</h2><p>All of the common aggregation methods, like <code>.min()</code>, <code>.max()</code>, and <code>.count()</code> are <code>GroupedData</code> methods. These are created by calling the <code>.groupBy()</code> DataFrame method. You’ll learn exactly what that means in a few exercises. For now, all you have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, <code>col</code>, in a DataFrame, <code>df</code>, you could do</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy().min(<span class="string">"col"</span>).show()</span><br></pre></td></tr></table></figure><p>This creates a <code>GroupedData</code> object (so you can use the <code>.min()</code> method), then finds the minimum value in <code>col</code>, and returns it as a DataFrame.</p><h3 id="Grouping-and-Aggregating-I"><a href="#Grouping-and-Aggregating-I" class="headerlink" title="Grouping and Aggregating I"></a>Grouping and Aggregating I</h3><p>Part of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: <code>pyspark.sql.GroupedData</code>, which you saw in the last two exercises.</p><p>You’ve learned how to create a grouped DataFrame by calling the <code>.groupBy()</code> method on a DataFrame with no arguments.</p><p>Now you’ll see that when you pass the name of one or more columns in your DataFrame to the <code>.groupBy()</code> method, the aggregation methods behave like when you use a <code>GROUP BY</code> statement in a SQL query!</p><h3 id="Grouping-and-Aggregating-II"><a href="#Grouping-and-Aggregating-II" class="headerlink" title="Grouping and Aggregating II"></a>Grouping and Aggregating II</h3><p>In addition to the <code>GroupedData</code> methods you’ve already seen, there is also the <code>.agg()</code> method. This method lets you pass an aggregate column expression that uses any of the aggregate functions from the <code>pyspark.sql.functions</code> submodule.</p><p>This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a <code>GroupedData</code> table.</p><h2 id="Joining"><a href="#Joining" class="headerlink" title="Joining"></a>Joining</h2><p>Another very common data operation is the <em>join</em>. Joins are a whole topic unto themselves, so in this course we’ll just look at simple joins. If you’d like to learn more about joins, you can take a look <a href="https://www.datacamp.com/courses/merging-dataframes-with-pandas" target="_blank" rel="noopener">here</a>.</p><p>A join will combine two different tables along a column that they share. This column is called the <em>key</em>. Examples of keys here include the <code>tailnum</code> and <code>carrier</code> columns from the <code>flights</code> table.</p><p>For example, suppose that you want to know more information about the plane that flew a flight than just the tail number. This information isn’t in the <code>flights</code> table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, you’d have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table <code>planes</code></p><p>When you join the <code>flights</code> table to this table of airplane information, you’re adding all the columns from the <code>planes</code> table to the <code>flights</code> table. To fill these columns with information, you’ll look at the tail number from the <code>flights</code> table and find the matching one in the <code>planes</code> table, and then use that row to fill out all the new columns.</p><h3 id="Joining-II"><a href="#Joining-II" class="headerlink" title="Joining II"></a>Joining II</h3><p>In PySpark, joins are performed using the DataFrame method <code>.join()</code>. This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, <code>on</code>, is the name of the key column(s) as a string. The names of the key column(s) must be the same in each table. The third argument, <code>how</code>, specifies the kind of join to perform. In this course we’ll always use the value <code>how=&quot;leftouter&quot;</code>.</p><h1 id="machine-learning-pipelines"><a href="#machine-learning-pipelines" class="headerlink" title="machine learning pipelines"></a>machine learning pipelines</h1><p>In the next two chapters you’ll step through every stage of the machine learning pipeline, from data intake to model evaluation. Let’s get to it!</p><p>At the core of the <code>pyspark.ml</code> module are the <code>Transformer</code> and <code>Estimator</code> classes. Almost every other class in the module behaves similarly to these two basic classes.</p><p><code>Transformer</code> classes have a <code>.transform()</code> method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class <code>Bucketizer</code> to create discrete bins from a continuous feature or the class <code>PCA</code> to reduce the dimensionality of your dataset using principal component analysis.</p><p><code>Estimator</code> classes all implement a <code>.fit()</code> method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a <code>StringIndexerModel</code> for including categorical data saved as strings in your models, or a <code>RandomForestModel</code> that uses the random forest algorithm for classification or regression.</p><h2 id="Join-the-DataFrames"><a href="#Join-the-DataFrames" class="headerlink" title="Join the DataFrames"></a>Join the DataFrames</h2><p>In the next two chapters you’ll be working to build a model that predicts whether or not a flight will be delayed based on the flights data we’ve been working with. This model will also include information about the plane that flew the route, so the first step is to join the two tables: <code>flights</code> and <code>planes</code>!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Rename year column</span></span><br><span class="line">planes = planes.withColumnRenamed(<span class="string">'year'</span>,<span class="string">'plane_year'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the DataFrames</span></span><br><span class="line">model_data = flights.join(planes, on=<span class="string">'tailnum'</span>, how=<span class="string">"leftouter"</span>)</span><br></pre></td></tr></table></figure><h2 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h2><p>Good work! Before you get started modeling, <strong>it’s important to know that Spark only handles numeric data</strong>. That means all of the columns in your DataFrame must be either integers or decimals (called ‘doubles’ in Spark).</p><p>When we imported our data, we let Spark guess what kind of information each column held. Unfortunately, Spark doesn’t always guess right and you can see that some of the columns in our DataFrame are strings containing numbers as opposed to actual numeric values.</p><p>To remedy this, you can use the <code>.cast()</code> method in combination with the <code>.withColumn()</code> method. It’s important to note that <code>.cast()</code> works on columns, while <code>.withColumn()</code> works on DataFrames.</p><p>The only argument you need to pass to <code>.cast()</code> is the kind of value you want to create, in string form. For example, to create integers, you’ll pass the argument <code>&quot;integer&quot;</code> and for decimal numbers you’ll use <code>&quot;double&quot;</code>.</p><p>You can put this call to <code>.cast()</code> inside a call to <code>.withColumn()</code> to overwrite the already existing column, just like you did in the previous chapter!</p><h3 id="String-to-integer"><a href="#String-to-integer" class="headerlink" title="String to integer"></a>String to integer</h3><p>Now you’ll use the <code>.cast()</code> method you learned in the previous exercise to convert all the appropriate columns from your DataFrame <code>model_data</code> to integers!</p><p>To convert the type of a column using the <code>.cast()</code> method, you can write code like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cast the columns to integers</span></span><br><span class="line">model_data = model_data.withColumn(<span class="string">"arr_delay"</span>, model_data.arr_delay.cast(<span class="string">'integer'</span>))</span><br><span class="line">model_data = model_data.withColumn(<span class="string">"air_time"</span>, model_data.air_time.cast(<span class="string">'integer'</span>))</span><br><span class="line">model_data = model_data.withColumn(<span class="string">"month"</span>, model_data.month.cast(<span class="string">'integer'</span>))</span><br><span class="line">model_data = model_data.withColumn(<span class="string">"plane_year"</span>, model_data.plane_year.cast(<span class="string">'integer'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the column plane_age</span></span><br><span class="line">model_data = model_data.withColumn(<span class="string">"plane_age"</span>, model_data.year - model_data.plane_year)</span><br></pre></td></tr></table></figure><h3 id="Making-a-Boolean"><a href="#Making-a-Boolean" class="headerlink" title="Making a Boolean"></a>Making a Boolean</h3><p>Consider that you’re modeling a yes or no question: is the flight late? However, your data contains the arrival delay in minutes for each flight. Thus, you’ll need to create a boolean column which indicates whether the flight was late or not!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create is_late</span></span><br><span class="line">model_data = model_data.withColumn(<span class="string">"is_late"</span>, model_data.arr_delay &gt; <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert to an integer</span></span><br><span class="line">model_data = model_data.withColumn(<span class="string">"label"</span>, model_data.is_late.cast(<span class="string">'integer'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove missing values</span></span><br><span class="line">model_data = model_data.filter(<span class="string">"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL"</span>)</span><br></pre></td></tr></table></figure><h3 id="Strings-and-factors"><a href="#Strings-and-factors" class="headerlink" title="Strings and factors"></a>Strings and factors</h3><p>As you know, Spark requires numeric data for modeling. So far this hasn’t been an issue; even boolean columns can easily be converted to integers without any trouble. But you’ll also be using the airline and the plane’s destination as features in your model. These are coded as strings and there isn’t any obvious way to convert them to a numeric data type.</p><p>Fortunately, PySpark has functions for handling this built into the <code>pyspark.ml.features</code> submodule. You can create what are called ‘one-hot vectors’ to represent the carrier and the destination of each flight. A <em>one-hot vector</em> is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1).</p><p>Each element in the vector corresponds to a level of the feature, so it’s possible to tell what the right level is by seeing which element of the vector is equal to one (1).</p><p>The first step to encoding your categorical feature is to create a <code>StringIndexer</code>. Members of this class are <code>Estimator</code>s that take a DataFrame with a column of strings and map each unique string to a number. Then, the <code>Estimator</code> returns a <code>Transformer</code> that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.</p><p>The second step is to encode this numeric column as a one-hot vector using a <code>OneHotEncoder</code>. This works exactly the same way as the <code>StringIndexer</code> by creating an <code>Estimator</code> and then a <code>Transformer</code>. The end result is a column that encodes your categorical feature as a vector that’s suitable for machine learning routines!</p><p>This may seem complicated, but don’t worry! All you have to remember is that you need to create a <code>StringIndexer</code> and a <code>OneHotEncoder</code>, and the <code>Pipeline</code> will take care of the rest.</p><h3 id="Carrier-amp-Destination"><a href="#Carrier-amp-Destination" class="headerlink" title="Carrier &amp; Destination"></a>Carrier &amp; Destination</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a StringIndexer</span></span><br><span class="line">carr_indexer = StringIndexer(inputCol=<span class="string">"carrier"</span>,outputCol=<span class="string">"carrier_index"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a OneHotEncoder</span></span><br><span class="line">carr_encoder = OneHotEncoder(inputCol=<span class="string">"carrier_index"</span>,outputCol=<span class="string">"carrier_fact"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a StringIndexer</span></span><br><span class="line">dest_indexer = StringIndexer(inputCol=<span class="string">'dest'</span>,outputCol=<span class="string">'dest_index'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a OneHotEncoder</span></span><br><span class="line">dest_encoder = OneHotEncoder(inputCol=<span class="string">'dest_index'</span>,outputCol=<span class="string">'dest_fact'</span>)</span><br></pre></td></tr></table></figure><h3 id="Assemble-a-vector"><a href="#Assemble-a-vector" class="headerlink" title="Assemble a vector"></a>Assemble a vector</h3><p>The last step in the <code>Pipeline</code> is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model’s point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.</p><p>Because of this, the <code>pyspark.ml.feature</code> submodule contains a class called <code>VectorAssembler</code>. This <code>Transformer</code> takes all of the columns you specify and combines them into a new vector column.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make a VectorAssembler</span></span><br><span class="line">vec_assembler = VectorAssembler(inputCols=[<span class="string">"month"</span>, <span class="string">"air_time"</span>, <span class="string">"carrier_fact"</span>, <span class="string">"dest_fact"</span>, <span class="string">"plane_age"</span>], outputCol=<span class="string">"features"</span>)</span><br></pre></td></tr></table></figure><h3 id="Create-the-pipeline"><a href="#Create-the-pipeline" class="headerlink" title="Create the pipeline"></a>Create the pipeline</h3><p>ou’re finally ready to create a <code>Pipeline</code>!</p><p><code>Pipeline</code> is a class in the <code>pyspark.ml</code> module that combines all the <code>Estimators</code> and <code>Transformers</code> that you’ve already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object. Neat, right?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import Pipeline</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make the pipeline</span></span><br><span class="line">flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])</span><br></pre></td></tr></table></figure><h3 id="Test-and-Train"><a href="#Test-and-Train" class="headerlink" title="Test and Train"></a>Test and Train</h3><p>After you’ve cleaned your data and gotten it ready for modeling, one of the most important steps is to split the data into a <em>test set</em> and a <em>train set</em>. After that, don’t touch your test data until you think you have a good model! As you’re building models and forming hypotheses, you can test them on your training data to get an idea of their performance.</p><p>Once you’ve got your favorite model, you can see how well it predicts the new data in your test set. This never-before-seen data will give you a much more realistic idea of your model’s performance in the real world when you’re trying to predict or classify new data.</p><p>In Spark it’s important to make sure you split the data <strong>after</strong> all the transformations. This is because operations like <code>StringIndexer</code> don’t always produce the same index even when given the same list of strings.</p><h3 id="Transform-the-data"><a href="#Transform-the-data" class="headerlink" title="Transform the data"></a>Transform the data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit and transform the data</span></span><br><span class="line">piped_data = flights_pipe.fit(model_data).transform(model_data)</span><br></pre></td></tr></table></figure><h3 id="Split-the-data"><a href="#Split-the-data" class="headerlink" title="Split the data"></a>Split the data</h3><p>Now that you’ve done all your manipulations, the last step before modeling is to split the data!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Split the data into training and test sets</span></span><br><span class="line">training, test = piped_data.randomSplit([<span class="number">.6</span>, <span class="number">.4</span>])</span><br></pre></td></tr></table></figure><h1 id="Model-tuning-and-selection"><a href="#Model-tuning-and-selection" class="headerlink" title="Model tuning and selection"></a>Model tuning and selection</h1><h2 id="What-is-logistic-regression"><a href="#What-is-logistic-regression" class="headerlink" title="What is logistic regression?"></a>What is logistic regression?</h2><p>The model you’ll be fitting in this chapter is called a <em>logistic regression</em>. This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.</p><p>To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a ‘yes’ (in this case, the flight being late), if it’s below, you classify it as a ‘no’!</p><p>You’ll tune this model by testing different values for several <em>hyperparameters</em>. A <em>hyperparameter</em> is just a value in the model that’s not estimated from the data, but rather is supplied by the user to maximize performance. For this course it’s not necessary to understand the mathematics behind all of these values - what’s important is that you’ll try out a few different choices and pick the best one.</p><h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>The <code>Estimator</code> you’ll be using is a <code>LogisticRegression</code> from the <code>pyspark.ml.classification</code> submodule.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import LogisticRegression</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a LogisticRegression Estimator</span></span><br><span class="line">lr = LogisticRegression()</span><br></pre></td></tr></table></figure><h3 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h3><p>In the next few exercises you’ll be tuning your logistic regression model using a procedure called <em>k-fold cross validation</em>. This is a method of estimating the model’s performance on unseen data (like your <code>test</code> DataFrame).</p><p>It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you’ll be using PySpark’s default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the <em>cross validation error</em> of the model, and is a good estimate of the actual error on the held out data.</p><p>You’ll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, <code>elasticNetParam</code> and <code>regParam</code>, and using the cross validation error to compare all the different models so you can choose the best one!</p><h3 id="Create-the-evaluator"><a href="#Create-the-evaluator" class="headerlink" title="Create the evaluator"></a>Create the evaluator</h3><p>The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the <code>pyspark.ml.evaluation</code> submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you’ll be using the <code>BinaryClassificationEvaluator</code> from the <code>pyspark.ml.evaluation</code> module.</p><p>This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. You’ll learn more about this towards the end of the chapter!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the evaluation submodule</span></span><br><span class="line"><span class="keyword">import</span> pyspark.ml.evaluation <span class="keyword">as</span> evals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a BinaryClassificationEvaluator</span></span><br><span class="line">evaluator = evals.BinaryClassificationEvaluator(metricName=<span class="string">"areaUnderROC"</span>)</span><br></pre></td></tr></table></figure><h3 id="Make-a-grid"><a href="#Make-a-grid" class="headerlink" title="Make a grid"></a>Make a grid</h3><p>Next, you need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule <code>pyspark.ml.tuning</code> includes a class called <code>ParamGridBuilder</code> that does just that (maybe you’re starting to notice a pattern here; PySpark has a submodule for just about everything!).</p><p>You’ll need to use the <code>.addGrid()</code> and <code>.build()</code> methods to create a grid that you can use for cross validation. The <code>.addGrid()</code> method takes a model parameter (an attribute of the model <code>Estimator</code>, <code>lr</code>, that you created a few exercises ago) and a list of values that you want to try. The <code>.build()</code> method takes no arguments, it just returns the grid that you’ll use later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the tuning submodule</span></span><br><span class="line"><span class="keyword">import</span> pyspark.ml.tuning <span class="keyword">as</span> tune</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the parameter grid</span></span><br><span class="line">grid = tune.ParamGridBuilder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the hyperparameter</span></span><br><span class="line">grid = grid.addGrid(lr.regParam, np.arange(<span class="number">0</span>, <span class="number">.1</span>, <span class="number">.01</span>))</span><br><span class="line">grid = grid.addGrid(lr.elasticNetParam, [<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the grid</span></span><br><span class="line">grid = grid.build()</span><br></pre></td></tr></table></figure><h3 id="Make-the-validator"><a href="#Make-the-validator" class="headerlink" title="Make the validator"></a>Make the validator</h3><p>The submodule <code>pyspark.ml.tuning</code> also has a class called <code>CrossValidator</code> for performing cross validation. This <code>Estimator</code> takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models.</p><p>The submodule <code>pyspark.ml.tune</code> has already been imported as <code>tune</code>. You’ll create the <code>CrossValidator</code> by passing it the logistic regression <code>Estimator</code> <code>lr</code>, the parameter <code>grid</code>, and the <code>evaluator</code> you created in the previous exercises.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the CrossValidator</span></span><br><span class="line">cv = tune.CrossValidator(estimator=lr,</span><br><span class="line">               estimatorParamMaps=grid,</span><br><span class="line">               evaluator=evaluator</span><br><span class="line">               )</span><br></pre></td></tr></table></figure><h3 id="Fit-the-model-s"><a href="#Fit-the-model-s" class="headerlink" title="Fit the model(s)"></a>Fit the model(s)</h3><p>You’re finally ready to fit the models and select the best one!</p><p>Unfortunately, cross validation is a very computationally intensive procedure. Fitting all the models would take too long on DataCamp.</p><p>To do this locally you would use the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit cross validation models</span></span><br><span class="line">models = cv.fit(training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract the best model</span></span><br><span class="line">best_lr = models.bestModel</span><br></pre></td></tr></table></figure><p>Remember, the training data is called <code>training</code> and you’re using <code>lr</code> to fit a logistic regression model. Cross validation selected the parameter values <code>regParam=0</code> and <code>elasticNetParam=0</code> as being the best. These are the default values, so you don’t need to do anything else with <code>lr</code> before fitting the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Call lr.fit()</span></span><br><span class="line">best_lr = lr.fit(training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print best_lr</span></span><br><span class="line">print(best_lr)</span><br></pre></td></tr></table></figure><h2 id="Evaluate-the-model"><a href="#Evaluate-the-model" class="headerlink" title="Evaluate the model"></a>Evaluate the model</h2><p>For this course we’ll be using a common metric for binary classification algorithms call the <em>AUC</em>, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. The details of what these things actually measure isn’t important for this course. All you need to know is that for our purposes, the closer the AUC is to one (1), the better the model is!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the model to predict the test set</span></span><br><span class="line">test_results = best_lr.transform(test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the predictions</span></span><br><span class="line">print(evaluator.evaluate(test_results))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;A platform for cluster computing.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="spark" scheme="http://wittyfans.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Bash for data manipulation</title>
    <link href="http://wittyfans.com/coding/Bash-for-data-manipulation.html"/>
    <id>http://wittyfans.com/coding/Bash-for-data-manipulation.html</id>
    <published>2021-02-25T15:20:39.000Z</published>
    <updated>2021-02-25T15:40:09.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Bash is a Unix shell and command language, it survived and thrived for almost 50 years because it lets people do complex things with just a few keystrokes. Sometimes called “the universal glue of programming,” it helps users combine existing programs in new ways, automate repetitive tasks, and run programs on clusters and clouds that may be halfway around the world. </p></blockquote><a id="more"></a><h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><blockquote><p>How to move around in the shell, and how to create, modify, and delete files and folders.</p></blockquote><ul><li><code>pwd</code> print working directory</li><li><code>ls</code>: listing files or directories</li><li><code>cd</code>: change directory</li><li><code>cp</code>: copy</li><li><code>mv</code>: move or rename</li><li><code>rm</code>: remove</li><li><code>$1</code> or <code>$2</code> in bash script file, receive ARGV in bash scripts. <code>$@</code> , <code>$*</code> means get ARGV list, <code>$#</code> means get ARGV length.</li></ul><h1 id="Find-files"><a href="#Find-files" class="headerlink" title="Find files"></a>Find files</h1><h2 id="by-name"><a href="#by-name" class="headerlink" title="by name"></a>by name</h2><ul><li>find ., list all file and folder below current</li><li>find folder</li><li>find . -type d, find all foldre, no file</li><li>find . -type f, find all file , no foldre</li><li>find . -type f -name “test.txt”, name as text txt file</li><li>find . -type f -name “text*”, name as txt all file</li><li>find . -type f -iname “text*”, 不区分大小写</li><li>find . -type f -name “*.py”</li></ul><h2 id="by-time"><a href="#by-time" class="headerlink" title="by time"></a>by time</h2><ul><li>find . -type f -mmin -10,过去十分钟修改过的文件</li><li>find . -type f -mmin +10</li><li>find . -type f -mmin +1 -mmin -5</li><li>find . -type f -mtime -20</li></ul><p>amin,atime: access min and access day; cmin,ctime: change min and change day; mmin,mtime: modify;</p><h2 id="by-size"><a href="#by-size" class="headerlink" title="by size"></a>by size</h2><ul><li>find . -size +5m, k,g is work too</li><li>ls -lah ./folders, info about sub folder and files,including size</li><li>find . -empty</li></ul><h2 id="by-permission"><a href="#by-permission" class="headerlink" title="by permission"></a>by permission</h2><ul><li>find. -perm 777, read, write, and excute</li><li>find folder -exec chown coreschafer:www-data {} +</li><li>find folder, will return all folder, -exec will run the command in that results, {} palceholder, + end of the command.</li><li>find folder -type f -exec chmod 664 {} +</li><li>find folder -perm 664</li><li>find . -type f -name “*.jpg”</li><li>find . -type f -name “*.jpg” -maxdepth 1, searched 1 level down</li><li>find . -type f -name “*.jpg” -maxdepth 1 -exec rm {} +, delete serched files</li></ul><h1 id="Grep"><a href="#Grep" class="headerlink" title="Grep"></a>Grep</h1><h2 id="Grep-single-file"><a href="#Grep-single-file" class="headerlink" title="Grep single file"></a>Grep single file</h2><blockquote><p>searched text</p></blockquote><ul><li>grep “text_you_want_search” filename.txt</li><li>grep -w “text_you_want_search” filename.txt, have to match all words</li><li>grep -wi “text_you_want_search” filename.txt, igore the lowcase and uppearcse.</li><li>grep -win “text_you_want_search” filename.txt, get info about the line number</li><li>grep -win -B 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, behind</li><li>grep -win -A 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, ahead</li><li>grep -win -C 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, two line before and two behind.</li></ul><h2 id="Grep-multi-file"><a href="#Grep-multi-file" class="headerlink" title="Grep multi file"></a>Grep multi file</h2><ul><li>grep -win “text_” ./*, all file</li><li>grep -win “text_” ./*.txt, txt file</li><li>grep -winr “text” ./ , search all subdir</li><li>grep -wirl “text” ./ , no need match info, just file list</li><li>grep -wirc “text” ./ , show matched number in eatch file</li></ul><h2 id="Grep-command-history"><a href="#Grep-command-history" class="headerlink" title="Grep command history"></a>Grep command history</h2><ul><li>history | grep “git commit”</li><li>history | grep “git commit” | grep “dotfile”</li></ul><h2 id="Grep-rgx"><a href="#Grep-rgx" class="headerlink" title="Grep rgx"></a>Grep rgx</h2><ul><li>grep -P “\d{3}-\d{3}-\d{4}” file.txt, work well in linux, mac need to config, I configed</li></ul><h1 id="cURL"><a href="#cURL" class="headerlink" title="cURL"></a>cURL</h1><h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><ul><li>curl url</li><li>curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a></li><li>curl http:<a href="http://www.wittyfans.com/json_file" target="_blank" rel="noopener">www.wittyfans.com/json_file</a></li><li>curl -i http:<a href="http://www.wittyfans.com/json_file" target="_blank" rel="noopener">www.wittyfans.com/json_file</a>, details info about the get</li><li>curl http:<a href="http://www.wittyfans.com/method" target="_blank" rel="noopener">www.wittyfans.com/method</a></li><li>curl -d “first=name&amp;last=lastname” http:<a href="http://www.wittyfans.com/method" target="_blank" rel="noopener">www.wittyfans.com/method</a>, d for data, Post request</li><li>curl -X PUT -d “first=name&amp;last=lastname” http:<a href="http://www.wittyfans.com/method" target="_blank" rel="noopener">www.wittyfans.com/method</a>, d for data, Pust request</li><li>curl -X DELETE http:<a href="http://www.wittyfans.com/method" target="_blank" rel="noopener">www.wittyfans.com/method</a>, delete request</li></ul><h2 id="Verify"><a href="#Verify" class="headerlink" title="Verify"></a>Verify</h2><p>Could not verify your access ?</p><p><code>curl -u username:password http://wittyfans.com, Auth</code></p><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><ul><li>curl <a href="http://wittyfans.com/folder">http://wittyfans.com/folder</a>, return binary file , error</li><li>curl -o filename.jpg <a href="http://wittyfans.com/folder">http://wittyfans.com/folder</a> , sucess</li><li>curl -o file_name.json http:/.api.wittyfans.com , Saving large json file</li></ul><h1 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h1><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><p>aviable in Mac, debian-based linux need to install</p><ul><li>apt-get install rsync</li><li>yum install rsync</li></ul><h2 id="Use"><a href="#Use" class="headerlink" title="Use"></a>Use</h2><ul><li>rsync folder1/* backup/ , sync fils to backup folder,will skping the subfolder’s file, but affected subfolder</li><li>rsync -r folder1/* backup/ , including subfolder’s file</li><li>rsync -r folder1 backup/, sync folder, not content in it</li></ul><h2 id="Check-chage-before-run"><a href="#Check-chage-before-run" class="headerlink" title="Check chage before run"></a>Check chage before run</h2><ul><li>rsync -a –dry-run folder1/* backup/, check before the command run, now view showed</li><li>rsync -av –dry-run folder1/* backup/, auto view</li></ul><h2 id="Source-folder-has-new-file"><a href="#Source-folder-has-new-file" class="headerlink" title="Source_folder has new file"></a>Source_folder has new file</h2><ul><li>rsync -av –delete –dry-run original/ backup/, check, be careful !</li></ul><h2 id="Do-it-in-local-and-host"><a href="#Do-it-in-local-and-host" class="headerlink" title="Do it in local and host"></a>Do it in local and host</h2><ul><li>rsync -zaP -p local_folder username@ip:~/public/, z for compress, a for all, P for tarnsfer in internet</li><li>rsync 0zaP username@ip:~/public/file ~/Downloads/, revers</li></ul><h1 id="Manipulating-data"><a href="#Manipulating-data" class="headerlink" title="Manipulating data"></a>Manipulating data</h1><blockquote><p>How to work with the data in those files</p></blockquote><p><code>cat</code>: view a files contents, meaning concatenate</p><p><code>less</code> &amp; <code>more</code>: view contents piece by piece, <code>more</code> is superseded by <code>less</code> now, In <code>less</code>:</p><ul><li><code>:n</code>, Move to next file</li><li><code>:p</code>, Go back to previous file</li><li><code>:q</code>, quit</li></ul><p><code>head</code>: look at the start of a text file, <code>head -3</code>, only display the first three lines</p><p><code>tail</code>: look at the end of a text file, <code>tail -n +7</code>, display content from line 7 to end</p><p><code>ls</code> : list everything below a directory, <code>ls -R -F</code>, <code>-R</code> recursive <code>-F</code>prints a <code>/</code> after the name of every directory and a <code>*</code> after the name of every runnable program. </p><p><code>man</code>: manual, automatically  invokes <code>less</code></p><p><code>cut -f 2-5,8 -d values.csv</code>: select columns 2 through 5 and columns 8, using comma as the separator. <code>-d</code> means delimiter, <code>-f</code> meaning fields to specify columns</p><p><code>!command</code>: re-run the most recent use of that command matched</p><p><code>grep bicuspid seasonal/winter.csv</code>: prints lines from winter.csv that contain “bicuspid”</p><p><code>cat two_cities.txt | egrep &#39;Sydney Carton|Charles Darnay&#39; | wc -l</code> : Count the number of lines in the book that contain <em>either</em> the character ‘Sydney Carton’ or ‘Charles Darnay’.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-c: <span class="built_in">print</span> a count of matching lines rather than the lines themselves</span><br><span class="line">-h: <span class="keyword">do</span> not <span class="built_in">print</span> the names of files when searching multiple files</span><br><span class="line">-i: ignore <span class="keyword">case</span> (e.g., treat <span class="string">"Regression"</span> and <span class="string">"regression"</span> as matches)</span><br><span class="line">-l: <span class="built_in">print</span> the names of files that contain matches, not the matches</span><br><span class="line">-n: <span class="built_in">print</span> line numbers <span class="keyword">for</span> matching lines</span><br><span class="line">-v: invert the match, i.e., only show lines that don<span class="string">'t match</span></span><br><span class="line"><span class="string">egrep: grep -E</span></span><br></pre></td></tr></table></figure><h2 id="Combining-tools"><a href="#Combining-tools" class="headerlink" title="Combining tools"></a>Combining tools</h2><blockquote><p>How to use this power to select the data you want, and introduce commands for sorting values and removing duplicates.</p></blockquote><p><code>head -n 5 seasonal/summer.csv &gt; top.csv</code>:  get first 5 rows content of summer.csv, write to top.csv</p><p><code>cut -d , -f 2 seasonal/summer.csv | grep -v Tooth</code>,  select all of the tooth names from column 2 of the comma delimited file.</p><p><code>wc</code>, word count, count a date from a file. <code>grep 2017-07 seasonal/spring.csv | wc -l</code></p><p><code>head -n 3 seasonal/s*</code>, show all s* files first 3 rows.</p><ul><li><code>*</code>, all</li><li><code>?</code>, single word</li><li><code>[...]</code> matches any one of the characters inside the square brackets, <code>201[78].txt</code> matches <code>2017.txt</code> or <code>2018.txt</code>, but not <code>2016.txt</code></li><li><code>{...}</code> matches any of the comma-separated patterns inside the curly brackets, so <code>{*.txt, *.csv}</code> matches any file whose name ends with <code>.txt</code> or <code>.csv</code>, but not files whose names end with <code>.pdf</code>.</li></ul><p><code>sort</code>, <code>-n</code>: sort numerically, <code>-r</code>: reverse, <code>-b</code>: ignore leading blanks, <code>-f</code>: be case-insensitive</p><p><code>uniq</code>, remove <strong>adjacent</strong> duplicated lines .</p><p><code>wc -l seasonal/*.csv</code>, Print line numbers for each file in folder seasonal</p><p><code>wc -l seasonal/*.csv | grep -v &#39;total&#39; | sort -n | head -n 1</code>, remove rows with word ‘total’ and select first row.</p><h2 id="Batch-processing"><a href="#Batch-processing" class="headerlink" title="Batch processing"></a>Batch processing</h2><blockquote><p>How to make your own pipelines do that. Along the way, you will see how the shell uses variables to store information.</p></blockquote><p><code>set</code>: check <strong>environment variables</strong></p><p><code>echo</code>: print</p><ul><li><code>$User</code>, user name</li><li><code>$OSTYPE</code> name of the kind of operating system you are using</li></ul><p><code>training=seasonal/summer.csv</code> then <code>echo $training</code>, define a variable and print it.</p><p><strong>For loop:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example 1</span></span><br><span class="line"><span class="keyword">for</span> filetype <span class="keyword">in</span> gif jpg png; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$filetype</span>; <span class="keyword">done</span></span><br><span class="line"><span class="comment"># Example 2</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> seasonal/*.csv; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$filename</span>; <span class="keyword">done</span></span><br><span class="line"><span class="comment"># Example 3</span></span><br><span class="line">datasets=seasonal/*.csv</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> <span class="variable">$datasets</span>; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$filename</span>; <span class="keyword">done</span></span><br><span class="line"><span class="comment"># Example 4</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> seasonal/*.csv; <span class="keyword">do</span> head -n 2 <span class="variable">$file</span> | tail -n 1; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><blockquote><p>Do not using space in file name, it will causing issue in bash.</p></blockquote><h2 id="Creating-new-tools"><a href="#Creating-new-tools" class="headerlink" title="Creating new tools"></a>Creating new tools</h2><blockquote><p>How to go one step further and create new commands of your own.</p></blockquote><p><strong>Edit file using <code>nano</code>:</strong></p><p><code>nano filename.txt</code>, edit a file</p><ul><li><code>Ctrl</code> + <code>K</code>: delete a line.</li><li><code>Ctrl</code> + <code>U</code>: un-delete a line.</li><li><code>Ctrl</code> + <code>O</code>: save the file (‘O’ stands for ‘output’). <em>You will also need to press Enter to confirm the filename!</em></li><li><code>Ctrl</code> + <code>X</code>: exit the editor.</li></ul><p><code>grep -h -v Tooth spring.csv summer.csv &gt; temp.csv</code>, <code>-h</code> stop it from printing filenames, <code>-v</code> printing all rows exclude Tooth</p><p><code>history | tail -n 3</code>, Show most recent 3 commands</p><p><code>$@</code>, pass filenames to scripts. <code>tail -q -n +2 $@ | wc -l</code></p><h2 id="Downloading-data"><a href="#Downloading-data" class="headerlink" title="Downloading data"></a>Downloading data</h2><blockquote><p>how to download data files from web servers via the command line</p></blockquote><p><code>curl</code>, Client for URLs.  <code>man curl</code> , check curl installation.</p><ul><li><code>curl -O url</code> save the file with it’s original name</li><li><code>curl -o newname.txt url</code> new file name</li><li>Download all 100 data file, <code>curl -O https://s3.amazonaws.com/datafile[001-100].txt</code></li></ul><p><code>Wget</code>, World Wide Web and get. better than curl when downloading multiple files recursively. <code>which wget</code>, check <code>wget</code> installation.</p><p><code>wget -c -b https://wittyfans.com/201812SpotifyData.zip</code></p><ul><li><code>-c</code>, resume broken download</li><li><code>-b</code>, go to background </li><li><code>wget --wait=1 -i url_list.txt</code>, # Create a mandatory 1 second pause between downloading all files in url_list.txt</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use curl, download and rename a single file from URL</span></span><br><span class="line">curl -O Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unzip, delete, then re-name to Spotify201812.csv</span></span><br><span class="line">unzip Spotify201812.zip &amp;&amp; rm Spotify201812.zip</span><br><span class="line">mv 201812SpotifyData.csv Spotify201812.csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># View url_list.txt to verify content</span></span><br><span class="line">cat url_list.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use Wget, limit the download rate to 2500 KB/s, download all files in url_list.txt</span></span><br><span class="line">wget --<span class="built_in">limit</span>-rate=2500k -i url_list.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Take a look at all files downloaded</span></span><br><span class="line">ls</span><br></pre></td></tr></table></figure><h2 id="CSV-Kit"><a href="#CSV-Kit" class="headerlink" title="CSV Kit"></a>CSV Kit</h2><blockquote><p>Using <code>csvkit</code> to convert, preview, filter and manipulate files to prepare our data for further analyses.</p></blockquote><p><code>pip install csvk</code> for install, <a href="https://csvkit.readthedocs.io/en/latest/tutorial.html" target="_blank" rel="noopener">Doc</a></p><h3 id="in2csv"><a href="#in2csv" class="headerlink" title="in2csv"></a>in2csv</h3><ul><li><code>in2csv -h</code>, converting files to csv.</li><li><code>in2csv SpotifyData.xlsx &gt; SpotifyData.csv</code></li><li><code>in2csv SpotifyData.xlsx --sheet &quot;Worksheet1_Popularity&quot; &gt; Spotify_Popularity.csv</code>, Only converting a sheet</li></ul><h3 id="csvlook"><a href="#csvlook" class="headerlink" title="csvlook"></a>csvlook</h3><ul><li><code>csvlook -h</code>, data preview on the command line.</li><li><code>csvlook SpotifyData.csv</code></li></ul><h3 id="csvsort"><a href="#csvsort" class="headerlink" title="csvsort"></a>csvsort</h3><ul><li><code>csvsort -c 2 Spotify_Popularity.csv | csvlook</code></li></ul><h3 id="csvstat"><a href="#csvstat" class="headerlink" title="csvstat"></a>csvstat</h3><ul><li><code>csvstat Spotify_Popularity.csv</code>, summary statistics</li></ul><h3 id="csvcut"><a href="#csvcut" class="headerlink" title="csvcut"></a>csvcut</h3><ul><li><code>csvcut -n Spotify_MusicAttributes.csv</code>, Print a list of column headers in data file</li><li><code>csvcut -c 1,3,5 Spotify_MusicAttributes.csv</code>, Print the first column, by position</li><li><code>csvcut -c &quot;track_id&quot;,&quot;duration_ms&quot;,&quot;loudness&quot; Spotify_MusicAttributes.csv</code>, Print the track id, song duration, and loudness, by name </li></ul><h3 id="csvgrep"><a href="#csvgrep" class="headerlink" title="csvgrep"></a>csvgrep</h3><ul><li><code>csvgrep -c &quot;danceability&quot; -m 0.812 Spotify_MusicAttributes.csv</code>, filter row danceability by value 0.812, column name must with “”.</li></ul><h3 id="csvstack"><a href="#csvstack" class="headerlink" title="csvstack"></a>csvstack</h3><ul><li><code>csvstack</code>, merge files. </li><li><code>csvstack Spotify_Rank6.csv Spotify_Rank7.csv &gt; Spotify.csv</code>, merge two files to one</li><li><code>csvstack -g &quot;Rank6&quot;,&quot;Rank7&quot; \ Spotify_Rank6.csv Spotify_Rank7.csv &gt; Spotify_Al</code>, merge two files to one and add a source column.</li></ul><h3 id="chain-commands"><a href="#chain-commands" class="headerlink" title="chain commands"></a>chain commands</h3><ul><li><code>;</code>, links commands together and runs sequentially</li><li><code>&amp;&amp;</code>, links commands together, but only runs the 2nd command if the 1st succeeds</li><li><code>&gt;</code>, using outputs from the 1st command</li><li><code>|</code>, using outputs form the 1st as input to the 2nd</li></ul><h3 id="sql2csv"><a href="#sql2csv" class="headerlink" title="sql2csv"></a>sql2csv</h3><ul><li><code>sql2csv -v</code> or <code>sql2csv --verbose</code>,  printing more tracebacks and logs </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pull the entire Spotify_Popularity table and print in log</span></span><br><span class="line">sql2csv --db <span class="string">"sqlite:///SpotifyDatabase.db"</span> \</span><br><span class="line">        --query <span class="string">"SELECT * FROM Spotify_Popularity"</span></span><br></pre></td></tr></table></figure><h3 id="csvsql"><a href="#csvsql" class="headerlink" title="csvsql"></a>csvsql</h3><p>Manipulating data using SQL syntax (Small to medium files only) :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reformat the output using csvlook </span></span><br><span class="line">csvsql --query <span class="string">"SELECT * FROM Spotify_MusicAttributes ORDER BY duration_ms LIMIT 1"</span> \</span><br><span class="line">Spotify_MusicAttributes.csv | csvlook</span><br></pre></td></tr></table></figure><p>using bash variable:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Store SQL query as shell variable</span></span><br><span class="line">sqlquery=<span class="string">"SELECT * FROM Spotify_MusicAttributes ORDER BY duration_ms LIMIT 1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply SQL query to Spotify_MusicAttributes.csv</span></span><br><span class="line">csvsql --query <span class="string">"<span class="variable">$sqlquery</span>"</span> Spotify_MusicAttributes.csv</span><br></pre></td></tr></table></figure><p>join two file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Store SQL query as shell variable</span></span><br><span class="line">sql_query=<span class="string">"SELECT ma.*, p.popularity FROM Spotify_MusicAttributes ma INNER JOIN Spotify_Popularity p ON ma.track_id = p.track_id"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Join 2 local csvs into a new csv using the saved SQL</span></span><br><span class="line">csvsql --query <span class="string">"<span class="variable">$sql_query</span>"</span> Spotify_MusicAttributes.csv Spotify_Popularity.csv &gt; Spotify_FullData.csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preview newly created file</span></span><br><span class="line">csvstat Spotify_FullData.csv</span><br></pre></td></tr></table></figure><p>Pushing data back to database:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Store SQL for querying from SQLite database </span></span><br><span class="line">sqlquery_pull=<span class="string">"SELECT * FROM SpotifyMostRecentData"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply SQL to save table as local file </span></span><br><span class="line">sql2csv --db <span class="string">"sqlite:///SpotifyDatabase.db"</span> --query <span class="string">"<span class="variable">$sqlquery_pull</span>"</span> &gt; SpotifyMostRecentData.csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store SQL for UNION of the two local CSV files</span></span><br><span class="line">sqlquery_union=<span class="string">"SELECT * FROM SpotifyMostRecentData UNION ALL SELECT * FROM Spotify201812"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply SQL to union the two local CSV files and save as local file</span></span><br><span class="line">csvsql --query <span class="string">"<span class="variable">$sqlquery_union</span>"</span> SpotifyMostRecentData.csv Spotify201812.csv &gt; UnionedSpotifyData.csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># Push UnionedSpotifyData.csv to database as a new table</span></span><br><span class="line">csvsql --db <span class="string">"sqlite:///SpotifyDatabase.db"</span> --insert UnionedSpotifyData.csv</span><br></pre></td></tr></table></figure><h1 id="Bash-Script"><a href="#Bash-Script" class="headerlink" title="Bash Script"></a>Bash Script</h1><h2 id="Stream-editor"><a href="#Stream-editor" class="headerlink" title="Stream editor"></a>Stream editor</h2><p><code>sed</code>: stream editor.</p><p><code>cat soccer_scores.csv | sed &#39;s/Cherno/Cherno City/g&#39; &gt; soccer_scores_edited.csv</code> : replace word <code>Cherno</code> to  <code>herno City</code> then save it to a new file, for more, check <a href="https://coolshell.cn/articles/9104.html" target="_blank" rel="noopener">this</a>.</p><h2 id="Argument"><a href="#Argument" class="headerlink" title="Argument"></a>Argument</h2><p><code>$1</code> or <code>$2</code> in bash script file, receive ARGV in bash scripts. <code>$@</code> , <code>$*</code> means get ARGV list, <code>$#</code> means get ARGV length. <code>cat hire_data/*.csv | grep &quot;$1&quot; &gt; &quot;$1&quot;.csv</code>: take in a city (an argument) as a variable, filter all the files by this city and output to a new CSV with the city name.</p><h2 id="Quotes"><a href="#Quotes" class="headerlink" title="Quotes"></a>Quotes</h2><p>Single,double,backticks. </p><ul><li>Single quotes (‘sometext’) = Shell interprets what is between literally</li><li>Double quotes (“sometext”) = Shell interpret literally except using $ and backticks</li><li>Backticks (`sometext`) = Shell runs the command and captures STDOUT back into a variable</li></ul><h2 id="Numeric-variables"><a href="#Numeric-variables" class="headerlink" title="Numeric variables"></a>Numeric variables</h2><p>In bash, Type <code>&gt;&gt;&gt; 1 + 5</code> will get error. instead, you need type <code>expr 1 + 5</code>.  <code>expr</code>  is utility program just like <code>cat</code> and <code>grep</code>. but <code>expr</code> cannot natively handle decimal places. <code>expr 1 + 2.5</code> will get <code>not a decimal nuber error</code>.</p><p>Introduce <code>bc</code> (basic calculator), a useful command-line program. using <code>bc</code> without opening the calculator:</p><p><code>echo &quot;5+7.5&quot; | bc</code> , <code>bc</code> has a <code>scale</code> argument for how many decimal places: <code>echo &quot;scale=3; 10 /3 | bc&quot;</code>, <code>;</code> is to separate lines in terminal.</p><h2 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h2><h3 id="Normal-array"><a href="#Normal-array" class="headerlink" title="Normal array"></a>Normal array</h3><p>Create array: <code>capital_cities=(&quot;Sydney&quot; &quot;New York&quot; &quot;Paris&quot;)</code></p><p>Add element: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a normal array with the mentioned elements using the declare method</span></span><br><span class="line"><span class="built_in">declare</span> -a capital_cities</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add (append) the elements</span></span><br><span class="line">capital_cities+=(<span class="string">"Sydney"</span>)</span><br><span class="line">capital_cities+=(<span class="string">"New York"</span>)</span><br><span class="line">capital_cities+=(<span class="string">"Paris"</span>)</span><br></pre></td></tr></table></figure><p>Get all element and length of array:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The array has been created for you</span></span><br><span class="line">capital_cities=(<span class="string">"Sydney"</span> <span class="string">"New York"</span> <span class="string">"Paris"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the entire array</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;capital_cities[@]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the array length</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;#capital_cities[@]&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Associative-arrays"><a href="#Associative-arrays" class="headerlink" title="Associative arrays"></a>Associative arrays</h3><blockquote><p>Like dictionary in python.</p></blockquote><p>Create:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create empty associative array</span></span><br><span class="line"><span class="built_in">declare</span> -A model_metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the key-value pairs</span></span><br><span class="line">model_metrics[model_accuracy]=98</span><br><span class="line">model_metrics[model_name]=<span class="string">"knn"</span></span><br><span class="line">model_metrics[model_f1]=0.82</span><br></pre></td></tr></table></figure><p>Create in one line:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># An associative array has been created for you</span></span><br><span class="line"><span class="built_in">declare</span> -A model_metrics=([model_accuracy]=98 [model_name]=<span class="string">"knn"</span> [model_f1]=0.82)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out just the keys</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;____model_metrics[____]&#125;</span></span><br></pre></td></tr></table></figure><p>Example:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create variables from the temperature data files</span></span><br><span class="line">temp_b=<span class="string">"<span class="variable">$(cat temps/region_B)</span>"</span></span><br><span class="line">temp_c=<span class="string">"<span class="variable">$(cat temps/region_C)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an array with these variables as elements</span></span><br><span class="line">region_temps=(<span class="variable">$temp_b</span> <span class="variable">$temp_c</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call an external program to get average temperature</span></span><br><span class="line">average_temp=$(<span class="built_in">echo</span> <span class="string">"scale=2; (<span class="variable">$&#123;region_temps[0]&#125;</span> + <span class="variable">$&#123;region_temps[1]&#125;</span>) / 2"</span> | bc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Append to array</span></span><br><span class="line">region_temps+=(<span class="variable">$average_temp</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the whole array</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;region_temps[@]&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Control-Statements"><a href="#Control-Statements" class="headerlink" title="Control Statements"></a>Control Statements</h2><h3 id="IF-statements"><a href="#IF-statements" class="headerlink" title="IF statements"></a>IF statements</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ condition1 ] &amp;&amp; [ condition2 ]; <span class="keyword">then</span></span><br><span class="line"><span class="comment"># some code</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="comment"># some code</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>Move files based on content:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extract Accuracy from first ARGV element</span></span><br><span class="line">accuracy=$(grep Accuracy <span class="variable">$1</span> | sed <span class="string">'s/.* //'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Conditionally move into good_models folder</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$accuracy</span> -ge 90 ]; <span class="keyword">then</span></span><br><span class="line">    mv <span class="variable">$1</span> ./good_models</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Conditionally move into bad_models folder</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$accuracy</span> -lt 90 ]; <span class="keyword">then</span></span><br><span class="line">    mv <span class="variable">$1</span> ./bad_models</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normal flags</span></span><br><span class="line">-eq =</span><br><span class="line">-ne !=</span><br><span class="line">-lt &lt;</span><br><span class="line">-le &lt;=</span><br><span class="line">-gt &gt;</span><br><span class="line">-ge &gt;=</span><br><span class="line"></span><br><span class="line"><span class="comment"># File related flags</span></span><br><span class="line">-e <span class="keyword">if</span> the file exists</span><br><span class="line">-s <span class="keyword">if</span> the file exists and has size greater than zero</span><br><span class="line">-r <span class="keyword">if</span> the file exists and is readable</span><br><span class="line">-w <span class="keyword">if</span> the file exists and is writable</span><br><span class="line"></span><br><span class="line"><span class="comment">## And and OR</span></span><br><span class="line">&amp;&amp; <span class="keyword">for</span> and</span><br><span class="line">|| <span class="keyword">for</span> or</span><br></pre></td></tr></table></figure><h2 id="For-loops"><a href="#For-loops" class="headerlink" title="For loops"></a>For loops</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use a FOR loop on files in directory</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> inherited_folder/*.R</span><br><span class="line"><span class="keyword">do</span>  </span><br><span class="line">    <span class="comment"># Echo out each file</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$file</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Move python file to to_keep if it using random forest class fier</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> robs_files/*.py</span><br><span class="line"><span class="keyword">do</span>  </span><br><span class="line">    <span class="comment"># Create IF statement using grep</span></span><br><span class="line">    <span class="keyword">if</span> grep -q <span class="string">'RandomForestClassifier'</span> <span class="variable">$file</span> ; <span class="keyword">then</span></span><br><span class="line">        <span class="comment"># Move wanted files to to_keep/ folder</span></span><br><span class="line">        mv <span class="variable">$file</span> to_keep/</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="Case"><a href="#Case" class="headerlink" title="Case"></a>Case</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a CASE statement matching the first ARGV element</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">  <span class="comment"># Match on all weekdays</span></span><br><span class="line">  Monday|Tuesday|Wednesday|Thursday|Friday)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"It is a Weekday!"</span>;;</span><br><span class="line">  <span class="comment"># Match on all weekend days</span></span><br><span class="line">  Saturday|Sunday)</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"It is a Weekend!"</span>;;</span><br><span class="line">  <span class="comment"># Create a default</span></span><br><span class="line">  *) </span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Not a day!"</span>;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> function_name &#123;    </span><br><span class="line"><span class="comment">#function_code    </span></span><br><span class="line"><span class="built_in">return</span> <span class="comment">#something</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">print_hello</span></span> () &#123;    </span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Hello world!"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">print_hello</span><br></pre></td></tr></table></figure><blockquote><p><strong>Scope: All variables in Bash are global by default!</strong>Using <code>local val</code> to restrict variable scope.</p></blockquote><p>To get data out from function:</p><ol><li>Assign to a global variable</li><li><code>echo</code> what we want back in last line and capture using shell-within-shell</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> convert &#123;</span><br><span class="line"><span class="built_in">echo</span> $(<span class="built_in">echo</span> <span class="variable">$1</span>)</span><br><span class="line">&#125;</span><br><span class="line">converted = $(convert 30)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a function with a local base variable</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">sum_array</span></span> () &#123;</span><br><span class="line">  <span class="built_in">local</span> sum=0</span><br><span class="line">  <span class="comment"># Loop through, adding to base variable</span></span><br><span class="line">  <span class="keyword">for</span> number <span class="keyword">in</span> <span class="string">"<span class="variable">$@</span>"</span></span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    sum=$(<span class="built_in">echo</span> <span class="string">"<span class="variable">$sum</span> + <span class="variable">$number</span>"</span> | bc)</span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">  <span class="comment"># Echo back the result</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$sum</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment"># Call function with array</span></span><br><span class="line">test_array=(14 12 23.5 16 19.34)</span><br><span class="line">total=$(sum_array <span class="string">"<span class="variable">$&#123;test_array[@]&#125;</span>"</span>)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"The total sum of the test array is <span class="variable">$total</span>"</span></span><br></pre></td></tr></table></figure><h1 id="Python-script-on-bash"><a href="#Python-script-on-bash" class="headerlink" title="Python script on bash"></a>Python script on bash</h1><blockquote><p>Scheduling a job with crontab</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preview both Python script and requirements text file</span></span><br><span class="line">cat create_model.py</span><br><span class="line">cat requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pip install Python dependencies in requirements file</span></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Python script on command line</span></span><br><span class="line">python create_model.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add CRON job that runs create_model.py every minute</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"* * * * * python create_model.py"</span> | crontab</span><br><span class="line"></span><br><span class="line"><span class="comment"># Verify that the CRON job has been scheduled via CRONTAB</span></span><br><span class="line">crontab -l</span><br></pre></td></tr></table></figure><h1 id="Cron"><a href="#Cron" class="headerlink" title="Cron"></a>Cron</h1><ul><li>crontab -l, list the crons</li></ul><blockquote><p>set your editor to nano, default vim</p></blockquote><ul><li>export EDITOR=/user/bin/nano</li><li>crontab -e, open editor</li><li>press i to input</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Algorhythm for schuel you task with time</span><br><span class="line"></span><br><span class="line"># ┌───────────── minute (0 - 59)</span><br><span class="line"># │ ┌───────────── hour (0 - 23)</span><br><span class="line"># │ │ ┌───────────── day of month (1 - 31)</span><br><span class="line"># │ │ │ ┌───────────── month (1 - 12)</span><br><span class="line"># │ │ │ │ ┌───────────── day of week (0 - 6) (Sunday to Saturday;</span><br><span class="line"># │ │ │ │ │                                       7 is also Sunday on some systems)</span><br><span class="line"># │ │ │ │ │</span><br><span class="line"># │ │ │ │ │</span><br><span class="line"># * * * * *  command_to_execute</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Bash is a Unix shell and command language, it survived and thrived for almost 50 years because it lets people do complex things with just a few keystrokes. Sometimes called “the universal glue of programming,” it helps users combine existing programs in new ways, automate repetitive tasks, and run programs on clusters and clouds that may be halfway around the world. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="bash" scheme="http://wittyfans.com/tags/bash/"/>
    
      <category term="linux" scheme="http://wittyfans.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>机器学习相关概念解读p1</title>
    <link href="http://wittyfans.com/coding/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E8%A7%A3%E8%AF%BBp1.html"/>
    <id>http://wittyfans.com/coding/机器学习相关概念解读p1.html</id>
    <published>2019-12-27T08:30:13.000Z</published>
    <updated>2019-12-27T08:38:25.146Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>今天开始准备对机器学习相关概念做个总结，part1会包括以下概念：Cross Validation、Confusion Matrix、Sensitivity 与 Specificity、Bias 与 Variance、ROC 与 AUC、Odds Ratios 与 Log(Odds Ratios)。</p></blockquote><a id="more"></a><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h1><p>假设你需要根据肩膀痛、血液状况、动脉阻塞、体重与否来预测患者有没有心脏病，输出的结果为是与否。<br>当你对数据进行训练的时候，你需要指定使用的机器学习模型，常用的比如：</p><ul><li>LR</li><li>KNN</li><li>SVM</li></ul><p>但你怎么知道选择哪一种模型呢？Cross Validation 可以让我们大概知道哪些模型会fit比较好，哪些模型在实际应用的时候表现会更好。</p><p>当训练模型的时候，需要将数据切割成训练组与测试组。如果你把所有的数据都拿去训练，那你就没有了测试数据，因为你的模型必须用它没有见过的数据来做测试。</p><p>通常我们使用数据中的前面百分之75的数据作为训练数据</p><p><img src="https://i.loli.net/2019/12/26/bH8Ep7IUjYXOziB.png" alt=""></p><p>后面的部分作为测试数据</p><p><img src="https://i.loli.net/2019/12/26/xqAwCLeT2GObjB9.png" alt=""></p><p>但是你怎么知道按照75:25的划分是最好的呢？如果我们使用前面百分之25的数据测试呢？或者是取中间的25%测试，其余的作为训练数据？</p><p>担心选择哪一段数据作为训练和测试数据，我们用Cross Validation，它会逐一将所有的数据都测试一遍，然后汇总结果。这样每一组数据都作为测试数据参与过模型训练，也参与过模型的检测。逐一测试后，再汇总所有结果，我们就可以选择最适合数据的模型。</p><p><img src="https://i.loli.net/2019/12/26/Hg4LjJTUpye7GlZ.png" alt=""></p><p>在这，如果你把数据分成4份，就叫4-Fold Cross Validation, 5份就叫5fold，以此类推。</p><h1 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h1><p>Confusion Matrix这个概念告诉我们，机器学习的算法预测的结果中，正确了多少、错了多少以及正确的分布在哪里，错的又分布在哪里。</p><p><img src="https://i.loli.net/2019/12/24/yndGs7EN8UCvbVz.png" alt=""></p><p>比如这个例子中，我们想要根据肩膀痛、血液状况、动脉阻塞、体重与否来预测患者有没有心脏病，输出的结果为是与否。</p><p>我们可以使用逻辑回归、knn、随机森林等模型来预测，或者你也可以使用任何其他的模型，但是我们如何来衡量这个模型的好坏呢？</p><p>首先将数据切分成训练组与测试组，在对上述模型进行训练之后，我们可以拿模型来对测试组的数据进行测试，然后将预测的结果与实际的结果进行对比，根据下面的表填入结果：</p><p><img src="https://i.loli.net/2019/12/24/eKaHMGr3tw8nDUb.png" alt=""></p><p>通过这个表，就可以知道你的模型预测的结果分布如何，对于每一个模型我们都可以绘制此表用来对比模型的性能。</p><h1 id="Sensitivity-与-Specificity"><a href="#Sensitivity-与-Specificity" class="headerlink" title="Sensitivity 与 Specificity"></a>Sensitivity 与 Specificity</h1><p><img src="https://i.loli.net/2019/12/26/FxOVkEmXPRdB1qH.png" alt=""></p><p>在你理解了Confusion Matrix之后，我们再来讨论Sensitivity 与 Specificity.</p><p>这里的Sensitivity，也可以理解为我们在sklearn课程中提及到的<em>Recall</em>，它等于 tp/(tp+fn)，即所有有心脏病的人中，多少预测对了；</p><p>这是Sensitivity的计算:<br><img src="https://i.loli.net/2019/12/26/EV8fWqBlXdxHChQ.png" alt=""></p><p>而Specificity，等于 tn/(tn+fp)，即所有真实没有心脏病的人中，多少预测对了。<br>这是Specificity的计算：<br><img src="https://i.loli.net/2019/12/26/bHhlMj8qpY29TIQ.png" alt=""></p><p>我们可以对不同的模型（如LR与随机森林）计算它们的Sensitivity 和 Specificity做比较。</p><p><img src="https://i.loli.net/2019/12/26/H8aXwTIn37Bo2Pq.png" alt=""></p><p>可以看到随机森林的sensitivity分数比较高，这意味着它对于区分positives的结果比较擅长，也就是预测哪些病人有心脏病。</p><p><img src="https://i.loli.net/2019/12/26/84bWv12kqtLTAfh.png" alt=""></p><p>而LR则在预测哪些病人没有心脏病方面比较擅长。</p><p>如果对你来说，更重要的是找出那些人有心脏病，那么你应该使用随机森林，如果你更看重那些人没有心脏病，那么应该使用LR.</p><p>如果你的Confusion Matrix是三行三列，那么计算方式就不一样了，不过也没那么复杂，注意好对应关系就好了：</p><p><img src="https://i.loli.net/2019/12/26/CLuoHWsUnjSIDXh.png" alt=""></p><p><img src="https://i.loli.net/2019/12/26/Zc2hEtBXgyvbxJ5.png" alt=""></p><p>更多请参考<a href="https://www.youtube.com/watch?v=vP06aMoz4v8&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=4" target="_blank" rel="noopener">油管视频</a>.</p><h1 id="Bias-与-Variance"><a href="#Bias-与-Variance" class="headerlink" title="Bias 与 Variance"></a>Bias 与 Variance</h1><p>你有一组老鼠的长度与体重数据，可以想象，老鼠的重量与长度是正比的，但它长到一定长度后，重量不会一直增长。我们使用LR训练一批老鼠数据，得到下面的图像:</p><p><img src="https://i.loli.net/2019/12/26/HcPiQa9w38V6XN7.png" alt=""></p><p>LR无法完全与训练数据重合，这就叫做bias，你的模型也可能完美的穿过这些测试数据，这样它的模型bias就为0.</p><p><img src="https://i.loli.net/2019/12/26/184lbpAgnfQ5G6i.png" alt=""></p><p>当我们用最小二乘法来衡量模型对测试数据的fit程度时，曲线模型无疑是最好的，但是基础，我们训练模型是用来做预测的，<strong>一个好的模型不是fit得好不好，而是预测的好不好。</strong></p><p><img src="https://i.loli.net/2019/12/26/kNMRjqLJuyDtrBb.png" alt=""></p><p>同样当我们用最小二乘法来测试预测结果时，LR直线模型却更好，这是因为曲线模型的Variance太高了。</p><p>bias高，可以理解为反应慢，但很稳。而Variance高，则意味反应很灵敏，发挥可能有时候很好，有时候很差。</p><p>直线的Variance比较低，因为对于不同的数据集的预测结果，算出来的最小二乘法之和比较低。</p><p>另外一种机器学习的描述方式，虽然我们的曲线模型对与训练数据fit的非常完美，但是在测试数据中却不理想，我们说这个模型overfit了。</p><p>理想的模型是variance和bias都比较低，能准确的反映数据的分布关系，以及做出稳定的预测。</p><p>要找到这个值，需要我们对模型做出调整，通常有三种方法：</p><ol><li>regularization</li><li>boosting</li><li>bagging</li></ol><p>后续我们再来介绍这些概念。</p><h1 id="ROC-与-AUC"><a href="#ROC-与-AUC" class="headerlink" title="ROC 与 AUC"></a>ROC 与 AUC</h1><h1 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h1><p>还是这组老鼠的数据，根据它们的体总来判定它是否是肥胖症，蓝色的点是判断为肥胖症的老鼠，红色的则没有肥胖症，其中有一个红点很特殊，它看起来体重很高但是没有肥胖（这肯定是只肌肉鼠），同时数据中也有一些不是很重但患有肥胖症的，这是因为它很短小，却很胖。</p><p><img src="https://i.loli.net/2019/12/27/jVYtvS4xTFE3qQh.png" alt=""></p><p>我们用LR fit这组数据，会得到这样的一个曲线，在LR中，y轴是我们的概率，值域为0-1，当你的LR模型绘制出来了的时候，给我一个老鼠的体重，我大概就可以判断它是不是有肥胖症，但是这需要一个threshold（临界值）作为判断点，比如在这里我们取概率为0.5对应的体重值。</p><p><img src="https://i.loli.net/2019/12/27/joR8bKSrmVMJ5IC.png" alt=""></p><p>当你将这个threshold定位0.5，我们就可以用test数据统计confusion matrax,并计算Sensitivity 与 Specificity,如果你觉得threshold设为0.5不太合适，你可以将其设为0.6，然后再计算Sensitivity 与 Specificity对比看看。</p><p>这个threshold可以不断的做出调整，以适应（fit）我们的数据。</p><p>比如，我们设置为它为0.1。</p><p><img src="https://i.loli.net/2019/12/27/3PNanWbGdTUR41K.png" alt=""></p><p>在这种情况下，所有0.1以上的都被标记成肥胖，增加了对肥胖预测的准确率，但会有一些非肥胖症的老鼠被预测称肥胖症。</p><p>你也可以设为0.9，所有0.9以下的老鼠都标记成非肥胖，这样会增加那些对非肥胖症预测的准确率，但会有一些肥胖的老鼠也被预测称非肥胖。</p><p><img src="https://i.loli.net/2019/12/27/fPx6QpAHoB2XqUm.png" alt=""></p><p>threshold可能是这其中的任何一个点，我们到底选哪个比较好呢？</p><p>这时候我们就可以使用ROC图像了，ROC图中x轴为 Specificity,y轴为Sensitivity，我们开始画图，第一步我们假设所有的老鼠都有肥胖症：</p><p><img src="https://i.loli.net/2019/12/27/8IqkjMU4FnhRtim.png" alt=""></p><p>对于Sensitivity，我们计算后为1，它是所有真实肥胖症老鼠中有多少预测对了。这意味着所有的老鼠都标记为肥胖症，也就是说所有真实患有肥胖症的老鼠都预测成功了。</p><p>对于Specificity，计算后也为1，它是所有真实没有肥胖症的老鼠中，多少预测对了。这意味着所有不是肥胖症的老鼠都被标记为肥胖症了。</p><p>我们可以将这个点与0连线，在这条线上，Sensitivity和Specificity相等，这意味着我们对测试数据预测正确的值与预测错误的比例相等（那还不如靠猜）。</p><p><img src="https://i.loli.net/2019/12/27/1G5jS2O9QcAPiUT.png" alt=""></p><p>现在我们把threshold设为0.3，我们得到sensitivity为1，specificity为0.75.</p><p><img src="https://i.loli.net/2019/12/27/nOaAb3Tz8hjfcLZ.png" alt=""></p><p>它的sensitivity&gt;specificity,所以在图上它出现在绿虚线的左边，所以threshold为0.3比之前的值要好。</p><p>我们继续增加threshold</p><p><img src="https://i.loli.net/2019/12/27/TcK1jJfu8sClVew.png" alt=""></p><p>这个结果又要好一点，我们一直继续，直到我们把所有的老鼠都预测为非肥胖症，将所有的点连起来，这就是我们的ROC曲线了。</p><ul><li>y轴为：真实数据中，肥胖的预测正确了多少。</li><li>x轴为：真实数据中，非肥胖的预测正确了多少。</li><li>x轴也可能为：预测数据中，肥胖的预测正确了多少。</li></ul><p><img src="https://i.loli.net/2019/12/27/YyrmfONXWc9Ub45.png" alt=""></p><p>这个图中越靠近左上的点，结果越好。同时我们可以根据我们可以接受多大的错误率，来选择threshold,</p><p><img src="https://i.loli.net/2019/12/27/aDWEdetqAZnKoTy.png" alt=""></p><h2 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h2><p>AUC就是ROC下面的曲线，如果我们通过更换模型，让曲线下方的面积变大了，那么新模型就比原来的模型更好。</p><p><img src="https://i.loli.net/2019/12/27/hbskK9xymApwYTF.png" alt=""></p><p>如果图中蓝色部分是随机森林模型生成的AUC，红色部分是LR生成的，那么你应该选择LR作为你的模型。</p><p>注：有时候人们会用precision代替specificity, 即precision=True Positive / True Positive + False positive</p><p>对比它们的定义：</p><p>specificity： 所有有肥胖症的老鼠中，预测正确了多少。<br>precision：所有预测为肥胖症的老鼠中，预测正确了多少。</p><h1 id="Odds-与-Probability"><a href="#Odds-与-Probability" class="headerlink" title="Odds 与 Probability"></a>Odds 与 Probability</h1><p>odds是你想要的结果除以你不想要的结果，而Probability是你想要的结果除以所有的结果。如在比赛中，你赢了5次，输了3次，那odds就是5/3,而概率测试5/8.</p><p><img src="https://i.loli.net/2019/12/25/QnURimjxBKEIMXt.png" alt=""></p><p>odds是可以通过概率来计算得出的，它的公式是：</p><p>$$\frac{p}{1-p}$$</p><p>我们可以思考一下odds的取值范围，如果有一场比赛，因为我的实力很差，10次比赛输了8次，那么我的odds就是2/8,如果更糟，0/10，也就是0。</p><p>可见odds的值域中，最低值是0，从1-0是你所有输掉比赛的情况，而从1到无穷大则是你赢了比赛的情况，因为你赢了一万场输一场的情况也是有可能的，它的odds值为10000/1=10000。</p><p>把odds的值放到数轴上看，你会发现这样的一条线：</p><p><img src="https://i.loli.net/2019/12/25/A6cUjIDus1orLbv.png" alt=""></p><p>但这里存在一个小问题，比如对于1:6和6:1，我输了6场和赢了6场，这两个结果看起来是对成的，得到的值画在数轴上却是不对称的。</p><p><img src="https://i.loli.net/2019/12/25/nl5tHL3TOx26FXE.png" alt=""></p><p>而使用log函数则可以解决这个问题：</p><p><img src="https://i.loli.net/2019/12/25/7YJxzFTvLpuE4Ij.png" alt=""></p><h1 id="Odds-Ratios-与-Log-Odds-Ratios"><a href="#Odds-Ratios-与-Log-Odds-Ratios" class="headerlink" title="Odds Ratios 与 Log(Odds Ratios)"></a>Odds Ratios 与 Log(Odds Ratios)</h1><p>odds本质上是ratio，但odds≠odds ratio,我们说odds ratio,指的是odds之间的运算，也就是两个不同结果之间odds值的运算。</p><p>odds ratio算出来的值可能在0-1之间，也可能在1到无穷大之间，如果我们需要让odds ratio值数轴上显示，对它取对数会比较好。</p><p>我们来举个例子，下面是癌症与基因突变的一组数据：</p><p><img src="https://i.loli.net/2019/12/25/lAECMGIj62ZOW8k.png" alt=""></p><p>这是什么意思呢？ odds ratio和log(ratio)说明了两个事物（这里是癌症与基因突变）之间的关系。</p><p>odds ratio值越大，基因突变这个变量对于预测癌症就越好用，越小，则说明这个变量不适合用来预测癌症。</p><p>然而要证明相关，还需要证明它们之间存在统计显著性，主要有三种方法：</p><ul><li>Fisher’s Exact Test</li><li>Chi-Square Test</li><li>The Wald Test</li></ul><p>有人比较喜欢用Fisher’s Exact Test和Chi-Square Test计算P-Value,有人比较喜欢用The Wald Test计算P-Value和置信区间。</p><p>假设有如下表：</p><p><img src="https://i.loli.net/2019/12/27/sPfknqYcBOL7INl.png" alt=""></p><h2 id="Fisher’s-Exact-Test"><a href="#Fisher’s-Exact-Test" class="headerlink" title="Fisher’s Exact Test"></a>Fisher’s Exact Test</h2><blockquote><p>参考:<a href="https://youtu.be/udyAvvaMjfM" target="_blank" rel="noopener">Fisher’s Exact Test</a></p></blockquote><h2 id="Chi-Square-Test"><a href="#Chi-Square-Test" class="headerlink" title="Chi-Square Test"></a>Chi-Square Test</h2><p>这是一种先假设癌症与基因变异之间没有关系的方法。<br>首先算出正常人患癌症的概率为：29/356 = 0.08.</p><p>有基因突变的为140人，如果按照正常人患癌症的概率，那么有基因突变的人中应该是 140*0.08=11.2 人有癌症，剩下 140-11.2=128.8 人无癌症。</p><p>同样的对于没有基因突变的人，患癌症的人为 216*0.08 = 17.3, 剩下 216-17.3 = 198.7人无癌症。</p><p>最后对比两个表，就可以算出p-vlue，如果你不知道具体如何算，可以参考别的文章。</p><p><img src="https://i.loli.net/2019/12/27/W5op2s8JqCfcawF.png" alt=""></p><h2 id="The-Wald-Test"><a href="#The-Wald-Test" class="headerlink" title="The Wald Test"></a>The Wald Test</h2><blockquote><p>todo.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;今天开始准备对机器学习相关概念做个总结，part1会包括以下概念：Cross Validation、Confusion Matrix、Sensitivity 与 Specificity、Bias 与 Variance、ROC 与 AUC、Odds Ratios 与 Log(Odds Ratios)。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="machine learning" scheme="http://wittyfans.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>加密货币市值分析</title>
    <link href="http://wittyfans.com/coding/%E5%8A%A0%E5%AF%86%E8%B4%A7%E5%B8%81%E5%B8%82%E5%80%BC%E5%88%86%E6%9E%90.html"/>
    <id>http://wittyfans.com/coding/加密货币市值分析.html</id>
    <published>2019-12-06T07:22:17.000Z</published>
    <updated>2019-12-06T07:27:54.773Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/12/06/UD9XlmW3fxoSIa5.png" alt=""></p><blockquote><p>自从08年比特币发布以来，数以百计类似的基于区块链技术的产品层出不穷。我们称这些为加密货币，时至今日，某些加密货币已经大幅上涨，某些在未来可能也极具上涨空间。实际上，在2017年12月6日，比特币的市值超过2000亿美元。</p></blockquote><a id="more"></a><p>加密货币市值分析</p><h1 id="席卷全球的加密货币"><a href="#席卷全球的加密货币" class="headerlink" title="席卷全球的加密货币"></a>席卷全球的加密货币</h1><p>让我们来研究研究加密货币，第一个任务，我们将从coinmarketcap API加载当前数据，并将其显示在输出中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Importing pandas</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># Importing matplotlib and setting aesthetics for plotting later.</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = &apos;svg&apos; </span><br><span class="line">plt.style.use(&apos;fivethirtyeight&apos;)</span><br><span class="line"></span><br><span class="line"># Reading in current data from coinmarketcap.com</span><br><span class="line">current = pd.read_json(&quot;https://api.coinmarketcap.com/v1/ticker/&quot;)</span><br><span class="line"></span><br><span class="line"># Printing out the first few lines</span><br><span class="line">print(current.head())</span><br></pre></td></tr></table></figure><p>从api返回的结果只包含了100条数据，像这种开放的api基本上都是只提供试用，如果想要更多更全的数据一般需要付费，这里我们使用一份我从网站里下载的csv数据，包括了17年6月的数据，这个数据集保存在本地，叫做<code>datasets/coinmarketcap_06122017.csv.</code></p><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Reading datasets/coinmarketcap_06122017.csv into pandas</span><br><span class="line">dec6 = pd.read_csv(&apos;datasets/coinmarketcap_06122017.csv&apos;)</span><br><span class="line"></span><br><span class="line"># Selecting the &apos;id&apos; and the &apos;market_cap_usd&apos; columns</span><br><span class="line">market_cap_raw = dec6[[&apos;id&apos;,&apos;market_cap_usd&apos;]]</span><br><span class="line"></span><br><span class="line"># Counting the number of values</span><br><span class="line">market_cap_raw.count()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">id                1326</span><br><span class="line">market_cap_usd    1031</span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><p>从输出种，我们看到这两个数据的条目数不一样，这是因为在列 <code>market_cap_usd</code> 种，存在一些空值（na值），而count函数是不将空值计算入内的，我们将它去掉。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Filtering out rows without a market capitalization</span><br><span class="line">cap = market_cap_raw.query(&apos;market_cap_usd &gt; 0&apos;)</span><br><span class="line"></span><br><span class="line"># Counting the number of values again</span><br><span class="line">cap.count()</span><br></pre></td></tr></table></figure><h1 id="比较比特币与其他加密货币"><a href="#比较比特币与其他加密货币" class="headerlink" title="比较比特币与其他加密货币"></a>比较比特币与其他加密货币</h1><p>现在比特币正处于与其他加密货币的激烈竞争中，但它仍然在市值中占主导地位，我们来比较一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#Declaring these now for later use in the plots</span><br><span class="line">TOP_CAP_TITLE = &apos;Top 10 market capitalization&apos;</span><br><span class="line">TOP_CAP_YLABEL = &apos;% of total cap&apos;</span><br><span class="line"></span><br><span class="line"># Selecting the first 10 rows and setting the index</span><br><span class="line">cap10 = cap.head(10).set_index(&apos;id&apos;)</span><br><span class="line"></span><br><span class="line"># Calculating market_cap_perc</span><br><span class="line">cap10 = cap10.assign(market_cap_perc =</span><br><span class="line">    lambda x: (x.market_cap_usd / cap.market_cap_usd.sum()) * 100)</span><br><span class="line"></span><br><span class="line"># Plotting the barplot with the title defined above </span><br><span class="line">ax = cap10.market_cap_perc.plot.bar(title=&apos;Top 10 market capitalization&apos;)</span><br><span class="line"></span><br><span class="line"># Annotating the y axis with the label defined above</span><br><span class="line">ax.set_ylabel(&apos;% of total cap&apos;)</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://i.loli.net/2019/12/06/XFakbKp1f4ML5ul.png" alt=""></p><p>可以看到比特币在整个加密货币中所占的份额多大，但是这个图还存在一个信息，可以对其进行改进，那就是比特币所占份额太大了，其他的加密货币之间就很难区分。</p><h1 id="可视化优化"><a href="#可视化优化" class="headerlink" title="可视化优化"></a>可视化优化</h1><p>解决上面的办法是我们需要更改坐标轴的单位，我们从百分比换成 log^10,如果你不知道如何优化你的数据可视化步骤，可以参考我的<a href="http://wittyfans.com/coding/数据可视化基础与技术.html">另一篇文章</a></p><p>另外，我们把类似的加密货币合并成同一个颜色,把x轴上的信息隐藏</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Colors for the bar plot</span><br><span class="line">COLORS = [&apos;orange&apos;, &apos;green&apos;, &apos;orange&apos;, &apos;cyan&apos;, &apos;cyan&apos;, &apos;blue&apos;, &apos;silver&apos;, &apos;orange&apos;, &apos;red&apos;, &apos;green&apos;]</span><br><span class="line"></span><br><span class="line"># Plotting market_cap_usd as before but adding the colors and scaling the y-axis  </span><br><span class="line">ax = cap10.market_cap_perc.plot.bar(title=&apos;Top 10 market capitalization&apos;,color=COLORS)</span><br><span class="line"></span><br><span class="line"># Annotating the y axis with &apos;USD&apos;</span><br><span class="line">ax.set_ylabel(&apos;USD&apos;)</span><br><span class="line"></span><br><span class="line"># Final touch! Removing the xlabel as it is not very informative</span><br><span class="line">ax.set_xlabel(&apos;&apos;)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/06/QJCehqgOsF6EdZT.png" alt=""></p><p><strong>隐藏X轴上的字的另一种方式：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_axis = ax.xaxis</span><br><span class="line">x_axis.label.set_visible(False)</span><br></pre></td></tr></table></figure><h1 id="恐怖的波动性"><a href="#恐怖的波动性" class="headerlink" title="恐怖的波动性"></a>恐怖的波动性</h1><p>加密货币发布以来就以其惊人的波动性著称，让我们来探索一下这种波动性，我们会选择24小时和一周的时间短来统计波动性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Selecting the id, percent_change_24h and percent_change_7d columns</span><br><span class="line">volatility = dec6[[&apos;id&apos;,&apos;percent_change_24h&apos;,&apos;percent_change_7d&apos;]]</span><br><span class="line"></span><br><span class="line"># Setting the index to &apos;id&apos; and dropping all NaN rows</span><br><span class="line">volatility = volatility.set_index(&apos;id&apos;).dropna()</span><br><span class="line"></span><br><span class="line"># Sorting the DataFrame by percent_change_24h in ascending order</span><br><span class="line">volatility = volatility.sort_values(by=&apos;percent_change_24h&apos;)</span><br><span class="line"></span><br><span class="line"># Checking the first few rows</span><br><span class="line">print(volatility.head())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">               percent_change_24h  percent_change_7d</span><br><span class="line">id                                                  </span><br><span class="line">flappycoin                 -95.85             -96.61</span><br><span class="line">credence-coin              -94.22             -95.31</span><br><span class="line">coupecoin                  -93.93             -61.24</span><br><span class="line">tyrocoin                   -79.02             -87.43</span><br><span class="line">petrodollar                -76.55             542.96</span><br></pre></td></tr></table></figure><p>第一名flappycoin，24小时内的波动为95%，这意味着1万块钱的投入24小时后只剩500.</p><p>的确，波动性如此之大的产品亏损起来是很快的，但高风险也意味着高收益，我们来看一下24小时内最大的赢家与输家的对比：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#Defining a function with 2 parameters, the series to plot and the title</span><br><span class="line">def top10_subplot(volatility_series, title):</span><br><span class="line">    # Making the subplot and the figure for two side by side plots</span><br><span class="line">    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))</span><br><span class="line">    </span><br><span class="line">    # Plotting with pandas the barchart for the top 10 losers</span><br><span class="line">    ax = volatility_series[:10].plot.bar(color=&apos;darkred&apos;,ax=axes[0])</span><br><span class="line">    </span><br><span class="line">    # Setting the figure&apos;s main title to the text passed as parameter</span><br><span class="line">    fig.suptitle(title)</span><br><span class="line">    </span><br><span class="line">    # Setting the ylabel to &apos;% change&apos;</span><br><span class="line">    ax.set_ylabel(&apos;% change&apos;)</span><br><span class="line">    </span><br><span class="line">    # Same as above, but for the top 10 winners</span><br><span class="line">    ax = volatility_series[-10:].plot.bar(color=&apos;darkblue&apos;,ax=axes[1])</span><br><span class="line">    </span><br><span class="line">    # Returning this for good practice, might use later</span><br><span class="line">    return fig, ax</span><br><span class="line"></span><br><span class="line">DTITLE = &quot;24 hours top losers and winners&quot;</span><br><span class="line"></span><br><span class="line"># Calling the function above with the 24 hours period series and title DTITLE  </span><br><span class="line">fig, ax = top10_subplot(volatility.percent_change_24h,&apos;24 hours top losers and winners&apos;)</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://i.loli.net/2019/12/06/6JMto7jFVD5gknu.png" alt=""></p><p>24小时百分之800的收益？为什么你还在看这篇文章不赶快去买比特币！（开玩笑😝）</p><p>我们再来看看周收益吧：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Sorting in ascending order</span><br><span class="line">volatility7d = volatility.sort_values(by=&apos;percent_change_7d&apos;)</span><br><span class="line"></span><br><span class="line">WTITLE = &quot;Weekly top losers and winners&quot;</span><br><span class="line"></span><br><span class="line"># Calling the top10_subplot function</span><br><span class="line">fig, ax = top10_subplot(volatility7d.percent_change_7d,WTITLE)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/06/tyx5TMCD8J3Hszh.png" alt=""></p><p>3500的收益，好了我不说什么了。</p><h1 id="关注低市值加密货币"><a href="#关注低市值加密货币" class="headerlink" title="关注低市值加密货币"></a>关注低市值加密货币</h1><p>上面的这些加密货币感觉都没见过，事实上这些加密货币的市值都很小,而投资低市值的加密货币风险也是很高的，这也印证了之前说的高风险高收益。</p><p>对于市值要怎么划分呢？多少可以划分成高市值，多少又是低市值呢？我们一般按下表来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Large cap: +10 billion</span><br><span class="line">Mid cap: 2 billion - 10 billion</span><br><span class="line">Small cap: 300 million - 2 billion</span><br><span class="line">Micro cap: 50 million - 300 million</span><br><span class="line">Nano cap: Below 50 million</span><br></pre></td></tr></table></figure><p>我们来看一下大于一百亿市值的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Selecting everything bigger than 10 billion </span><br><span class="line">largecaps = cap.query(&apos;market_cap_usd&gt;10000000000&apos;)</span><br><span class="line"></span><br><span class="line"># Printing out largecaps</span><br><span class="line">print(largecaps)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">             id  market_cap_usd</span><br><span class="line">0       bitcoin    2.130493e+11</span><br><span class="line">1      ethereum    4.352945e+10</span><br><span class="line">2  bitcoin-cash    2.529585e+10</span><br><span class="line">3          iota    1.475225e+10</span><br></pre></td></tr></table></figure><h1 id="大多数加密货币市值很低"><a href="#大多数加密货币市值很低" class="headerlink" title="大多数加密货币市值很低"></a>大多数加密货币市值很低</h1><p>通过将加密货币按照市值分类，进而对其按区间统计个数，我们可以看到🧍‍♂️市值区间加密货币的个数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Making a nice function for counting different marketcaps from the</span><br><span class="line"># &quot;cap&quot; DataFrame. Returns an int.</span><br><span class="line"># INSTRUCTORS NOTE: Since you made it to the end, consider it a gift :D</span><br><span class="line">def capcount(query_string):</span><br><span class="line">    return cap.query(query_string).count().id</span><br><span class="line"></span><br><span class="line"># Labels for the plot</span><br><span class="line">LABELS = [&quot;biggish&quot;, &quot;micro&quot;, &quot;nano&quot;]</span><br><span class="line"></span><br><span class="line"># Using capcount count the biggish cryptos</span><br><span class="line">biggish = capcount(&apos;market_cap_usd&gt;3000000000&apos;)</span><br><span class="line"></span><br><span class="line"># Same as above for micro ...</span><br><span class="line">micro = capcount(&apos;market_cap_usd&gt;50000000 and market_cap_usd&lt;300000000&apos;)</span><br><span class="line"></span><br><span class="line"># ... and for nano</span><br><span class="line">nano =  capcount(&apos;market_cap_usd&lt;50000000&apos;)</span><br><span class="line"></span><br><span class="line"># Making a list with the 3 counts</span><br><span class="line">values = [biggish,micro,nano]</span><br><span class="line"></span><br><span class="line"># Plotting them with matplotlib </span><br><span class="line">plt.bar(range(len(values)),values,label=LABELS)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/06/arEDO1QFAGVwqCB.png" alt=""></p><p>可以发现，大部分的加密货币市值都很小。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/06/UD9XlmW3fxoSIa5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;自从08年比特币发布以来，数以百计类似的基于区块链技术的产品层出不穷。我们称这些为加密货币，时至今日，某些加密货币已经大幅上涨，某些在未来可能也极具上涨空间。实际上，在2017年12月6日，比特币的市值超过2000亿美元。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="pandas" scheme="http://wittyfans.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>数据分析之母亲们的救星：因倡导洗手而被送去精神病院</title>
    <link href="http://wittyfans.com/coding/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E6%AF%8D%E4%BA%B2%E4%BB%AC%E7%9A%84%E6%95%91%E6%98%9F%EF%BC%9A%E5%9B%A0%E5%80%A1%E5%AF%BC%E6%B4%97%E6%89%8B%E8%80%8C%E8%A2%AB%E9%80%81%E5%8E%BB%E7%B2%BE%E7%A5%9E%E7%97%85%E9%99%A2.html"/>
    <id>http://wittyfans.com/coding/数据分析之母亲们的救星：因倡导洗手而被送去精神病院.html</id>
    <published>2019-12-06T04:53:06.000Z</published>
    <updated>2019-12-06T04:55:44.126Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>他被称为母亲们的救星，却因倡导洗手而被送往精神病院。</p></blockquote><a id="more"></a><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>伊格纳兹·菲利普·塞麦尔维斯（Ignatius Philipp Semmelweis，1818年7月1日－1865年8月13日），匈牙利产科医师，现代产科消毒法倡导者之一，被尊称为“母亲们的救星”。</p><p>他在维也纳（1847-1849年）和布达佩斯（1850年）医院产科工作时，经过细致观察，证实了产褥热是由于接生人员的手或器械受到污染传染产妇引起的败血症；于是他提倡使用漂白粉溶液消毒接生人员的手和器械；后来采用这种方法的医院产褥热死亡率显著减少。当他提出医生接生前要洗手时，冒犯了所有接生不洗手的同行，无法在医学界立足。最终被送去精神病院直到死亡。据验尸记录，他应为伤口感染败血症而死（有说法指，他是遭病院职员毒打的伤口感染而死；也有说法指出他是在给产褥热的病人做尸体解剖时划伤了自己的手指，感染了产褥热病菌而引发的败血症。）。</p><p>塞麦尔维斯当时留下了他记录下来的数据，我们今天来分析一下他的数据：</p><h1 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h1><p>我们先导入相关模块，并读取数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># importing modules</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># Read datasets/yearly_deaths_by_clinic.csv into yearly</span><br><span class="line">yearly = pd.read_csv(&apos;datasets/yearly_deaths_by_clinic.csv&apos;)</span><br><span class="line"></span><br><span class="line"># Print out yearly</span><br><span class="line">print(yearly)</span><br></pre></td></tr></table></figure><p>输出数据，year是记录的时间；births出现产褥热的人数，deaths是死亡人数，clinic有两种类型记录，1和2，说的是两个不同的诊所。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    year  births  deaths    clinic</span><br><span class="line">0   1841    3036     237  clinic 1</span><br><span class="line">1   1842    3287     518  clinic 1</span><br><span class="line">2   1843    3060     274  clinic 1</span><br><span class="line">3   1844    3157     260  clinic 1</span><br><span class="line">4   1845    3492     241  clinic 1</span><br><span class="line">5   1846    4010     459  clinic 1</span><br><span class="line">6   1841    2442      86  clinic 2</span><br><span class="line">7   1842    2659     202  clinic 2</span><br><span class="line">8   1843    2739     164  clinic 2</span><br><span class="line">9   1844    2956      68  clinic 2</span><br><span class="line">10  1845    3241      66  clinic 2</span><br><span class="line">11  1846    3754     105  clinic 2</span><br></pre></td></tr></table></figure><p>这里可以看到1841年到1846年产褥热的发病情况，这种疾病在当时是很危险的，死亡率非常高，大部分发生在妇女生完小孩之后。</p><h1 id="查看死亡率"><a href="#查看死亡率" class="headerlink" title="查看死亡率"></a>查看死亡率</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yearly[&apos;proportion_deaths&apos;] = yearly.deaths/yearly.births</span><br><span class="line">yearly1 = yearly[yearly.clinic==&apos;clinic 2&apos;]</span><br><span class="line">yearly2 = yearly[yearly.clinic==&apos;clinic 2&apos;]</span><br></pre></td></tr></table></figure><p>我们在数据中新增一列，保存死亡率，随后将两个诊所的数据分开。</p><p>先来看一下诊所1的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Print out yearly2</span><br><span class="line">print(yearly2)</span><br><span class="line"></span><br><span class="line">  year  births  deaths    clinic  proportion_deaths</span><br><span class="line">0  1841    3036     237  clinic 1           0.078063</span><br><span class="line">1  1842    3287     518  clinic 1           0.157591</span><br><span class="line">2  1843    3060     274  clinic 1           0.089542</span><br><span class="line">3  1844    3157     260  clinic 1           0.082357</span><br><span class="line">4  1845    3492     241  clinic 1           0.069015</span><br><span class="line">5  1846    4010     459  clinic 1           0.114464</span><br></pre></td></tr></table></figure><p>注意到1846年，死亡率居然有百分之11.</p><h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>我们来对比看看两个诊所数据，为了更直观的展示，我们将这些数据按线图绘制出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># This makes plots appear in the notebook</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"># Plot yearly proportion of deaths at the two clinics</span><br><span class="line">ax = yearly1.plot(x=&apos;year&apos;,y=&apos;proportion_deaths&apos;,label=&apos;clinic 1&apos;)</span><br><span class="line">yearly2.plot(x=&apos;year&apos;,y=&apos;proportion_deaths&apos;,label=&apos;clinic 2&apos;,ax=ax)</span><br><span class="line">ax.set_ylabel(&quot;Proportion deaths&quot;)</span><br></pre></td></tr></table></figure><p>下面是图像：</p><p><img src="https://i.loli.net/2019/12/06/vuSKmM9Vor4LxkF.png" alt=""></p><p>诊所1竟然比诊所2的数据高那么多！这是为什么？塞麦尔维斯决心要揭开这个谜，他通过平时的观察发现，这两个诊所之间唯一的区别就是医护人员，诊所1的医护人员同时也在验尸房工作，而诊所2的医护人员仅仅是接生。</p><p>当时还没有发现细菌，对于当时的人来说，根本不觉得从验尸房出来再接生需要洗手，根据这个发现，塞麦尔维斯开始让医护人员洗手，并记录了按月的发病与死亡相关数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Read datasets/monthly_deaths.csv into monthly</span><br><span class="line">monthly = pd.read_csv(&apos;datasets/monthly_deaths.csv&apos;,parse_dates=[&apos;date&apos;])</span><br><span class="line"></span><br><span class="line"># Calculate proportion of deaths per no. births</span><br><span class="line">monthly[&quot;proportion_deaths&quot;] = monthly.deaths/monthly.births</span><br><span class="line"></span><br><span class="line"># Print out the first rows in monthly</span><br><span class="line">print(monthly.head())</span><br></pre></td></tr></table></figure><h1 id="开始洗手"><a href="#开始洗手" class="headerlink" title="开始洗手"></a>开始洗手</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Plot monthly proportion of deaths</span><br><span class="line">ax = monthly.plot(x=&apos;date&apos;,y=&apos;proportion_deaths&apos;)</span><br><span class="line">ax.set_ylabel(&apos;Proportion deaths&apos;)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/06/rEW7sSuoAwdIxqm.png" alt=""></p><p>可以看到，医护人员开始洗手后，明显的降低了死亡率。我们将开始洗手前，开始洗手后的数据分开绘制，这样看的更明显:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Date when handwashing was made mandatory</span><br><span class="line">import pandas as pd</span><br><span class="line">handwashing_start = pd.to_datetime(&apos;1847-06-01&apos;)</span><br><span class="line"></span><br><span class="line"># Split monthly into before and after handwashing_start</span><br><span class="line">before_washing = monthly[monthly[&apos;date&apos;]&lt;handwashing_start]</span><br><span class="line">after_washing = monthly[monthly[&apos;date&apos;]&gt;=handwashing_start]</span><br><span class="line"></span><br><span class="line"># Plot monthly proportion of deaths before and after handwashing</span><br><span class="line">ax = before_washing.plot(x=&apos;date&apos;,y=&apos;proportion_deaths&apos;,label=&apos;before_washing&apos;)</span><br><span class="line">after_washing.plot(x=&apos;date&apos;,y=&apos;proportion_deaths&apos;,label=&apos;after_washing&apos;,ax=ax)</span><br><span class="line">ax.set_ylabel(&apos;Proportion deaths&apos;)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/06/EIRXBY7AMlvmQ9z.png" alt=""></p><p>我们再来看下，总体上洗手降低了多少的死亡率呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Difference in mean monthly proportion of deaths due to handwashing</span><br><span class="line">before_proportion = before_washing[&apos;proportion_deaths&apos;]</span><br><span class="line">after_proportion = after_washing[&apos;proportion_deaths&apos;]</span><br><span class="line">mean_diff = after_proportion.mean()-before_proportion.mean()</span><br><span class="line">mean_diff</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output: -0.08395660751183336</span><br></pre></td></tr></table></figure><h1 id="Bootstrap分析"><a href="#Bootstrap分析" class="headerlink" title="Bootstrap分析"></a>Bootstrap分析</h1><p>通过简单的洗手，竟然降低产褥热8%的死亡率！有人会说，洗手降低死亡率是不是只是一个随机出现的情况呢？为了去处不确定性的影响，了解洗手能减少多少死亡率，我们可以看一下置信区间。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># A bootstrap analysis of the reduction of deaths due to handwashing</span><br><span class="line">boot_mean_diff = []</span><br><span class="line">for i in range(3000):</span><br><span class="line">    boot_before = before_proportion.sample(frac=1,replace=True)</span><br><span class="line">    boot_after = after_proportion.sample(frac=1,replace=True)</span><br><span class="line">    boot_mean_diff.append(boot_after.mean()-boot_before.mean())</span><br><span class="line"></span><br><span class="line"># Calculating a 95% confidence interval from boot_mean_diff </span><br><span class="line">confidence_interval = pd.Series(boot_mean_diff).quantile([0.025, 0.975])</span><br><span class="line">confidence_interval</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output:</span><br><span class="line">0.025   -0.100879</span><br><span class="line">0.975   -0.066952</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>这里可以看出根据95%的置信区间，<strong>通过洗手，大约降低了6.7 到 10死亡率</strong>。</p><p>对于<code>pandas</code>的<code>sample</code>方法，这里解释一下：</p><p>sample会返回随机选择数据中一个子集，frac可以指定子集的大小，如果不指定则默认返回一条记录，如果指定为1，则返回所有的结果。</p><p>我们把这个比喻为从一个盒子里拿1000个球，那么就是不放回的一个一个从盒子里取，直到取完1000个为止，看一个下面的例子。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame([x for x in range(1000)])</span><br><span class="line">dff = df.sample(frac=1)</span><br><span class="line">dff.columns=[&apos;values&apos;]</span><br><span class="line">dff[&apos;values&apos;].value_counts()</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">999    1</span><br><span class="line">328    1</span><br><span class="line">341    1</span><br><span class="line">340    1</span><br><span class="line">339    1</span><br><span class="line">      ..</span><br><span class="line">661    1</span><br><span class="line">660    1</span><br><span class="line">659    1</span><br><span class="line">658    1</span><br><span class="line">0      1</span><br><span class="line">Name: values, Length: 1000, dtype: int64</span><br></pre></td></tr></table></figure><p>可以看到输出就是我们拿出来的球，每样一个。如果像上面产褥热的数据中一样指定<code>replace=True</code>,那就变成了有放回的拿了，也就是说我拿一个球，记录它是多少号，再放回去，这样重复一千次，看下面的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame([x for x in range(1000)])</span><br><span class="line">dff = df.sample(frac=1,replace=True)</span><br><span class="line">dff.columns=[&apos;values&apos;]</span><br><span class="line">dff[&apos;values&apos;].value_counts()</span><br></pre></td></tr></table></figure><p>因为是有放回的拿，所以肯定会有重复的值出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">48     5</span><br><span class="line">553    4</span><br><span class="line">479    4</span><br><span class="line">89     4</span><br><span class="line">852    4</span><br><span class="line">      ..</span><br><span class="line">435    1</span><br><span class="line">436    1</span><br><span class="line">784    1</span><br><span class="line">439    1</span><br><span class="line">338    1</span><br><span class="line">Name: values, Length: 630, dtype: int64</span><br></pre></td></tr></table></figure><p>这里48号球出现了5次，其余的球也出现了重复的记录,在这里我们是需要有放回的,有放回的抽样第一次抽取的结果不影响第二次的结果，它们彼此是独立的，从数学上来说，它们之间没有相关性。</p><blockquote><p>By default, pandas’ sample randomly selects rows without replacement. Sampling with replacement is very useful for statistical techniques like bootstrapping</p></blockquote><h1 id="悲剧"><a href="#悲剧" class="headerlink" title="悲剧"></a>悲剧</h1><p>悲剧的是，尽管有证据显示塞梅尔维斯的理论是正确的，即产褥热是由尸检室尸体中的某种“物质”（今天我们称为细菌）引起的。当这却被当时的科学家嘲笑。</p><p>医学界在很大程度上拒绝了他的发现，1849年他被迫永久离开维也纳综合医院。</p><p>原因之一是统计和统计思维在1800年代医学界并不常见。 塞梅尔维斯仅将其数据发布为原始数据，但他未显示任何图形或置信区间。 如果他能够获得我们刚刚汇总的分析结果，那么他可能会更成功地让维也纳的医生洗手。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;他被称为母亲们的救星，却因倡导洗手而被送往精神病院。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Python神经网络分析基础</title>
    <link href="http://wittyfans.com/coding/Python%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80.html"/>
    <id>http://wittyfans.com/coding/Python神经网络分析基础.html</id>
    <published>2019-12-04T08:30:41.000Z</published>
    <updated>2019-12-04T08:37:19.708Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/12/04/b8mhdtkcqeSiwEg.png" alt=""></p><blockquote><p>今天我们学习神经网络与nexworkX.</p></blockquote><a id="more"></a><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="网络基础"><a href="#网络基础" class="headerlink" title="网络基础"></a>网络基础</h1><p>什么是神经网络呢？我们可以想象它是我们的社交网络，或者是我们的高铁运输网络，神经网络对于模拟 <em>实体之间的关系</em> 非常有用，通过这种模拟，你可以了解到，在整个网络中，哪一个实体（或者说节点node）是比较重要的,各个实体之间的联系，实体之间的距离怎么走最短。</p><p>对于网络 <em>graph</em>，是由节点与关系组成的，节点叫做 <em>node</em>,关系叫做 <em>edge</em>.</p><p>网络中的<em>node</em>与<em>edge</em>都可以有自己的属性。来看下面这张社交关系的图：</p><p><img src="https://i.loli.net/2019/12/01/dlXxE2yc73U8Lez.png" alt=""></p><p>图中Hugo和Eric是朋友，他们各自都有各自的属性</p><ul><li>id</li><li>age</li></ul><p>他们是朋友，所以关系edge即 Friendship，关系也有属性，标注了他们第一次成为朋友的时间。</p><h1 id="NetworkX基础"><a href="#NetworkX基础" class="headerlink" title="NetworkX基础"></a>NetworkX基础</h1><p>在python中，我们如何表示网络与网络节点之间的关系呢？在python中，我们使用networkX这个包。下面建立一个网络的基本代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import networkx as nx</span><br><span class="line">G = nx.Graph()</span><br></pre></td></tr></table></figure><p>G即为一个网络，可以通过 <code>‌G.add_nodes_from([1, 2, 3])</code>往其中添加节点，节点添加进去以后，就可以使用<code>G.nodes()</code>来查看节点，此处会返回这个网络的所有节点。</p><p>那关系又要怎么定义呢？我们可以直接使用G.add_edge(1, 2) 方法将两个实体连接起来，这会在节点1与2之间创建关系。</p><p>前面提到了，我们的节点可以有属性，给节点的属性赋值很简单，直接赋值即可：<code>G.node[1][&#39;label&#39;] = &#39;blue&#39;</code>.</p><p>如果你想知道网络的节点有哪些属性，通过<code>G.nodes(data=True)</code>即可查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Out: [(1, &#123;&apos;label&apos;: &apos;blue&apos;&#125;), (2, &#123;&#125;), (3, &#123;&#125;)]</span><br></pre></td></tr></table></figure><h1 id="图的类型"><a href="#图的类型" class="headerlink" title="图的类型"></a>图的类型</h1><p>图的类型简单就是说节点之间的关系，举例来说，Facebook是undirected graphs,当一个人与另一个人成为朋友之后，这两个人自动就连接到了一起，在这种关系中，两个节点知识连接到了一起，没有方向性。undirected graphs类型的图在networkx中的类型就是graph，当你用<code>type</code>命令检测它的类型时,会显示<code>networkx.classes.graph.Graph</code>.</p><p>另外第一种是Directed grarphs,这种图里的节点就好像twitter中的用户之间的关系，比如一个用户也许关注了一个人，但是另外一个人可能却没有关注它，这时候两者的关系就是单向的关系，这两个node之间的关系我们会用一个箭头表示，这种类型的图在networkx中用<code>networkx.classes.graph.Digraph</code>表示。</p><p>两个节点之间的关系也会有多条edge存在的情况,比如两个站点之间所有的路线，这种图叫做MutilDIGraph。</p><p>对于两个两个节点之间，如果存在多个edge，这会导致计算量变得很大，这时候我们会将它们之间edge的管家信息保存成edge的属性，方便计算，比如将3条edge合并成1条，同时指定这条edge的metadata为3.</p><p>需要注意，在我们的车站的例子中，还有一种情况是我们的起点与终点是同一个站，这种特殊的情况叫做self-loops.</p><h1 id="绘制图"><a href="#绘制图" class="headerlink" title="绘制图"></a>绘制图</h1><p>我们使用 nxviz 这个包来绘制接下来介绍的三种图，Matrix、arc和Circos图，强烈推荐你安装，你可以通过pip<br><code>pip install nxviz</code> 或者conda命令<code>conda install -c conda-forge nxviz</code>安装，<a href="https://pypi.org/project/nxviz/" target="_blank" rel="noopener">更多</a>关于nxviz的资料.</p><h2 id="Matrix-plots"><a href="#Matrix-plots" class="headerlink" title="Matrix plots"></a>Matrix plots</h2><p>Matrix plots图是一种用矩阵方格来表示节点之间关系的图，左边是Matrix图，右边是我们的节点关系。</p><p><img src="https://i.loli.net/2019/12/02/d78ojGXvCaFiJTE.png" alt=""></p><ul><li>A-A是白的，表示它与它自己没有联系</li><li>A-B是黑色的，表示A指向B之间存在联系</li><li>B-A是白色的，表示B指向A无联系</li></ul><p>每个节点都对应一列与一行，两个节点之间的边由值1表示。但是，这样做仅保留了weight元数据（edge上的属性）；</p><p>下面是使用nxviz绘制graph对象T的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Import nxviz</span><br><span class="line">import nxviz as nv</span><br><span class="line"></span><br><span class="line"># Create the MatrixPlot object: m</span><br><span class="line">m = nv.MatrixPlot(T)</span><br><span class="line"></span><br><span class="line"># Draw m to the screen</span><br><span class="line">m.draw()</span><br><span class="line"></span><br><span class="line"># Display the plot</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Convert T to a matrix format: A</span><br><span class="line">A = nx.to_numpy_matrix(T)</span><br><span class="line"></span><br><span class="line"># Convert A back to the NetworkX form as a directed graph: T_conv</span><br><span class="line">T_conv = nx.from_numpy_matrix(A, create_using=nx.DiGraph())</span><br><span class="line"></span><br><span class="line"># Check that the `category` metadata field is lost from each node</span><br><span class="line">for n, d in T_conv.nodes(data=True):</span><br><span class="line">    assert &apos;category&apos; not in d.keys()</span><br></pre></td></tr></table></figure><p>图像如下：</p><p><img src="https://i.loli.net/2019/12/02/5VGJYM3B8kzFA2q.png" alt="Matrix plots with nxviz"></p><h2 id="Circos-plots"><a href="#Circos-plots" class="headerlink" title="Circos plots"></a>Circos plots</h2><p>Circos plots是一种围绕着圆心绘制的图，看起来非常美观。</p><p><img src="https://i.loli.net/2019/12/02/iPw4r5xAB6lYds8.png" alt=""></p><p>绘制上图的代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Import necessary modules</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from nxviz import CircosPlot</span><br><span class="line"></span><br><span class="line"># Create the CircosPlot object: c</span><br><span class="line">c = CircosPlot(T)</span><br><span class="line"></span><br><span class="line"># Draw c to the screen</span><br><span class="line">c.draw()</span><br><span class="line"></span><br><span class="line"># Display the plot</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Arc-plots"><a href="#Arc-plots" class="headerlink" title="Arc plots"></a>Arc plots</h2><p>Arc plots即弧形图，它就好像把Circos plots展开成一条线，正因如此它需要我们指定排序。</p><p><img src="https://i.loli.net/2019/12/02/qe7yQNREpa5KzhT.png" alt=""></p><p>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Import necessary modules</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from nxviz import ArcPlot</span><br><span class="line"></span><br><span class="line"># Create the un-customized ArcPlot object: a</span><br><span class="line">a = ArcPlot(T)</span><br><span class="line"></span><br><span class="line"># Draw a to the screen</span><br><span class="line">a.draw()</span><br><span class="line"></span><br><span class="line"># Display the plot</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Create the customized ArcPlot object: a2</span><br><span class="line">a2 = ArcPlot(T,node_order=&apos;category&apos;,node_color=&apos;category&apos;)</span><br><span class="line"></span><br><span class="line"># Draw a2 to the screen</span><br><span class="line">a2.draw()</span><br><span class="line"></span><br><span class="line"># Display the plot</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><code>node_order=&#39;keyX&#39;</code> 与 <code>node_color=&#39;keyX&#39;</code> 即指定这个图的排序方式与颜色区分参考选项。</p><h1 id="重要的节点"><a href="#重要的节点" class="headerlink" title="重要的节点"></a>重要的节点</h1><p>如果我们需要分析图，我们就需要对图的节点有所了解，对于一张图来说，其中有重要的节点，也有不那么重要的节点，比如对于地铁来说，那些市中心的换乘站比其他线路的终点站要重要，接下来我们介绍几个与节点有关的概念。</p><h2 id="Degree-Centrality"><a href="#Degree-Centrality" class="headerlink" title="Degree Centrality"></a>Degree Centrality</h2><p>Degree即一个node拥有多少个邻居，Degree Centrality即根据图中节点邻居的个数来给每个节点打分，如果一个节点连接的邻居更多，那它就更重要，分数也更高，Degree Centrality的定义为:</p><p>$$\frac{Neighbors\ I\ Have}{Neighbors\ I\ Could\ Possible\ Have}$$</p><p>这里的 <em>Neighbors I could possible have</em> 就是所有的节点，如果我们的讨论氛围允许self-loops存在，则它也包括我们自己，如果不允许，则只算所有除我之外的节点。</p><p>在networkx中，你可以使用 <code>G.neighbors(node_name)</code> 来查看一个节点拥有多少个邻居，如果你传进一个没有的节点，networkx则会抛出一个错误。</p><p>当我们有了所有节点的邻居数，就可以根据它来计算每个节点在图中的重要性，你可以使用 <em>nx.degree_centrality(G)</em> 来查看所有节点的Degree Centrality分数，注意这里分数的计算中，不包含self-loops的情况。</p><p>我们定义一个函数，它会找到我们网络中邻居数最高的节点,有了它我们就可以很方便的处理一些问题，如找到Twitter用户中，那些最有影响力的用户：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># Define find_nodes_with_highest_deg_cent()</span><br><span class="line">def find_nodes_with_highest_deg_cent(G):</span><br><span class="line"></span><br><span class="line">    # Compute the degree centrality of G: deg_cent</span><br><span class="line">    deg_cent = nx.degree_centrality(G)</span><br><span class="line"></span><br><span class="line">    # Compute the maximum degree centrality: max_dc</span><br><span class="line">    max_dc = max(list(deg_cent.values()))</span><br><span class="line"></span><br><span class="line">    nodes = set()</span><br><span class="line"></span><br><span class="line">    # Iterate over the degree centrality dictionary</span><br><span class="line">    for k, v in deg_cent.items():</span><br><span class="line"></span><br><span class="line">        # Check if the current value has the maximum degree centrality</span><br><span class="line">        if v == max_dc:</span><br><span class="line"></span><br><span class="line">            # Add the current node to the set of nodes</span><br><span class="line">            nodes.add(k)</span><br><span class="line"></span><br><span class="line">    return nodes</span><br><span class="line"></span><br><span class="line"># Find the node(s) that has the highest degree centrality in T: top_dc</span><br><span class="line">top_dc = find_nodes_with_highest_deg_cent(T)</span><br><span class="line">print(top_dc)</span><br><span class="line"></span><br><span class="line"># Write the assertion statement</span><br><span class="line">for node in top_dc:</span><br><span class="line">    assert nx.degree_centrality(T)[node] == max(nx.degree_centrality(T).values())</span><br></pre></td></tr></table></figure><h2 id="Path-finding"><a href="#Path-finding" class="headerlink" title="Path finding"></a>Path finding</h2><p>path finding有很多的应用范围，比如两个车站之间最短路线的寻找，消息、病毒的传播，我们知道两个节点之间有很多种路线可以选择，那么要如何找到最短的这条路线呢？</p><p>一种解决方案是，使用Bread-first search算法，它的原理如下图：</p><p>我们从黄色的节点开始，第一次找黄色节点的邻居，并检查目标节点是否在当前节点的邻居中，如果不存在就继续搜寻下一层节点，找到为止。</p><p>我们首先定一个寻找节点之间路径的算法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Define path_exists()</span><br><span class="line">def path_exists(G, node1, node2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    This function checks whether a path exists between two nodes (node1, node2) in graph G.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    visited_nodes = set()</span><br><span class="line"></span><br><span class="line">    # Initialize the queue of nodes to visit with the first node: queue</span><br><span class="line">    queue = [node1]</span><br><span class="line"></span><br><span class="line">    # Iterate over the nodes in the queue</span><br><span class="line">    for node in queue:</span><br><span class="line"></span><br><span class="line">        # Get neighbors of the node</span><br><span class="line">        neighbors = G.neighbors(node)</span><br><span class="line"></span><br><span class="line">        # Check to see if the destination node is in the set of neighbors</span><br><span class="line">        if node2 in neighbors:</span><br><span class="line">            print(&apos;Path exists between nodes &#123;0&#125; and &#123;1&#125;&apos;.format(node1, node2))</span><br><span class="line">            return True</span><br><span class="line">            break</span><br></pre></td></tr></table></figure><p>这个算法会确定两个节点之间是否存在路径，仔细查看上面的代码，如果两个节点直接存在路径，该函数就返回true，如果不存在呢？不存在我们则需要对剩下的节点进一步的搜寻，让我们把剩下的代码补全：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def path_exists(G, node1, node2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    This function checks whether a path exists between two nodes (node1, node2) in graph G.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    visited_nodes = set()</span><br><span class="line">    queue = [node1]</span><br><span class="line"></span><br><span class="line">    for node in queue:</span><br><span class="line">        neighbors = G.neighbors(node)</span><br><span class="line">        if node2 in neighbors:</span><br><span class="line">            print(&apos;Path exists between nodes &#123;0&#125; and &#123;1&#125;&apos;.format(node1, node2))</span><br><span class="line">            return True</span><br><span class="line">          </span><br><span class="line">        else:</span><br><span class="line">            # Add current node to visited nodes</span><br><span class="line">            visited_nodes.add(node)</span><br><span class="line"></span><br><span class="line">            # Add neighbors of current node that have not yet been visited</span><br><span class="line">            queue.extend([n for n in neighbors if n not in visited_nodes])</span><br></pre></td></tr></table></figure><p>如果函数中的queue已经空了，也就是找完了了都没找到路径，那我们也要返回结果：即这两个节点之间没有路径存在：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def path_exists(G, node1, node2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    This function checks whether a path exists between two nodes (node1, node2) in graph G.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    visited_nodes = set()</span><br><span class="line">    queue = [node1]</span><br><span class="line"></span><br><span class="line">    for node in queue:</span><br><span class="line">        neighbors = G.neighbors(node)</span><br><span class="line">        if node2 in neighbors:</span><br><span class="line">            print(&apos;Path exists between nodes &#123;0&#125; and &#123;1&#125;&apos;.format(node1, node2))</span><br><span class="line">            return True</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            visited_nodes.add(node)</span><br><span class="line">            queue.extend([n for n in neighbors if n not in visited_nodes])</span><br><span class="line"></span><br><span class="line">        # Check to see if the final element of the queue has been reached</span><br><span class="line">        if node == queue[-1]:</span><br><span class="line">            print(&apos;Path does not exist between nodes &#123;0&#125; and &#123;1&#125;&apos;.format(node1, node2))</span><br><span class="line"></span><br><span class="line">            # Place the appropriate return statement</span><br><span class="line">            return False</span><br></pre></td></tr></table></figure><h2 id="Betweenness-centrality"><a href="#Betweenness-centrality" class="headerlink" title="Betweenness centrality"></a>Betweenness centrality</h2><p>Betweenness centrality这个概念与最短路径息息相关，上面我们已经定义了如何找到最短路径的算法，那么对于我们图中所有的节点对（两个节点），都有它们的最短路径，这些最短路径通过的节点中，哪些节点的通过数是最高的，则它的重要性就更高，这就是Betweenness centrality的意思，它的定义为：</p><p>$$\frac{num.\ shortest\ paths\ through\ node}{all\ possible\ shortest\ paths}$$</p><p>Betweenness centrality对于找到两个群体之间联系必须通过的那些实体很有帮助，比如那些帮助两个党派之间的联系的，充当桥梁作用的人，互联网中两个地区之间的网络流量交流，确定那些很重要的节点。</p><p>Betweenness centrality可以直接通过nx的方法计算得到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bet_cen = nx.betweenness_centrality(T)</span><br></pre></td></tr></table></figure><h1 id="Communities-amp-cliques"><a href="#Communities-amp-cliques" class="headerlink" title="Communities &amp; cliques"></a>Communities &amp; cliques</h1><p>cliques即我们所说的小团体、小集团，事实上这个说法就来源于我们的社交生活，在我们所说的小团体中，我们认识其他所有人，而cliques也是这样，每个一个节点都与其他的节点都有连接。</p><p>举例，一个最简单的cliques就是一个三角形，在Facebook的社交网络中，如果a认识b，b认识c，如果c也认识a，那么这就是一个三角形，但是如果不认识，我们就可以推荐他们认识，如何找到这些缺了一条联系就可以组成三角形的关系呢？</p><p>我们定义一个函数，它接受两个参数，G：一个网络，n一个节点，这个函数会确定n节点在G网络中，是否存在<strong>三角关系</strong>,即: node <code>n</code> in graph <code>G</code> is in a triangle relationship or not.</p><p>在我们的算法中，我们根据传入的网络，找到n的所有邻居，然后遍历其所有邻居对，如果其中有某一对存在edge关系，那么这就组成了一个三角关系（因为这两者已经是n的邻居，而这两者之间又有关系）</p><p>算法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from itertools import combinations</span><br><span class="line"></span><br><span class="line"># Define is_in_triangle()</span><br><span class="line">def is_in_triangle(G, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Checks whether a node `n` in graph `G` is in a triangle relationship or not.</span><br><span class="line"></span><br><span class="line">    Returns a boolean.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    in_triangle = False</span><br><span class="line"></span><br><span class="line">    # Iterate over all possible triangle relationship combinations</span><br><span class="line">    for n1, n2 in combinations(G.neighbors(n),2):</span><br><span class="line"></span><br><span class="line">        # Check if an edge exists between n1 and n2</span><br><span class="line">        if G.has_edge(n1,n2):</span><br><span class="line">            in_triangle = True</span><br><span class="line">            break</span><br><span class="line">    return in_triangle</span><br></pre></td></tr></table></figure><blockquote><p>combinations是一个遍历工具，用于组合节点的所有邻居对。</p></blockquote><p>我们修改一下，找出所有的三角关系:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from itertools import combinations</span><br><span class="line"></span><br><span class="line"># Write a function that identifies all nodes in a triangle relationship with a given node.</span><br><span class="line">def nodes_in_triangle(G, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Returns the nodes in a graph `G` that are involved in a triangle relationship with the node `n`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    triangle_nodes = set([n])</span><br><span class="line"></span><br><span class="line">    # Iterate over all possible triangle relationship combinations</span><br><span class="line">    for n1, n2 in combinations(G.neighbors(n),2):</span><br><span class="line"></span><br><span class="line">        # Check if n1 and n2 have an edge between them</span><br><span class="line">        if G.has_edge(n1,n2):</span><br><span class="line"></span><br><span class="line">            # Add n1 to triangle_nodes</span><br><span class="line">            triangle_nodes.add(n1)</span><br><span class="line"></span><br><span class="line">            # Add n2 to triangle_nodes</span><br><span class="line">            triangle_nodes.add(n2)</span><br><span class="line"></span><br><span class="line">    return triangle_nodes</span><br><span class="line"></span><br><span class="line"># Write the assertion statement</span><br><span class="line">assert len(nodes_in_triangle(T, 1)) == 35</span><br></pre></td></tr></table></figure><p>如果你想要做一个好友推荐系统，那么你可能需要找出那些open triangle关系，即我们刚才提到的，a认识b，b认识c，而a并不认识c，让我们写一个找这种open triangle关系的函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from itertools import combinations</span><br><span class="line"></span><br><span class="line"># Define node_in_open_triangle()</span><br><span class="line">def node_in_open_triangle(G, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Checks whether pairs of neighbors of node `n` in graph `G` are in an &apos;open triangle&apos; relationship with node `n`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    in_open_triangle = False</span><br><span class="line"></span><br><span class="line">    # Iterate over all possible triangle relationship combinations</span><br><span class="line">    for n1, n2 in combinations(G.neighbors(n),2):</span><br><span class="line"></span><br><span class="line">        # Check if n1 and n2 do NOT have an edge between them</span><br><span class="line">        if not G.has_edge(n1,n2):</span><br><span class="line"></span><br><span class="line">            in_open_triangle = True</span><br><span class="line"></span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    return True</span><br></pre></td></tr></table></figure><p>我们可以利用这个函数来查看一个图中，有多少个open triangle:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Compute the number of open triangles in T</span><br><span class="line">num_open_triangles = 0</span><br><span class="line"></span><br><span class="line"># Iterate over all the nodes in T</span><br><span class="line">for n in T.nodes():</span><br><span class="line"></span><br><span class="line">    # Check if the current node is in an open triangle</span><br><span class="line">    if node_in_open_triangle(T,n):</span><br><span class="line"></span><br><span class="line">        # Increment num_open_triangles</span><br><span class="line">        num_open_triangles += 1</span><br><span class="line"></span><br><span class="line">print(num_open_triangles)</span><br></pre></td></tr></table></figure><h2 id="Maximal-cliques"><a href="#Maximal-cliques" class="headerlink" title="Maximal cliques"></a>Maximal cliques</h2><p>我们已经有了cliques的概念，maximal cliques的意思是，在一个团体中，我们增加一个node，如果整个图像中的节点还是彼此互相连接着的，那么这个cliques还是一个cliques，但如果我们增加node后，整个图像的节点不再是彼此连接着，那增加前的图像就是我们的maximal cliques.</p><p>nexworkx提供了一个<code>find_cliques</code>方法来寻找所有的cliques与Maximal cliques。</p><p>下面是对<code>find_cliques</code>方法的一个实践，我们定义了一个函数用来寻找一个图中拥有size数的max_cliques的个数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Define maximal_cliques()</span><br><span class="line">def maximal_cliques(G, size):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Finds all maximal cliques in graph `G` that are of size `size`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    mcs = []</span><br><span class="line">    for clique in nx.find_cliques(G):</span><br><span class="line">        if len(clique) == size:</span><br><span class="line">            mcs.append(clique)</span><br><span class="line">    return mcs</span><br><span class="line"></span><br><span class="line"># Check that there are 33 maximal cliques of size 3 in the graph T</span><br><span class="line">assert len(maximal_cliques(T, 3)) == 33</span><br></pre></td></tr></table></figure><h2 id="Subgraphs"><a href="#Subgraphs" class="headerlink" title="Subgraphs"></a>Subgraphs</h2><p>Subgraphs即子图的意思，对于一个图，我们需要对一些感兴趣的信息突出展示，这时候就需要用到子图。</p><p>下面是绘制子图的一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 导入包</span><br><span class="line">In [1]: import networkx as nx</span><br><span class="line"># 创建图</span><br><span class="line">In [2]: G = nx.erdos_renyi_graph(n=20, p=0.2)</span><br><span class="line"># 查看节点</span><br><span class="line">In [3]: G.nodes()</span><br><span class="line">Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,</span><br><span class="line">17, 18, 19]</span><br><span class="line"></span><br><span class="line"># 将上面图中的节点8复制给nodes</span><br><span class="line">In [4]: nodes = G.neighbors(8)</span><br><span class="line">In [5]: nodes</span><br><span class="line">Out[5]: [2, 3, 4, 10]</span><br><span class="line">In [6]: nodes.append(8)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 将子图添加到G</span><br><span class="line">In [7]: G_eight = G.subgraph(nodes)</span><br><span class="line"></span><br><span class="line"># 查看G_eight的edges</span><br><span class="line">In [8]: G_eight.edges()</span><br><span class="line">Out[8]: [(8, 2), (8, 3), (8, 4), (8, 10), (2, 10)]</span><br><span class="line">In [9]: G_eight</span><br><span class="line">Out[9]: &lt;networkx.classes.graph.Graph at 0x10cae39e8&gt;</span><br><span class="line">In [10]: G</span><br><span class="line">Out[10]: &lt;networkx.classes.graph.Graph at 0x10cad1f60&gt;</span><br></pre></td></tr></table></figure><p>效果:</p><p><img src="https://i.loli.net/2019/12/04/JzZorRNK7sd95X1.png" alt=""></p><p>因为绘制子图的操作经常要用到，我们可以写一个函数来简化这个步骤，这个函数接受一个图与节点数组（即感兴趣的节点）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Define get_nodes_and_nbrs()</span><br><span class="line">def get_nodes_and_nbrs(G, nodes_of_interest):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Returns a subgraph of the graph `G` with only the `nodes_of_interest` and their neighbors.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    nodes_to_draw = []</span><br><span class="line"></span><br><span class="line">    # Iterate over the nodes of interest</span><br><span class="line">    for n in nodes_of_interest:</span><br><span class="line"></span><br><span class="line">        # Append the nodes of interest to nodes_to_draw</span><br><span class="line">        nodes_to_draw.append(n)</span><br><span class="line"></span><br><span class="line">        # Iterate over all the neighbors of node n</span><br><span class="line">        for nbr in G.neighbors(n):</span><br><span class="line"></span><br><span class="line">            # Append the neighbors of n to nodes_to_draw</span><br><span class="line">            nodes_to_draw.append(nbr)</span><br><span class="line"></span><br><span class="line">    return G.subgraph(nodes_to_draw)</span><br></pre></td></tr></table></figure><p>测试一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nodes_of_interest = [29, 38, 42]</span><br><span class="line"></span><br><span class="line"># Extract the subgraph with the nodes of interest: T_draw</span><br><span class="line">T_draw = get_nodes_and_nbrs(T,nodes_of_interest)</span><br><span class="line"></span><br><span class="line"># Draw the subgraph to the screen</span><br><span class="line">nx.draw(T_draw)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/12/04/JqXiLphy4bBcFaZ.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/12/04/b8mhdtkcqeSiwEg.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;今天我们学习神经网络与nexworkX.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="nexworkX" scheme="http://wittyfans.com/tags/nexworkX/"/>
    
  </entry>
  
  <entry>
    <title>机器学习：泛化误差与随机森林</title>
    <link href="http://wittyfans.com/coding/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.html"/>
    <id>http://wittyfans.com/coding/机器学习：泛化误差与随机森林.html</id>
    <published>2019-11-24T13:03:54.000Z</published>
    <updated>2019-11-24T13:07:18.541Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/24/xpv9ZiqB734oNAO.png" alt=""></p><blockquote><p>这一篇文章我们来了解误差，如何处理误差，以及学习随机森林的基础知识。</p></blockquote><a id="more"></a><p>机器学习：泛化误差与随机森林</p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="泛化误差-Generalization-Error"><a href="#泛化误差-Generalization-Error" class="headerlink" title="泛化误差 (Generalization Error)"></a>泛化误差 (Generalization Error)</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>我们知道在监督学习的线性模型中，我们数据分布符合下面的函数，即:</p><p>$$y=f(x)$$</p><p>其中，y和x是已知的，而 <em>f</em> 是未知的。</p><p>同时在我们的数据分布中，数据集不可能完全按照我们的函数分布，总是有些点（噪音）事覆盖在我们的函数周边：</p><p><img src="https://i.loli.net/2019/11/22/V1xpq4uPFtJl6XM.png" alt=""></p><p>当你训练你的模型的时候，你希望这些噪音尽可能的去除，同时预测的结果尽可能少出错。</p><p>为了实现这个目标，你会遇到两个难题，即过度拟合与欠拟合。</p><p>过度拟合即我们的模型过于敏感，欠拟合即模型过于迟钝，如下面两张图图所示：</p><p><img src="https://i.loli.net/2019/11/22/JzMd6FZbENkY23s.png" alt="过度拟合-Overfitting"></p><p><img src="https://i.loli.net/2019/11/22/1mp6JLoCWhESyKG.png" alt="欠拟合-Underfitting"></p><p>什么是泛化误差呢？这是一种衡量我们模型与数据分布之间误差，对于一个模型的泛化误差，即它对于新的数据的预测中的误差，由三个部分组成：</p><p>$$\hat{f}=bias^2+variance+irreducible\ error$$</p><p>先看第一个参数，bias。</p><p>下面是一组老鼠的体重与体长的分布数据，我们把它分割成train组与test组，我们在train组中训练我们的模型得到一条直线，可以看到数据集的分布是一条曲线，而当我们的数据集分布是一个曲线的时候，我们用一条直线是无法完整的描述数据集的分布的，无论我们怎么调整这根直线，这种情况就叫做bias:</p><p><img src="https://i.loli.net/2019/11/24/VocITXg4FG23kWY.png" alt=""></p><p>另外，我们模型也可能是生成一条曲线，而这条曲线完美的穿过了我们的train数据集：</p><p><img src="https://i.loli.net/2019/11/24/TvAPh3HNk2uOE1o.png" alt=""></p><p>当我们用最小二乘法去衡量模型的好坏时，无疑第二个模型是最完美的，它与每个点的的距离都是0。</p><p>但是，当我们该曲线与test组的数据放到一起时，情况可能就不一样了，这就引出了variance的定义：</p><p><img src="https://i.loli.net/2019/11/24/1qIShcuvRgAj5zr.png" alt=""></p><p><img src="https://i.loli.net/2019/11/24/AF2SmliXMGOZtHo.png" alt=""></p><p><img src="https://i.loli.net/2019/11/24/eKsd9pgExNOlLmB.png" alt=""></p><p>现在知道了bias以及variance，我们就知道如何去优化模型了，<br>我们需要均衡设置这三个参数，让他们各自都在一个最合适的位置：</p><p><img src="https://i.loli.net/2019/11/22/JTEIruvo58VG6Mn.png" alt=""></p><h2 id="处理误差"><a href="#处理误差" class="headerlink" title="处理误差"></a>处理误差</h2><p>我们尽可能的使这些误差最小，但这并不容易，首先 <em>f</em> 函数我们不知道，你只有一对数据，并且噪声事无法避免的。</p><p>怎么办呢？还是按照以前的方法，将数据分为train组和test组，而且我们需要使用之前介绍过的 <em>cross-validation</em> 技术，CV。</p><p>CV有两种：</p><ul><li>K-Fold CV</li><li>Hold-Out CV</li></ul><p>在这里，我们只介绍K-Fold CV, Cross-validation技术我们之前已经有过介绍，可以查看之前的文章了解详情。</p><p>在我们对数据进行fold的时，每一次fold都可以算出一次error，随后再算总的fold error平均，我们来看一个10fold的公式，其中E为每一次fold的error：</p><p>$$CV_{error}=\frac{E_1+…+E_{10}}{10}$$</p><p><img src="https://i.loli.net/2019/11/24/J1u4OSiMqdWrwIa.png" alt=""></p><p>如果我们CV后的方差比train组的值要高，那就说明我们我们过度拟合了，在tree model上来看就是说我们无视了那些应该视为leaf的节点，而将其继续拆分了。如果你发现过度拟合的情况，应该降低你的模型的复杂性，比如将模型的 <em>Max depth</em> 降低，收集更多的数据。</p><p>如果我们的bias过高，即CV后的bias值与train组的值近似或者大于它，这意味着欠拟合，则需要增加模型的复杂性，增加模型的 <em>Max depth</em>,增加更多的features。</p><h2 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h2><p>回归一下CARTs的优点：</p><ol><li>易于理解</li><li>容易使用</li><li>可以描述线性回归无法描述的情况</li><li>不需要标准化数据</li></ol><p>但CARTs也有缺点，那就是容易高方差，过度拟合，不过我们可以通过 ensemble learning 解决这个问题。</p><p>简单描述一下Ensemble Learning的步骤：</p><ol><li>用不同的模型training同一份数据</li><li>每个模型预测其自己的结果</li><li>聚合所有模型的结果</li><li>最终的预测：更可靠的数据</li></ol><p><strong>Ensemble prediction:</strong></p><p><img src="https://i.loli.net/2019/11/24/UxRoDOCBf3ZsvKP.png" alt=""></p><p>我们考虑一个Ensemble prediction的例子，叫做voting classifier，假设我们有i个预测器对同一组数据进行预测，产生i个结果叫做p1,p2,pi，结果只有两种，0或者1，i个结果都预测出来之后，我用投票的方式来觉得使用哪个结果，比如下面的图中，有两个预测器的结果是1，那么我们的结果就是1.</p><p><img src="https://i.loli.net/2019/11/24/aZ1oXrnkP39HphQ.png" alt=""></p><p><strong>Ensemble Learning code:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># Set seed for reproducibility</span><br><span class="line">SEED=1</span><br><span class="line"></span><br><span class="line"># Instantiate lr</span><br><span class="line">lr = LogisticRegression(random_state=SEED)</span><br><span class="line"></span><br><span class="line"># Instantiate knn</span><br><span class="line">knn = KNN(n_neighbors=27)</span><br><span class="line"></span><br><span class="line"># Instantiate dt</span><br><span class="line">dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)</span><br><span class="line"></span><br><span class="line"># Define the list classifiers</span><br><span class="line">classifiers = [(&apos;Logistic Regression&apos;, lr), (&apos;K Nearest Neighbours&apos;, knn), (&apos;Classification Tree&apos;, dt)]</span><br><span class="line"></span><br><span class="line"># Iterate over the pre-defined list of classifiers</span><br><span class="line">for clf_name, clf in classifiers:    </span><br><span class="line"> </span><br><span class="line">    # Fit clf to the training set</span><br><span class="line">    clf.fit(X_train, y_train)    </span><br><span class="line">   </span><br><span class="line">    # Predict y_pred</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    </span><br><span class="line">    # Calculate accuracy</span><br><span class="line">    accuracy = accuracy_score(y_test,y_pred) </span><br><span class="line">   </span><br><span class="line">    # Evaluate clf&apos;s accuracy on the test set</span><br><span class="line">    print(&apos;&#123;:s&#125; : &#123;:.3f&#125;&apos;.format(clf_name, accuracy))</span><br></pre></td></tr></table></figure><p><strong>Voting:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Import VotingClassifier from sklearn.ensemble</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line"># Instantiate a VotingClassifier vc</span><br><span class="line">vc = VotingClassifier(estimators=classifiers)     </span><br><span class="line"></span><br><span class="line"># Fit vc to the training set</span><br><span class="line">vc.fit(X_train, y_train)   </span><br><span class="line"></span><br><span class="line"># Evaluate the test set predictions</span><br><span class="line">y_pred = vc.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Calculate accuracy score</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(&apos;Voting Classifier: &#123;:.3f&#125;&apos;.format(accuracy))</span><br></pre></td></tr></table></figure><h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p><em>‌Ensemble Learning</em> 中，我们使用不同的算法对同一组数据进行处理再投票出结果，而 <em>Bagging</em> 正好相反，它用同一个算法，选取数据集中的不同组数据，计算结果，Bagging可以减少Variance。它和我们之前提及的 <em>Bootstrap</em> 方法类似。</p><p><img src="https://i.loli.net/2019/11/24/F2YAbs6IBEJQmik.png" alt="Bootstrap"></p><p><img src="https://i.loli.net/2019/11/24/4wjQF79uSWYsxUg.png" alt=""></p><p>bagging不同model的结果，也是通过voting的出来的：</p><p><img src="https://i.loli.net/2019/11/24/DIGNp2niJ9xFjwa.png" alt=""></p><p>对于分类问题，结果通过major voting 获得，对于regression问题，结果通过平均结果获得。</p><p><strong>Bagging python code:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Import DecisionTreeClassifier</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"># Import BaggingClassifier</span><br><span class="line">from sklearn.ensemble import BaggingClassifier</span><br><span class="line"></span><br><span class="line"># Instantiate dt</span><br><span class="line">dt = DecisionTreeClassifier(random_state=1)</span><br><span class="line"></span><br><span class="line"># Instantiate bc</span><br><span class="line">bc = BaggingClassifier(base_estimator=dt, n_estimators=50,random_state=1)</span><br></pre></td></tr></table></figure><p><strong>Bagging performance:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Fit bc to the training set</span><br><span class="line">bc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Predict test set labels</span><br><span class="line">y_pred = bc.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Evaluate acc_test</span><br><span class="line">acc_test = accuracy_score(y_test,y_pred)</span><br><span class="line">print(&apos;Test set accuracy of bc: &#123;:.2f&#125;&apos;.format(acc_test))</span><br></pre></td></tr></table></figure><p>在 <strong>bagging</strong> 中，那些我们分割出来的test数据没有参与模型训练，这些模型没有见过的数据叫做 <em>out of bag</em>(OOB)，我们可以在每一次模型sample数据的时候，将这些数据分割为train和test数据组，如下图所示：</p><p><img src="https://i.loli.net/2019/11/24/jcvZGonAzmar6yt.png" alt=""></p><h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><p>随机森林也是一种基于决策树的算法，假设我们有一组心脏病数据，根据它来创建随机森林。</p><p>首先我们根据原始数据创建bootstrap dataset,即随机的（可重复）从原数据选择：</p><p><img src="https://i.loli.net/2019/11/24/j3YVgtX1v45TnGu.png" alt=""></p><p>拿到随机创建的数据之后，我们随机的选择n个（这里暂时定为2个）feature，建立决策树，如何确定第一个树呢？可以根据gini算法，这里我们假设Good Blood最适合作为root节点，第一个树确定以后，我们可以再从剩下的树中随机的选择两棵树作为其他节点，示例图如下：</p><p><img src="https://i.loli.net/2019/11/24/B7c4ygI2wZ8UHWe.png" alt=""></p><p><img src="https://i.loli.net/2019/11/24/ChGPzE321nA5BdR.png" alt=""></p><blockquote><p>从剩下的树中选的时候，我们需要根据feature来选，根据哪些features来选， 以及多少features，都是需要提前定好的，在这里我们定为2。在sklearn中，是features的平方根，即如果我们有100个features，则每次用10个。</p></blockquote><p>于是我们得到了一颗树，现在重复上述步骤，创建bootstrap数据，再建立树，这样我们可以重复很多次，于是随机森林就建立起来了。</p><p><img src="https://i.loli.net/2019/11/24/xpv9ZiqB734oNAO.png" alt=""></p><p>随机森林建立好后，就可以开始预测，对于分类问题，我们使用投票机制决定输出，对于regresion问题，我们还是一样采用平均值。</p><p><strong>随机森林python code：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Import RandomForestRegressor</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line"></span><br><span class="line"># Instantiate rf</span><br><span class="line">rf = RandomForestRegressor(n_estimators=25,</span><br><span class="line">            random_state=2)</span><br><span class="line">            </span><br><span class="line"># Fit rf to the training set    </span><br><span class="line">rf.fit(X_train, y_train) </span><br><span class="line"></span><br><span class="line"># Import mean_squared_error as MSE</span><br><span class="line">from sklearn.metrics import mean_squared_error as MSE</span><br><span class="line"></span><br><span class="line"># Predict the test set labels</span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Evaluate the test set RMSE</span><br><span class="line">rmse_test = MSE(y_test, y_pred)**(1/2)</span><br><span class="line"></span><br><span class="line"># Print rmse_test</span><br><span class="line">print(&apos;Test set RMSE of rf: &#123;:.2f&#125;&apos;.format(rmse_test))</span><br></pre></td></tr></table></figure><p><strong>查看feature权重：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Create a pd.Series of features importances</span><br><span class="line">importances = pd.Series(data=rf.feature_importances_,</span><br><span class="line">                        index= X_train.columns)</span><br><span class="line"></span><br><span class="line"># Sort importances</span><br><span class="line">importances_sorted = importances.sort_values()</span><br><span class="line"></span><br><span class="line"># Draw a horizontal barplot of importances_sorted</span><br><span class="line">importances_sorted.plot(kind=&apos;barh&apos;, color=&apos;lightgreen&apos;)</span><br><span class="line">plt.title(&apos;Features Importances&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/24/xpv9ZiqB734oNAO.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这一篇文章我们来了解误差，如何处理误差，以及学习随机森林的基础知识。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="sklearn" scheme="http://wittyfans.com/tags/sklearn/"/>
    
      <category term="data science" scheme="http://wittyfans.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>机器学习：Classification and Regression Trees</title>
    <link href="http://wittyfans.com/coding/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9AClassification-and-Regression-Trees.html"/>
    <id>http://wittyfans.com/coding/机器学习：Classification-and-Regression-Trees.html</id>
    <published>2019-11-22T08:06:42.000Z</published>
    <updated>2019-11-22T08:15:21.097Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/22/poSKWwveBCuU16n.png" alt=""></p><blockquote><p>机器学习决策树的第一部分，分类与回归树（CART）的原理（多图)。</p></blockquote><a id="more"></a><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>为什么使用决策树呢，主要有两个原因，一是决策树通常模仿人类的思维，因此它很容易理解数据并做出一些很好的解释。二是决策树可以让你看到数据的逻辑。</p><p>我们看一个金融机构决定是否投资一家公司决策树：</p><p><img src="https://i.loli.net/2019/11/22/xqRKiL3Qa2Cr4UI.png" alt=""></p><p>从决策树的组成来看，它是由一系列节点（nodes）的继承关系组成的，节点可以看作是对问题做出回答，也可以是问题的答案。这些问题有些依据数据的分类，如你是否吸烟，有的是数据本身你一天吸几根，（数值型），决策树中主要有三类节点：</p><ol><li>Root, root节点没有父节点，有两个子节点</li><li>Internal root, 一个父节点，两个子节点</li><li>Leaf, 一个父节点，没有子节点</li></ol><p>每一个internal root代表一个“test” on an attribute，即在这个节点根据这个属性做出的判断，每一个分支则代表了判断的输出，每一个leaf代表了一个class label，即最后的决定。</p><p>下面是一些决策树中的常见名词：</p><ol><li>Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets.</li><li>Splitting: It is a process of dividing a node into two or more sub-nodes.</li><li>Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node.</li><li>Leaf/ Terminal Node: Nodes do not split is called Leaf or Terminal node.</li><li>Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.</li><li>Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree.</li><li>Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.</li></ol><h1 id="决策树如何学习"><a href="#决策树如何学习" class="headerlink" title="决策树如何学习"></a>决策树如何学习</h1><p>要建立决策树，就得对数据进行分类，第一个节点即根节点的分类范围是最大的，它把所有的问题分成两类。</p><p>如何找到这个分类呢？这取决于那些最能对数据集进行分类的属性，怎么确定哪些分类最能对数据进行分类呢？</p><p>看下面一个心脏病的例子，我们把每一个分类的都单独拿出来，计算分别统计它相对结果的预测量，如下图：</p><p><img src="https://i.loli.net/2019/11/22/XrWPkLu2aAMoGfT.png" alt=""></p><p>有没有心脏病是我们的leaf，也就是结果输出，而胸痛是feature。我们统计胸痛的值与现有的心脏病的值的对应关系，如果胸痛，是否是心脏病？如果胸不痛，是否是心脏病？这样我们就得到四列数据，结束对胸痛这个feature的统计后，继续对Good Blood进行同样的操作。</p><p><img src="https://i.loli.net/2019/11/22/cFErmQLRdjSGANk.png" alt=""></p><p>可以看到这个结果中，不管是胸痛，还是血液情况，都没办法在yes与no后就准确预测心脏病，所以上面的这些数据都是impurity的，那我们如何衡量这种impurity的程度呢？</p><p>有很多衡量 impurity 的方法，其中一种叫做Gini, 下面是Gini的计算方式：</p><p><img src="https://i.loli.net/2019/11/22/bNtKIPiq3QS7uXF.png" alt=""></p><p>同样的方法计算右边的节点:</p><p><img src="https://i.loli.net/2019/11/22/VosX56SFIqnPd1k.png" alt=""></p><p>但是左右两边的患者数量是不一样的，一个是144,一个是159:</p><p><img src="https://i.loli.net/2019/11/22/R8wpJ6X7HPgmrlI.png" alt=""></p><p>我们需要对左右节点都平均一下再相加：</p><p><img src="https://i.loli.net/2019/11/22/ujM8EtbcvxDTmK5.png" alt=""></p><p>再把所有的feature都算一遍，</p><p><img src="https://i.loli.net/2019/11/22/oKLqZMGXEfDTxd2.png" alt=""></p><p>我们选择gini数最低的那个一，作为root node</p><p><img src="https://i.loli.net/2019/11/22/c29ySGZYRu3rdpT.png" alt=""><br><img src="https://i.loli.net/2019/11/22/LqWxzZajrP4QnDC.png" alt=""></p><p>root node确定了之后，我们还需要确定后续的节点使用哪个feature，这个步骤和前面确定root node是一样的：</p><p><img src="https://i.loli.net/2019/11/22/Rg865Yih9Xo3BDE.png" alt=""></p><p>进一步往下面确定子节点的时候，我们发现，在动脉阻塞(blocked)的两个子节点下， 没有发生动脉阻塞这个节点中，只有13个患有心脏病，也就是说89%的患者都没有心脏病！</p><p><img src="https://i.loli.net/2019/11/22/poSKWwveBCuU16n.png" alt=""></p><p>我们来算一下这个节点的gini值与进一步按照胸痛往下面细分后的gini值：</p><p><img src="https://i.loli.net/2019/11/22/6cGaSCwIEmQPXsq.png" alt=""></p><p>可以看到按照胸痛细分后的gini值为0.29，而不细分的值则为0.2，所以对于这个节点来说，就没有必要再往下面细分节点了，所以我们就不处理 <code>13/102</code> 这个节点了，它也就成了我们说的 leaf 节点。</p><p>将剩下的部分以及右边的子节点补上，我们的决策树就建好了:</p><p><img src="https://i.loli.net/2019/11/22/EP5aXncFzWkSysY.png" alt=""></p><p>不过还有一个问题，我们前面都是基于yes or no的问题，那如果遇到数值型的数据怎么办？</p><p>首先，我们将数值数据从小到大排列：</p><p><img src="https://i.loli.net/2019/11/22/XP1On7DCsAFzxya.png" alt=""></p><p>两两取数值中间的平均值，计算gini值：</p><p><img src="https://i.loli.net/2019/11/22/CJgeWPEai8zuvdt.png" alt=""></p><p><img src="https://i.loli.net/2019/11/22/fDPcg3Ts2WjLrAR.png" alt=""></p><p><img src="https://i.loli.net/2019/11/22/pfD1JGA5UICPlhb.png" alt=""></p><p>重复计算所有其他的gini值，最后选出最小的那个。</p><p><img src="https://i.loli.net/2019/11/22/Nowl8QfDPbpWRZO.png" alt=""></p><p>这就是数值型数据的计算了。不过还有一些数据它是选择型的，如颜色的选择，对于颜色的选择，我们可以把选择分开来计算，如下图：</p><p><img src="https://i.loli.net/2019/11/22/UJevq6O9CKyR1wk.png" alt=""></p><blockquote><p>关于决策数的<a href="https://www.youtube.com/watch?v=7VeUPuFGJHk" target="_blank" rel="noopener">视频</a></p></blockquote><p>上面的这个gini值，我们又叫做IG(Information Gain)值，我们知道，如果IG值为0，或者我们发现该node分割后gini值更高，我们就不继续分割，该node就叫做leaf。如果我们固定了一颗数的深度，那么当分割node的时候，超过了规定的深度，我们也会停止分割子节点。</p><p>另外，拆分内部节点总是涉及最大化信息增益！</p><h1 id="使用决策树"><a href="#使用决策树" class="headerlink" title="使用决策树"></a>使用决策树</h1><p>决策树可以解决classfication问题，也可以解决regression问题，我们先讨论分类树。</p><p>训练决策树：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Import DecisionTreeClassifier from sklearn.tree</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"># Instantiate a DecisionTreeClassifier &apos;dt&apos; with a maximum depth of 6</span><br><span class="line">dt = DecisionTreeClassifier(max_depth=6,random_state=SEED)</span><br><span class="line"></span><br><span class="line"># Fit dt to the training set</span><br><span class="line">dt.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Predict test set labels</span><br><span class="line">y_pred = dt.predict(X_test)</span><br><span class="line">print(y_pred[0:5])</span><br></pre></td></tr></table></figure><p>决策树评分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Import accuracy_score</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line"># Predict test set labels</span><br><span class="line">y_pred = dt.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Compute test set accuracy  </span><br><span class="line">acc = accuracy_score(y_pred, y_test)</span><br><span class="line">print(&quot;Test set accuracy: &#123;:.2f&#125;&quot;.format(acc))</span><br></pre></td></tr></table></figure><p>逻辑回归与分类树:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import LogisticRegression from sklearn.linear_model</span><br><span class="line">from sklearn.linear_model import  LogisticRegression</span><br><span class="line"></span><br><span class="line"># Instatiate logreg</span><br><span class="line">logreg = LogisticRegression(random_state=1)</span><br><span class="line"></span><br><span class="line"># Fit logreg to the training set</span><br><span class="line">logreg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Define a list called clfs containing the two classifiers logreg and dt</span><br><span class="line">clfs = [logreg, dt]</span><br><span class="line"></span><br><span class="line"># Review the decision regions of the two classifiers</span><br><span class="line">plot_labeled_decision_regions(X_test, y_test, clfs)</span><br></pre></td></tr></table></figure><h2 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Import DecisionTreeClassifier from sklearn.tree</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"># Instantiate dt_entropy, set &apos;entropy&apos; as the information criterion</span><br><span class="line">dt_entropy = DecisionTreeClassifier(max_depth=8, criterion=&apos;entropy&apos;, random_state=1)</span><br><span class="line"></span><br><span class="line"># Fit dt_entropy to the training set</span><br><span class="line">dt_entropy.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Import accuracy_score from sklearn.metrics</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line"># Use dt_entropy to predict test set labels</span><br><span class="line">y_pred = dt_entropy.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Evaluate accuracy_entropy</span><br><span class="line">accuracy_entropy = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"># Print accuracy_entropy</span><br><span class="line">print(&apos;Accuracy achieved by using entropy: &apos;, accuracy_entropy)</span><br><span class="line"></span><br><span class="line"># Print accuracy_gini</span><br><span class="line">print(&apos;Accuracy achieved by using the gini index: &apos;, accuracy_gini)</span><br></pre></td></tr></table></figure><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Import DecisionTreeRegressor from sklearn.tree</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"># Instantiate dt</span><br><span class="line">dt = DecisionTreeRegressor(max_depth=8,</span><br><span class="line">             min_samples_leaf=0.13,</span><br><span class="line">            random_state=3)</span><br><span class="line"></span><br><span class="line"># Fit dt to the training set</span><br><span class="line">dt.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h3 id="衡量回归树"><a href="#衡量回归树" class="headerlink" title="衡量回归树"></a>衡量回归树</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import mean_squared_error from sklearn.metrics as MSE</span><br><span class="line">from sklearn.metrics import mean_squared_error as MSE</span><br><span class="line"></span><br><span class="line"># Compute y_pred</span><br><span class="line">y_pred = dt.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Compute mse_dt</span><br><span class="line">mse_dt = MSE(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"># Compute rmse_dt</span><br><span class="line">rmse_dt = mse_dt**(1/2)</span><br><span class="line"></span><br><span class="line"># Print rmse_dt</span><br><span class="line">print(&quot;Test set RMSE of dt: &#123;:.2f&#125;&quot;.format(rmse_dt))</span><br></pre></td></tr></table></figure><h2 id="Linear-regression-vs-regression-tree"><a href="#Linear-regression-vs-regression-tree" class="headerlink" title="Linear regression vs regression tree"></a>Linear regression vs regression tree</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Predict test set labels </span><br><span class="line">y_pred_lr = lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Compute mse_lr</span><br><span class="line">mse_lr = MSE(y_test, y_pred_lr)</span><br><span class="line"></span><br><span class="line"># Compute rmse_lr</span><br><span class="line">rmse_lr = mse_lr**(1/2)</span><br><span class="line"></span><br><span class="line"># Print rmse_lr</span><br><span class="line">print(&apos;Linear Regression test set RMSE: &#123;:.2f&#125;&apos;.format(rmse_lr))</span><br><span class="line"></span><br><span class="line"># Print rmse_dt</span><br><span class="line">print(&apos;Regression Tree test set RMSE: &#123;:.2f&#125;&apos;.format(rmse_dt))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/22/poSKWwveBCuU16n.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;机器学习决策树的第一部分，分类与回归树（CART）的原理（多图)。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="sklearn" scheme="http://wittyfans.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习：PCA and NMF</title>
    <link href="http://wittyfans.com/coding/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9APCA-and-NMF.html"/>
    <id>http://wittyfans.com/coding/非监督学习：PCA-and-NMF.html</id>
    <published>2019-11-21T08:20:37.000Z</published>
    <updated>2019-11-21T08:24:31.315Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/21/I34aLMFuywnHqzA.png" alt=""></p><blockquote><p>非监督学习的最后两章，我们会学习 Dimension reduction, Dimension reduction就是从数据中发现一定的模式，通过这种模式我们就可以对数据进行压缩，这对于计算和存储来说都是非常有利的，特别是在大数据时代。</p></blockquote><a id="more"></a><p>dimension reduction核心的功能是去除数据中的噪音，保留那些最基本的东西，噪音去除之后，有助于减少我们处理 classfication 与 regression 中碰到的问题。</p><p>接下来我们介绍最基本的 dimension reduction 算法，<em>Principal Component Analysis</em> (PCA).</p><h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>PCA 主要有两个步骤：</p><ol><li>decorrlation</li><li>dimension reduction</li></ol><p>我们先讲第一个步骤，decorrlation, decorrlation会将数据旋转到与坐标轴对齐，并将其均值移动到0附近，所以PCA并不会改变原始数据。</p><p><img src="https://i.loli.net/2019/11/20/qolJbVCpOUg5PBY.png" alt=""></p><p>PCA和 StandardScaler 一样，有fit和transform方法，fit方法会学习如何去shift与rotate数据，但并不会做出任何操作。</p><p>transform方法，则会应用fit学习到的东西。</p><p>在PCA对数据transform后，原来数据的columns对应数据的features，转换后的数据列则对应pca的features。</p><p>我们用皮尔逊系数来衡量相关性，如果原来的数据是有相关性的，则转化后的数据不会再具备这种特性。</p><p>以我们的谷物的长、宽数据为例，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Perform the necessary imports</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.stats import pearsonr</span><br><span class="line"></span><br><span class="line"># Assign the 0th column of grains: width</span><br><span class="line">width = grains[:,0]</span><br><span class="line"></span><br><span class="line"># Assign the 1st column of grains: length</span><br><span class="line">length = grains[:,1]</span><br><span class="line"></span><br><span class="line"># Scatter plot width vs length</span><br><span class="line">plt.scatter(width, length)</span><br><span class="line">plt.axis(&apos;equal&apos;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Calculate the Pearson correlation</span><br><span class="line">correlation, pvalue = pearsonr(width,length)</span><br><span class="line"></span><br><span class="line"># Display the correlation</span><br><span class="line">print(correlation)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/20/XuxqSG6vWB7Ddlf.png" alt="transform前"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Import PCA</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line"># Create PCA instance: model</span><br><span class="line">model = PCA()</span><br><span class="line"></span><br><span class="line"># Apply the fit_transform method of model to grains: pca_features</span><br><span class="line">pca_features = model.fit_transform(grains)</span><br><span class="line"></span><br><span class="line"># Assign 0th column of pca_features: xs</span><br><span class="line">xs = pca_features[:,0]</span><br><span class="line"></span><br><span class="line"># Assign 1st column of pca_features: ys</span><br><span class="line">ys = pca_features[:,1]</span><br><span class="line"></span><br><span class="line"># Scatter plot xs vs ys</span><br><span class="line">plt.scatter(xs, ys)</span><br><span class="line">plt.axis(&apos;equal&apos;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Calculate the Pearson correlation of xs and ys</span><br><span class="line">correlation, pvalue = pearsonr(xs, ys)</span><br><span class="line"></span><br><span class="line"># Display the correlation</span><br><span class="line">print(correlation)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/20/kPyiZ9QhEKxY58S.png" alt="transform后"></p><p>可以看到，谷物的长与宽是有相关性的，即它的叶片越长，宽度也越宽。但在transofor后，数据就与坐标系对齐了。</p><h2 id="Intrinsic-dimension"><a href="#Intrinsic-dimension" class="headerlink" title="Intrinsic dimension"></a>Intrinsic dimension</h2><p>Intrinsic dimension 即在降维或者压缩数据过程中，为了让你的数据特征最大程度的保持，你最低限度需要保留哪些features。</p><p>它同时也告诉了我们可以把数据压缩到什么样的程度，所以你需要了解哪些 feature 对你的数据集影响是最大的。</p><p>比如我们衡量电脑性能，我们可以把键盘鼠标这些feature去掉，但是cpu、gpu、内存性能这些你肯定要保留对不对？cpu、gpu、内存性能，这就是我们说的 features required to approximate it。</p><p>但有的人说，我觉得硬盘也得加上去，这样我们的 intrinsic dimension 就从 3 增加到4了。</p><p>所以你看，intrinsic dimension 数量是一个视不同情况而定的值。</p><p>那我们将设为多少比较合适呢？考虑我们的iris数据，我们选择</p><ul><li>sepal width</li><li>sepal length</li><li>petal width</li></ul><p>这3个变量，这样我们就可以将它绘制到3维空间，但我们发现它的图像在3维空间是很平的，这意味着某个变量的值的方差很小，也就是说它对整个图像的影响不大，我们只选择另外2个变量，也可以不损失太多的细节。</p><p><img src="https://i.loli.net/2019/11/20/iUkctja8u2RovbA.png" alt=""></p><p>但scatter图只能表示3维以下的数据，如果我们的 features 有很多个，我们要如何找到那些影响程度大的 features 呢？</p><p>这就是 PCA 擅长的地方，我们可以统计 PCA 中那些方差显著的features。</p><p>下面是iris数据集各个features的方差排序图：</p><p><img src="https://i.loli.net/2019/11/20/qDPFfX7E8xjMwAu.png" alt=""></p><p>可以看到最后的那个数据方差非常小，所以它对我们数据集的影响完全可以去除掉。</p><p>那么如何绘制数据集的 variances 呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Perform the necessary imports</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.pipeline import make_pipeline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># Create scaler: scaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"></span><br><span class="line"># Create a PCA instance: pca</span><br><span class="line">pca = PCA()</span><br><span class="line"></span><br><span class="line"># Create pipeline: pipeline</span><br><span class="line">pipeline = make_pipeline(scaler,pca)</span><br><span class="line"></span><br><span class="line"># Fit the pipeline to &apos;samples&apos;</span><br><span class="line">pipeline.fit(samples)</span><br><span class="line"></span><br><span class="line"># Plot the explained variances</span><br><span class="line">features = range(pca.n_components_)</span><br><span class="line">plt.bar(features,pca.explained_variance_)</span><br><span class="line">plt.xlabel(&apos;PCA feature&apos;)</span><br><span class="line">plt.ylabel(&apos;variance&apos;)</span><br><span class="line">plt.xticks(features)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h2><p>我们在做数据降维的时候，得指定需要保留的features，即通过指定pca的参数<code>n_components=2</code>, 我们保留多少个features，这可以通过Intrinsic dimension来计算。</p><p>我们以iris数据为例，当我们用PCA把数据从四维（petal width,length和sepal length,width）降到2维后，我们再绘制它的scatter图，会发现它们还是分为三部分。</p><p>但是对于如字词统计的例子中，我们可能需要用其他的算法来代替，在这种数据中，每一行代表一个文档，每一列代表某个固定词组（fixed vocabulary）中的词语.</p><p>我们会统计，每个词语在每个文档中出现的次数array。不过，只有少数的词语会出现很多的文档之中，大部分词语在某些文档中的出现次数都是0，这样的array叫做sparse(稀疏的），sparse array中大部分的值都是0.</p><p>我们使用 <em>csr_matrix</em> 来代替 <em>numpy array</em> 去处理这样的数据。</p><p><em>csr_matrix</em> 只会保存那些非0值，这样就更方便，但是PCA不支持 <em>csr_matrix</em>，我们需要用 <em>TruncatedSVD</em> 来代替 PCA。</p><p><em>‌TruncatedSVD</em> 的效果与 PCA 是一样的，只不过它接受 <em>csr_matrix</em> 数组，除此之外使用方法都是一样的。</p><p>示例，我们来对一组词语进行分词操作，我们的词组如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># data</span><br><span class="line">documents=[&apos;cats say meow&apos;, &apos;dogs say woof&apos;, &apos;dogs chase cats&apos;]</span><br><span class="line"></span><br><span class="line"># Import TfidfVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"></span><br><span class="line"># Create a TfidfVectorizer: tfidf</span><br><span class="line">tfidf = TfidfVectorizer() </span><br><span class="line"></span><br><span class="line"># Apply fit_transform to document: csr_mat</span><br><span class="line">csr_mat = tfidf.fit_transform(documents)</span><br><span class="line"></span><br><span class="line"># Print result of toarray() method</span><br><span class="line">print(csr_mat.toarray())</span><br><span class="line"></span><br><span class="line"># Get the words: words</span><br><span class="line">words = tfidf.get_feature_names()</span><br><span class="line"></span><br><span class="line"># Print words</span><br><span class="line">print(words)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output: [[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]</span><br><span class="line"> [0.         0.         0.51785612 0.         0.51785612 0.68091856]</span><br><span class="line"> [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]</span><br><span class="line"> </span><br><span class="line"> [&apos;cats&apos;, &apos;chase&apos;, &apos;dogs&apos;, &apos;meow&apos;, &apos;say&apos;, &apos;woof&apos;]</span><br></pre></td></tr></table></figure><h1 id="NMF"><a href="#NMF" class="headerlink" title="NMF"></a>NMF</h1><p>NMF与PCA一样，都是降维算法。不过NMF不能用于所有的预测，只能用于非负值。</p><p>NMF会将数据集中的每组数据分解并分别计算其和。</p><p>考虑字词统计的例子，NMF使用tf-idf来计算字词出现的频率。<br>tf是词语出现的频率，在每一个document中，如果一个词占整个document中文字的百分之10，那么NMF的输出的tf就是0.1.</p><p>而idf用来降低那些常用词的影响，如,<code>的,那个，这个</code>等等。</p><p>我们知道，在PCA中，我们定义了多少个<code>n_components</code>,就相当于是保留了多少个features。</p><p>在NMF中，它也会去学习数据集的 <em>component dimension</em>。</p><p>需注意，NMF的条目，以及features的值，总是正数。</p><h2 id="NMF算法使用"><a href="#NMF算法使用" class="headerlink" title="NMF算法使用"></a>NMF算法使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import NMF</span><br><span class="line">from sklearn.decomposition import NMF</span><br><span class="line"></span><br><span class="line"># Create an NMF instance: model</span><br><span class="line">model = NMF(n_components=6)</span><br><span class="line"></span><br><span class="line"># Fit the model to articles</span><br><span class="line">model.fit(articles)</span><br><span class="line"></span><br><span class="line"># Transform the articles: nmf_features</span><br><span class="line">nmf_features = model.transform(articles)</span><br><span class="line"></span><br><span class="line"># Print the NMF features</span><br><span class="line">print(nmf_features)</span><br></pre></td></tr></table></figure><h2 id="论文字词统计"><a href="#论文字词统计" class="headerlink" title="论文字词统计"></a>论文字词统计</h2><blockquote><p>Components correspond to topics of documents, and the NMF features reconstruct the documents from the topics</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import pandas</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># Create a DataFrame: components_df</span><br><span class="line">components_df = pd.DataFrame(model.components_,columns=words)</span><br><span class="line"></span><br><span class="line"># Print the shape of the DataFrame</span><br><span class="line">print(components_df.shape)</span><br><span class="line"></span><br><span class="line"># Select row 3: component</span><br><span class="line">component = components_df.iloc[3]</span><br><span class="line"></span><br><span class="line"># Print result of nlargest</span><br><span class="line">print(component.nlargest())</span><br></pre></td></tr></table></figure><h2 id="用户喜好预测"><a href="#用户喜好预测" class="headerlink" title="用户喜好预测"></a>用户喜好预测</h2><p>NMF有一个重要的应用是作为内容推荐，假设我们有一个用户和一部电影，我们想要根据电影的内容做出预测：</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">喜剧片</th><th style="text-align:left">动作片</th></tr></thead><tbody><tr><td style="text-align:left">小雅</td><td style="text-align:left">喜欢</td><td style="text-align:left">不喜欢</td></tr><tr><td style="text-align:left">百货战警</td><td style="text-align:left">3</td><td style="text-align:left">1</td></tr></tbody></table><blockquote><p>红番区是动作喜剧片。</p></blockquote><p>现在假设小雅喜欢喜剧片，不喜欢动作片，同时假设我们现在只根据一部电影拥有多少喜剧成分与动作片成分来评价它。</p><p>现在因为小雅喜欢喜剧片<code>+3</code>，不喜欢动作片<code>+0</code>，所以它可能给出的评价是 3分 。</p><p>我们再来考虑小明的情况：</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">喜剧片</th><th style="text-align:left">动作片</th></tr></thead><tbody><tr><td style="text-align:left">小明</td><td style="text-align:left">不喜欢</td><td style="text-align:left">喜欢</td></tr><tr><td style="text-align:left">红番区</td><td style="text-align:left">3</td><td style="text-align:left">1</td></tr></tbody></table><p>小明喜欢动作片，不喜欢喜剧片，所以喜剧片的3分不计，动作片1分计入，即小明只给出了1分的评价。</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">喜剧片</th><th style="text-align:left">动作片</th></tr></thead><tbody><tr><td style="text-align:left">小雅</td><td style="text-align:left">喜欢</td><td style="text-align:left">喜欢</td></tr><tr><td style="text-align:left">红番区</td><td style="text-align:left">3</td><td style="text-align:left">1</td></tr></tbody></table><p>再考虑一个例子：</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">喜剧片</th><th style="text-align:left">动作片</th></tr></thead><tbody><tr><td style="text-align:left">小刘</td><td style="text-align:left">喜欢</td><td style="text-align:left">喜欢</td></tr><tr><td style="text-align:left">让子弹飞</td><td style="text-align:left">1</td><td style="text-align:left">3</td></tr></tbody></table><p>小刘喜欢喜剧也喜欢动作，所以这部电影他可能会给出1+3，四分的评价。</p><p>我们把一批电影编号为M1-M5,列表示分别占喜剧与动作的成分，同时把一批用户分类为他们是否喜欢喜剧或动作片。</p><p><img src="https://i.loli.net/2019/11/21/1RKTpP4MFcVuyfS.png" alt=""></p><p>当我们把左边的两个表结合，就可以得到后面的这个表：</p><p><img src="https://i.loli.net/2019/11/21/PTgzFwfcnsX5uQp.png" alt=""></p><p>这是通过算点积算出来的，你可以看到：</p><p><img src="https://i.loli.net/2019/11/21/Astuv1eTKCGHDyP.png" alt=""></p><ul><li>有些人的喜好一样，则他们给出的电影评分也是一样的</li><li>如果两个电影所占的成分差不多，则也会得到一样的评分</li><li>B+C=D,这是因为B的喜欢与C的喜好相反，而D对与这两者都喜欢</li><li>M2与M3的平均值是M5，即M5的喜剧与动作成分是M2与M3的平均值，所以M5的评分也是M2与M3的平均值</li></ul><p>上面的这个表叫做Matrix Factorization, 我们要存储这个表是非常费存储的，假设我们有很多的电影与用户，算一下我们需要一个多大的Matrix.</p><p><img src="https://i.loli.net/2019/11/21/SnX2JGWdIgsM6oL.png" alt=""></p><p>所以我们在存储的时候，只存储用户的喜好与电影的信息,再通过图表演示一下：</p><p><img src="https://i.loli.net/2019/11/21/TqmufiSXRHQchPZ.png" alt=""></p><p>第一种方案是我们直接存储用户对所有电影评分与可能的评分，假设有2000个用户，1000部电影，这意味着200万份数据。</p><p><img src="https://i.loli.net/2019/11/21/8fwGTU7L96uIc5e.png" alt=""></p><p>如果我们通过feature计算，则只需要30万份数据</p><p><img src="https://i.loli.net/2019/11/21/k9Ml5mUPoBg4SZC.png" alt=""></p><p>现在我们回到以前，对与NMP算法，其中一个问题是，</p><p><img src="https://i.loli.net/2019/11/21/Astuv1eTKCGHDyP.png" alt=""></p><p>如何通过中间的点积，得到两边的数据？就像24可以是2x12,也可以是4x6.</p><p>最开始我们可以随机的将两边的数据列出来，算出点积并与手上的点积比对，如果过低，那么我们将相应的用户喜好与电影评分做出调整，直到所有的值接近我们已有的点积中的值。</p><p><img src="https://i.loli.net/2019/11/21/c9QkmIZ5TbEUFMY.png" alt=""></p><p>我们算到什么程度，才停止呢？我们需要定义一个error function,它就是告诉机器，你哪里错了，做的还不够，得继续算。</p><p><img src="https://i.loli.net/2019/11/21/I34aLMFuywnHqzA.png" alt=""></p><p>这里的error就是我们手上的数据与机器算出来的数据的差的平方，我们把每个用户或者每个电影的error加起来，error function的作用就是尽量让error的和最小。</p><p>在现实的情况中，我们不可能每个用户对每个电影都有评分，所以我们拿到的点积图中间可能有很多空白:</p><p><img src="https://i.loli.net/2019/11/21/q865U2sbpad9x3l.png" alt=""></p><p>但是我们可以根据这些值算出来用户与电影的属性,进而给用户推荐电影:</p><p><img src="https://i.loli.net/2019/11/21/qUIHQT75DZ6Rhdk.png" alt=""></p><blockquote><p><a href="https://www.youtube.com/watch?v=ZspR5PZemcs&amp;t=693s" target="_blank" rel="noopener">参考:How does Netflix recommend movies? Matrix Factorization</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/21/I34aLMFuywnHqzA.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;非监督学习的最后两章，我们会学习 Dimension reduction, Dimension reduction就是从数据中发现一定的模式，通过这种模式我们就可以对数据进行压缩，这对于计算和存储来说都是非常有利的，特别是在大数据时代。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="sklearn" scheme="http://wittyfans.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>非监督学习：Clustering and Visualization</title>
    <link href="http://wittyfans.com/coding/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9AClustering-and-Visualization.html"/>
    <id>http://wittyfans.com/coding/非监督学习：Clustering-and-Visualization.html</id>
    <published>2019-11-19T15:02:57.000Z</published>
    <updated>2019-11-20T03:14:45.440Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/19/GIND7pRkxahrXBu.png" alt=""></p><blockquote><p>非监督学习：即根据现有的数据去发掘存在数据中的一些模式,比如根据用户的购买记录，(<em>clustering</em>) 定义用户画像, 或根据数据的模式来压缩数据（<em>dimension reduction</em>.</p></blockquote><a id="more"></a><h1 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="非监督学习介绍"><a href="#非监督学习介绍" class="headerlink" title="非监督学习介绍"></a>非监督学习介绍</h2><p>相比较监督学习，如对癌症良性（<em>benign</em>）与恶性（<em>cancerous</em>）的分类，在这个例子中，我们发现的模式是: <em>guided</em>,或者说是: <em>supervised</em>，这一过程依赖我们标记过的数据，而非监督学习不需要我们使用标记过的数据去发现这些模式，我们不需要去guide the machine。</p><h1 id="聚类与数据探索"><a href="#聚类与数据探索" class="headerlink" title="聚类与数据探索"></a>聚类与数据探索</h1><h2 id="iris-数据集"><a href="#iris-数据集" class="headerlink" title="iris 数据集"></a>iris 数据集</h2><p>iris数据集包括了三种植物，它们有四种衡量方式：</p><ul><li>petal width</li><li>petal length</li><li>sepal width</li><li>sepql length</li></ul><p>这些叫做数据的 <em>features</em> ,在这门课程中，像这样的课程将会将数据集定义为2维数组,列定义衡量标准( <em>the features</em> )，行代表一个样例( <em>a sample</em> ).</p><p>我们的数据集有<strong>四个衡量方式，对应一个四维数组</strong>，事实上你可以推算出来：</p><p>$$Dimension=number\ of\ features$$</p><p>我们无法直接对四维数据可视化，但我们可以使用非监督学习技术来从其中获取想要的信息。</p><p>这一章，我们会使用 k-means 聚类算法将所有的样例数据分类。</p><p>k-means算法会从数据集中发现特定的分类，即把n个点（可以是样本的一次观察或一个实例）划分到k个聚类中，使得每个点都属于离它最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。</p><p>k-means算法在sklearn中有实现，我们来看一个示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">modal = KMeans(n_clusters=3)</span><br><span class="line"></span><br><span class="line">modal.fit(samples)</span><br><span class="line"></span><br><span class="line">labels = model.predit(samples)</span><br></pre></td></tr></table></figure><ol><li>导入kmeans包</li><li>实例化，并指定你想要从数据中发现的聚类数量(n_clusters)</li><li>fit sample数据</li><li>用同样的数据做预测（演示需要），这会返回所有的sample，同时其中包含它所属的聚类。</li></ol><blockquote><p><a href="http://shabal.in/visuals/kmeans/4.html" target="_blank" rel="noopener">这里</a>有k-means算法的动画演示,看一下你就明白啦，另外一个油管的<a href="http://shabal.in/visuals/kmeans/4.html" target="_blank" rel="noopener">视频版</a></p></blockquote><h2 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h2><p><strong>我们的数据：</strong></p><p>我们的数据 <em>points</em> 是一个300行，2列的数据集。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.06544649, -0.76866376],</span><br><span class="line">       [-1.52901547, -0.42953079],</span><br><span class="line">       [ 1.70993371,  0.69885253],</span><br><span class="line">       [ 1.16779145,  1.01262638],</span><br><span class="line">       [-1.80110088, -0.31861296],</span><br><span class="line">       [-1.63567888, -0.02859535])</span><br></pre></td></tr></table></figure><p><strong>数据集的分布</strong>:</p><p><img src="https://i.loli.net/2019/11/19/9ALwWz1bRlOBCor.png" alt=""></p><p>我们会把数据集中的第一列作为 x,第二列作为 y，使用数组的切片分别将 x,y 部分的值赋给xs,ys.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xs = points[:,0]</span><br><span class="line">ys = points[:1]</span><br></pre></td></tr></table></figure><p>有了数据之后，我们就可以对模型进行初始化与训练了，我们初始化Kmeans实例，并将数据整个传给它进行训练，训练之后我们用新准备的数据new_points来预测它的性能。</p><p>model预测后的结果保存在labels中，这是对new_points数据的预测，labels的数据只有一列，保存着分类的情况，它跟我们的数组new_points对应的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import KMeans</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line"># Create a KMeans instance with 3 clusters: model</span><br><span class="line">model = KMeans(n_clusters=3)</span><br><span class="line"></span><br><span class="line"># Fit model to points</span><br><span class="line">model.fit(points)</span><br><span class="line"></span><br><span class="line"># Determine the cluster labels of new_points: labels</span><br><span class="line">labels = model.predict(new_points)</span><br><span class="line"></span><br><span class="line"># Print cluster labels of new_points</span><br><span class="line">print(labels)</span><br></pre></td></tr></table></figure><p>因为预测的数据跟原始数据是对应的，所以我们以数据中的第一列作为x，第二列作为y，绘制scatter图，然后指定scatter的颜色参数c为labels，这样就可以在数据可视化的时候将不同聚类的数据分开表示。</p><p><strong>聚类的中心：centroids</strong></p><p>不同的聚类，它们都有自己的中心点，这个中心点可以直接通过模型的属性 <code>model.cluster_centers_</code> 找到。我们的模型找到了几个聚类，cluster_centers_就有几组数据，下面是3个<code>n_clusters</code>的例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[-1.57568905, -0.22531944],</span><br><span class="line">       [ 0.18034887, -0.81701955],</span><br><span class="line">       [ 1.01378685,  0.98288627]])</span><br></pre></td></tr></table></figure><p>我们将聚类后的数据与centroids都绘出来的效果：</p><p><img src="https://i.loli.net/2019/11/19/Q4gzo9pkG7vr1PI.png" alt=""></p><h2 id="选择-n-clusters（inertia）"><a href="#选择-n-clusters（inertia）" class="headerlink" title="选择 n_clusters（inertia）"></a>选择 n_clusters（inertia）</h2><p>如何选择n_clusters,一个好的聚合器，需要尽可能的使不同聚类的数量少，同时聚类中的数据集与centroids的距离最近。</p><p>聚类器有一个参数专门来描述这种关系，即 <em>‌inertia_</em>， 我们可以对同一个数据变化不同的n_clusters来查看它的inertia_的变化:</p><p><img src="https://i.loli.net/2019/11/19/IPUG5LMecSDAxoy.png" alt=""></p><p>在这里例子中，可以看到k从1-3一直是急剧下降的，随后开始变得平缓，我们一般选择图像从急剧下降到变的平缓之间的这个值，这个例子中，3是不错的选择。</p><h2 id="缩放数据与pipline"><a href="#缩放数据与pipline" class="headerlink" title="缩放数据与pipline"></a>缩放数据与pipline</h2><p>我们有一组这样的数据:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[242.0, 23.2, 25.4, 30.0, 38.4, 13.4]</span><br></pre></td></tr></table></figure><p>每一列是features，对数据集进行聚类，但是发现效果并不理想，原因是这些列之间的方差很大，我们需要将其标准化到同一个维度，如0，1或者-1，1,这个步骤叫 <em>‌Scaling</em>。</p><p>我们使用 <em>‌StandardScaler</em> 来缩放数据，另外这里我们使用管道来将缩放与kmeans的步骤结合到一起，关于pipline的介绍，请参考之前的文章。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 导入数据</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.pipeline import make_pipeline</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line"># 创建scaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"></span><br><span class="line"># 创建 kmeans 实例</span><br><span class="line">kmeans = KMeans(n_clusters=4)</span><br><span class="line"></span><br><span class="line"># 创建管道 pipeline</span><br><span class="line">pipeline = make_pipeline(scaler,kmeans)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对管道fit数据</span><br><span class="line">pipeline.fit(samples)</span><br><span class="line"></span><br><span class="line"># 计算 labels</span><br><span class="line">labels = pipeline.predict(samples)</span><br><span class="line"></span><br><span class="line"># 根据 labels 和 species 组合成dataframe</span><br><span class="line">df = pd.DataFrame(&#123;&apos;labels&apos;:labels,&apos;species&apos;:species&#125;)</span><br><span class="line"></span><br><span class="line"># 计算 crosstab</span><br><span class="line">ct = pd.crosstab(df[&apos;labels&apos;],df[&apos;species&apos;])</span><br><span class="line"></span><br><span class="line"># 打印 ct</span><br><span class="line">print(ct)</span><br></pre></td></tr></table></figure><blockquote><p>如果你对 <em>crosstab</em> 函数不是很了解，<a href="https://medium.com/@yangdustin5/quick-guide-to-pandas-pivot-table-crosstab-40798b33e367" target="_blank" rel="noopener">这里</a>有 <em>pandas</em> 中关于<em>crosstab</em> 函数与 <em>pivot_table</em> 函数的对比。</p></blockquote><h2 id="根据股价变动聚类股票"><a href="#根据股价变动聚类股票" class="headerlink" title="根据股价变动聚类股票"></a>根据股价变动聚类股票</h2><p>我们这里有一份股价数据，它的shape为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [2]: movements.shape</span><br><span class="line">Out[2]: (60, 963)</span><br></pre></td></tr></table></figure><p>其中每一行是一个公司的数据，对于 movements 其中的每一行数据，长度为963:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [5]: len(movements[0])</span><br><span class="line">Out[5]: 963</span><br></pre></td></tr></table></figure><p>其记录的是这个公司每天的股价波动情况。</p><p>我们前面处理的数据，在缩放的时候，我们使用的是 <em>‌StandardScaler</em>, standardscaler缩放的方式是移除所有数据的平均值然后缩放到一个维度，但对于股价来说，我们使用 <em>‌Normalizer</em>，它会针对每个公司的股价分别缩放，彼此独立。</p><p><strong>缩放数据并创建管道</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Import Normalizer</span><br><span class="line">from sklearn.preprocessing import Normalizer</span><br><span class="line"></span><br><span class="line"># Create a normalizer: normalizer</span><br><span class="line">normalizer = Normalizer()</span><br><span class="line"></span><br><span class="line"># Create a KMeans model with 10 clusters: kmeans</span><br><span class="line">kmeans = KMeans(n_clusters=10)</span><br><span class="line"></span><br><span class="line"># Make a pipeline chaining normalizer and kmeans: pipeline</span><br><span class="line">pipeline = make_pipeline(normalizer,kmeans)</span><br><span class="line"></span><br><span class="line"># Fit pipeline to the daily price movements</span><br><span class="line">pipeline.fit(movements)</span><br></pre></td></tr></table></figure><p><strong>聚类股票</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Import pandas</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># Predict the cluster labels: labels</span><br><span class="line">labels = pipeline.predict(movements)</span><br><span class="line"></span><br><span class="line"># Create a DataFrame aligning labels and companies: df</span><br><span class="line">df = pd.DataFrame(&#123;&apos;labels&apos;: labels, &apos;companies&apos;: companies&#125;)</span><br><span class="line"></span><br><span class="line"># Display df sorted by cluster label</span><br><span class="line">print(df.sort_values(by=&apos;labels&apos;))</span><br></pre></td></tr></table></figure><p>这里的companies数据，是在导入数据之前就剔出来了的。</p><h1 id="可视化层次结构"><a href="#可视化层次结构" class="headerlink" title="可视化层次结构"></a>可视化层次结构</h1><p>对于那些没有技术背景的人来说，最好使用图表表达你的发现。对于非监督学习这部分，我们学习两种可视化技术：</p><ul><li>t-SNE</li><li>Hierarchical clustring</li></ul><p>我们先介绍 <em>Hierarchical clustring</em>。</p><h2 id="Hierarchical-clustring"><a href="#Hierarchical-clustring" class="headerlink" title="Hierarchical clustring"></a>Hierarchical clustring</h2><p>Hierarchical clustring 的原理是，我们处理数据时，第一次使用比较大的 <em>‌n_clusters</em>，随后慢慢缩小，到最后只剩一个 <em>‌n_clusters</em>。</p><p>为了绘制Hierarchical clustring，我们需要的包主要是 linkage,dendrogram.</p><ul><li>linkage: 负责计算层次聚类信息</li><li>dendrogram: 负责可视化</li></ul><p>下面我们以一份谷物数据举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Perform the necessary imports</span><br><span class="line">from scipy.cluster.hierarchy import linkage, dendrogram</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># Calculate the linkage: mergings</span><br><span class="line">mergings = linkage(samples,method=&apos;complete&apos;)</span><br><span class="line"></span><br><span class="line"># Plot the dendrogram, using varieties as labels</span><br><span class="line">dendrogram(mergings,</span><br><span class="line">           labels=varieties,</span><br><span class="line">           leaf_rotation=90,</span><br><span class="line">           leaf_font_size=6,</span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/19/kaw1DUq2AYxlEfi.png" alt=""></p><h2 id="股票的层次化示例"><a href="#股票的层次化示例" class="headerlink" title="股票的层次化示例"></a>股票的层次化示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Import normalize</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line"></span><br><span class="line"># Normalize the movements: normalized_movements</span><br><span class="line">normalized_movements = normalize(movements)</span><br><span class="line"></span><br><span class="line"># Calculate the linkage: mergings</span><br><span class="line">mergings = linkage(normalized_movements,method=&apos;complete&apos;)</span><br><span class="line"></span><br><span class="line"># Plot the dendrogram</span><br><span class="line">dendrogram(mergings,labels=companies,leaf_rotation=90,leaf_font_size=6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这里我们来对比下标准化与无标准化的结果,即<code>linkage(normalized_movements,method=&#39;complete&#39;)</code>与<code>linkage(movements,method=&#39;complete&#39;)</code>的区别：</p><p><img src="https://i.loli.net/2019/11/19/Ns8USVibcFehlxw.png" alt="not normalized"></p><p><img src="https://i.loli.net/2019/11/19/QtBcrdTgPMA6U73.png" alt="normalized"></p><h2 id="intermediate-clustering"><a href="#intermediate-clustering" class="headerlink" title="intermediate clustering"></a>intermediate clustering</h2><p>dendrogram的图是有高度的，它的高度值描述的是两个features之间的距离，chevron与exxon的距离是它们两者merge的时候那个柱子的高度，也就是这两者之间的距离，在这个层面，它们之间的距离是最短的。</p><p>而看整个图像的话，图像最高的部分发生的合并，则说明它们之间的距离是最远的。</p><p>当我们在计算层次聚类信息的时候，linkage方法中有一个method，我们把它指定为 <em>complete</em>,它的意思是我们不会限定 features之间的距离，我需要计算所有features之间的距离，不管他们多远；如果我在这里作出限制，比如15，那么两个features之间距离超过15的部分我就忽略了。</p><p>怎么从一堆数据集中，根据 features之间的距离 来筛选数据呢？我们需要 fcluster 方法。</p><p>我们看看设定高度为 15 的情况: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Perform the necessary imports</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.cluster.hierarchy import linkage, dendrogram</span><br><span class="line"></span><br><span class="line"># Calculate the linkage: mergings</span><br><span class="line">mergings = linkage(samples,method=&apos;single&apos;)</span><br><span class="line"></span><br><span class="line"># Plot the dendrogram</span><br><span class="line">dendrogram(mergings,labels=country_names,leaf_rotation=90,leaf_font_size=6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/19/FoWTypesfNmHgtI.png" alt=""></p><p>下面是fcluster方法使用示例,筛选高度为6以内的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Perform the necessary imports</span><br><span class="line">import pandas as pd</span><br><span class="line">from scipy.cluster.hierarchy import fcluster</span><br><span class="line"></span><br><span class="line"># Use fcluster to extract labels: labels</span><br><span class="line">labels = fcluster(mergings,6,criterion=&apos;distance&apos;)</span><br><span class="line"></span><br><span class="line"># Create a DataFrame with labels and varieties as columns: df</span><br><span class="line">df = pd.DataFrame(&#123;&apos;labels&apos;: labels, &apos;varieties&apos;: varieties&#125;)</span><br><span class="line"></span><br><span class="line"># Create crosstab: ct</span><br><span class="line">ct = pd.crosstab(df[&apos;labels&apos;],df[&apos;varieties&apos;])</span><br><span class="line"></span><br><span class="line"># Display ct</span><br><span class="line">print(ct)</span><br></pre></td></tr></table></figure><h1 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h1><p>t-sne算法的原理就是把高维度的数据转化成2维或者3维的，所以它是一种用于降维的机器学习方法。当我们想要对高维数据进行分类，又不清楚这个数据集有没有很好的可分性（即同类之间间隔小，异类之间间隔大），可以通过t-SNE投影到2维或者3维的空间中观察一下。</p><ul><li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li><li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li></ul><p>我们看到t-SNE模型是非监督的降维，他跟kmeans等不同，他不能通过训练得到一些东西之后再用于其它数据（比如kmeans可以通过训练得到k个点，再用于其它数据集，而t-SNE只能单独的对数据做操作，也就是说他只有fit_transform，而没有fit操作）</p><blockquote><p>参考 <a href="https://blog.csdn.net/hustqb/article/details/78144384" target="_blank" rel="noopener">数据降维与可视化——t-SNE</a> 以及 <a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html" target="_blank" rel="noopener">t-SNE完整笔记</a></p></blockquote><p>t-sne中有两个地方值得注意的，一个是fit_transform方法，它的作用是将X投影到一个嵌入空间并返回转换结果，另一个是learning_rate,它是学习率，建议取值为10.0-1000.0。</p><p>t-sne生成的数据是不一样的，尽管你绘图的代码是一样的。</p><p><strong>TSNE对于谷物数据的可视化</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Import TSNE</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line"></span><br><span class="line"># Create a TSNE instance: model</span><br><span class="line">model = TSNE(learning_rate=200)</span><br><span class="line"></span><br><span class="line"># Apply fit_transform to samples: tsne_features</span><br><span class="line">tsne_features = model.fit_transform(samples)</span><br><span class="line"></span><br><span class="line"># Select the 0th feature: xs</span><br><span class="line">xs = tsne_features[:,0]</span><br><span class="line"></span><br><span class="line"># Select the 1st feature: ys</span><br><span class="line">ys = tsne_features[:,1]</span><br><span class="line"></span><br><span class="line"># Scatter plot, coloring by variety_numbers</span><br><span class="line">plt.scatter(xs,ys,c=variety_numbers)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/19/kj4BNZbtY1l7Qzx.png" alt=""></p><p><strong>TSNE对于股票数据的可视化</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Import TSNE</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line"></span><br><span class="line"># Create a TSNE instance: model</span><br><span class="line">model = TSNE(learning_rate=50)</span><br><span class="line"></span><br><span class="line"># Apply fit_transform to normalized_movements: tsne_features</span><br><span class="line">tsne_features = model.fit_transform(normalized_movements)</span><br><span class="line"></span><br><span class="line"># Select the 0th feature: xs</span><br><span class="line">xs = tsne_features[:,0]</span><br><span class="line"></span><br><span class="line"># Select the 1th feature: ys</span><br><span class="line">ys = tsne_features[:,1]</span><br><span class="line"></span><br><span class="line"># Scatter plot</span><br><span class="line">plt.scatter(xs,ys,alpha=0.5)</span><br><span class="line"></span><br><span class="line"># Annotate the points</span><br><span class="line">for x, y, company in zip(xs, ys, companies):</span><br><span class="line">    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/19/ONSqKFLUwQ7EDBJ.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/19/GIND7pRkxahrXBu.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;非监督学习：即根据现有的数据去发掘存在数据中的一些模式,比如根据用户的购买记录，(&lt;em&gt;clustering&lt;/em&gt;) 定义用户画像, 或根据数据的模式来压缩数据（&lt;em&gt;dimension reduction&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="sklearn" scheme="http://wittyfans.com/tags/sklearn/"/>
    
      <category term="data science" scheme="http://wittyfans.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>机器学习：处理jira工单的分类问题</title>
    <link href="http://wittyfans.com/coding/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%84%E7%90%86jira%E5%B7%A5%E5%8D%95%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98.html"/>
    <id>http://wittyfans.com/coding/机器学习：处理jira工单的分类问题.html</id>
    <published>2019-11-18T12:10:20.000Z</published>
    <updated>2019-11-18T12:12:17.045Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>如何根据jira工单的category自动找到处理它的组呢？这是一个利用机器学习中knn算法的小实践.</p></blockquote><a id="more"></a><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>我们使用knn分类算法，knn是广为使用的一种机器学习算法，应用于各种应用，如金融、医疗保健、政治学、手写字识别、图像识别与视频识别。</p><p>在信用评级中，金融机构用来预测客户的信用评级。在贷款支付机构中，银行来预测贷款是否安全。在政治中，它被用来预测潜在选名是否会投票。</p><p>简单介绍下knn:</p><p>knn是non-parametric算法，即无假设的意思，它不会对基础数据的分布做任何假设，即不会根据数据集来确定模型结构。</p><p>这很有用，因为在现实世界中，很多的数据集是不遵从数学理论的假设的。</p><p>同时knn是lazy algorithm算法，即它不需要在模型建立前做大量的数据训练工作。</p><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><p>我们已经把一些数据保存到了jira数据库，现在将它们的分类与所属组拿出来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sql = select category,assign_group from issues limit 10000</span><br><span class="line">db = pymysql.connect(&apos;db_host&apos;,&apos;db_username&apos;,&apos;db_pwd&apos;,&apos;db&apos;)</span><br><span class="line">df = pd.read_sql(sql,con=db)</span><br></pre></td></tr></table></figure><p>现在数据已经保存到了df中。</p><h1 id="A-basic-Exapple"><a href="#A-basic-Exapple" class="headerlink" title="A basic Exapple"></a>A basic Exapple</h1><h2 id="单个features"><a href="#单个features" class="headerlink" title="单个features"></a>单个features</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing</span><br><span class="line">le = preprocessing.LabelEncoder()</span><br><span class="line">category_encoded = le.fit_transform(df.category)</span><br><span class="line">reporter_encoded = le.fit_transform(reporter)</span><br><span class="line">features = list(zip(category_encoded,reporter_encoded))</span><br><span class="line">label = le.fit(df.assign_group)</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=3)</span><br><span class="line">knn.fit(features,label)</span><br><span class="line">knn.predict([[0,2]])</span><br></pre></td></tr></table></figure><h2 id="多个features"><a href="#多个features" class="headerlink" title="多个features"></a>多个features</h2><p>另外一种方式，x是我们的category，y则是所属组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neighbors import KNeighborsClassifier </span><br><span class="line"></span><br><span class="line">x_df = df.category</span><br><span class="line">x_train = pd.get_dummies(x_df)</span><br><span class="line"></span><br><span class="line">y_df = df.assign_group</span><br><span class="line">y_train = y_df.values</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=6)</span><br><span class="line">knn.fit(x_train,y_train)</span><br></pre></td></tr></table></figure><p>随后你可以直接将x_train传给knn做个预测，然后跟真实的数据做个对比。</p><blockquote><p>拿训练的数据来做预测是不可取的，你必须给模型一些它从来没有看见过的数据预测，这样才能准确的测试模型的性能</p></blockquote><p>你也可以自己生成一些数据来查看，如你已经知道get_dummies后的df有144列，那么你可以自行生成一个2d的dummie数组传给knn.predit.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr_list = np.concatenate(np.zeros((1,143)))</span><br><span class="line">temp_arr = arr_list.tolist()</span><br><span class="line">temp_arr.insert(44,1)</span><br><span class="line">new_x = [temp_arr.insert]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output: array([&apos;Helpdesk CN&apos;])</span><br></pre></td></tr></table></figure><h2 id="分割数据并查看得分"><a href="#分割数据并查看得分" class="headerlink" title="分割数据并查看得分"></a>分割数据并查看得分</h2><p>下面演示了将数据拆分成train,和test后的预测分数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)</span><br><span class="line"></span><br><span class="line">knn= KNeighborsClassifier(n_neighbors=6)</span><br><span class="line">knn.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line">knn.score(X_test,y_test)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output:0.781</span><br></pre></td></tr></table></figure><p>78%，这个模型效果还是不错的！</p><h2 id="选择Neighbors"><a href="#选择Neighbors" class="headerlink" title="选择Neighbors"></a>选择Neighbors</h2><p>我们知道对于knn算法，其中一个关键的参数是neighbors,那么对于不同的neighbors，我们的accuracy的分数是什么样的呢？我们可以来画个图检测一下：</p><p><img src="https://i.loli.net/2019/11/18/Qzlb7BVyCJT3X6F.png" alt=""></p><p>可以看到neighbors在6-7之间，accuracy是最高的，在2-5之间，test的得分竟然比训练数据都要高。</p><h2 id="使用knn的优缺点"><a href="#使用knn的优缺点" class="headerlink" title="使用knn的优缺点"></a>使用knn的优缺点</h2><ul><li>knn算法对于处理非线性数据很有用，可以用它一起处理回归问题</li><li>knn的测试阶段较慢且耗资源，需要很多的内存来保存来预测的整个数据集</li><li>强烈建议将数据进行同比例的标准化，比如将所有数据缩放到0,1的区间内</li><li>knn不适合太高维度的数据，在那种情况下，需要将维度降低</li><li>处理缺失值有助于改善模型的准确度</li></ul><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p><strong>1. knn fit其中的x,x一定只能是get_dummies算出来的数据吗？</strong></p><p>不是，比如 iris 的数据摘要，X的数据是各个部分的长、宽:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[5.1,3.5,1.4,0.2],</span><br><span class="line">[4.9,3,1.4,0.2]]</span><br></pre></td></tr></table></figure><p><strong>2. knn fit其中的y，y可以是原始数据类型吗？如a,b,a,c</strong></p><p>可以，如 datacamp 题目中的政党数据，y=df[‘party’]:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Out[8]: </span><br><span class="line">0      republican</span><br><span class="line">1      republican</span><br><span class="line">2        democrat</span><br><span class="line">3        democrat</span><br><span class="line">4        democrat</span><br><span class="line">5        democrat</span><br><span class="line">6        democrat</span><br><span class="line">7      republican</span><br><span class="line">8      republican</span><br><span class="line">9        democrat</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;如何根据jira工单的category自动找到处理它的组呢？这是一个利用机器学习中knn算法的小实践.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="jira" scheme="http://wittyfans.com/tags/jira/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="machine learning" scheme="http://wittyfans.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>数学复习：求和符号</title>
    <link href="http://wittyfans.com/math/%E6%95%B0%E5%AD%A6%E5%A4%8D%E4%B9%A0%EF%BC%9A%E6%B1%82%E5%92%8C%E7%AC%A6%E5%8F%B7.html"/>
    <id>http://wittyfans.com/math/数学复习：求和符号.html</id>
    <published>2019-11-16T08:00:56.000Z</published>
    <updated>2019-11-16T08:09:59.554Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>简单介绍一下了求和符号的定义与运算规则.</p></blockquote><a id="more"></a><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="求和符号"><a href="#求和符号" class="headerlink" title="求和符号"></a>求和符号</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>$$\sum$$</p><p>叫做求和符号，读作<em>sigma</em>,它的意思就是连续的加法，比如:</p><p>$$\sum_{k=1}^{n}ak=a_1+a_2+a_3+..a_n$$</p><p>k是下标，它会从k变化到n，当k=1的时候，ak就是a1，当k=2的时候，ak就是a2，最后一项就是k=n,segima的意思呢就是把这些项全都加起来。</p><p>这里再多提一点初学者容易犯的错误,请看下面的公式,其中c为常数：</p><p>$$\sum_{k=1}^{n}C=nc$$</p><p>这里的结果等于<em>n乘c</em>,而不是<em>c</em>。</p><h2 id="分拆求和符号"><a href="#分拆求和符号" class="headerlink" title="分拆求和符号"></a>分拆求和符号</h2><p><em>segima</em>可以分拆，请看下面的公式:</p><p>$$\sum_{k=1}^{n}(a_k+b_k)=\sum_{k=1}^{n}a_k+\sum_{k=1}^{n}b_k$$</p><p>我们先来分析第一部分:<br>$$\sum_{k=1}^{n}(a_k+b_k)$$</p><p>它的意思呢就是：<br>$$(a_1+b_1)+(a_2+b_2)+…+(a_n+b_n)$$</p><p>我们把 ai 与 b_i 单独拿出来排列，得到:</p><p>$$(a_1+a_2+a_3+a_i)+(b_1+b_2+b_3+b_n)$$</p><p>那么根据sigma的性质，即得到:</p><p>$$\sum_{k=1}^{n}a_k+\sum_{k=1}^{n}b_k$$</p><h2 id="常数可脱离"><a href="#常数可脱离" class="headerlink" title="常数可脱离"></a>常数可脱离</h2><p>$$\sum_{k=1}^{n}Cak=C\sum_{k=1}^{n}ak$$</p><p>首先看左边，等于:</p><p>$$Ca_1+Ca_2+Ca_n$$</p><p>那么把C提出来，则得：</p><p>$$C(a_1+a_2+a_n)$$</p><p>即：</p><p>$$C\sum_{k=1}^{n}a_k$$</p><h2 id="乘除不等"><a href="#乘除不等" class="headerlink" title="乘除不等"></a>乘除不等</h2><p>$$\sum_{k=1}^{n}(a_k*b_k)≠\sum_{k=1}^{n}(a_k)\sum_{k=1}^{n}(b_k)$$</p><p>证明很简单,让 $$n=2$$ 显然下列两项不相等：</p><p>$$(a_1b_1)+(a_2b_2)≠(a_1+a_2)(b_1+b_2)$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;简单介绍一下了求和符号的定义与运算规则.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="math" scheme="http://wittyfans.com/categories/math/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="math" scheme="http://wittyfans.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>数学复习：指数与对数</title>
    <link href="http://wittyfans.com/math/%E6%95%B0%E5%AD%A6%E5%A4%8D%E4%B9%A0%EF%BC%9A%E6%8C%87%E6%95%B0%E5%AF%B9%E6%95%B0.html"/>
    <id>http://wittyfans.com/math/数学复习：指数对数.html</id>
    <published>2019-11-16T07:57:44.000Z</published>
    <updated>2019-11-16T08:00:43.911Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章包括指数与对数的性质，函数图像等基本知识。</p></blockquote><a id="more"></a><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="指数"><a href="#指数" class="headerlink" title="指数"></a>指数</h1><h2 id="指数的性质"><a href="#指数的性质" class="headerlink" title="指数的性质"></a>指数的性质</h2><p>$$a^{mn}=(a^m)^n$$<br>$$a^\frac{m}{n}=\sqrt[^n!]{(a^m)}$$<br>$$a^{m+n}=a^ma^n$$<br>$$a^{m-n}=\frac{a^m}{a^n}$$<br>$$a^{-m}=\frac{1}{a^m}$$</p><h2 id="指数函数的图像与性质"><a href="#指数函数的图像与性质" class="headerlink" title="指数函数的图像与性质"></a>指数函数的图像与性质</h2><p>$$y= a^x(a&gt;0,a≠1)$$</p><ul><li>x可取全体实数</li><li>结果y&gt;0</li><li>图像永远经过(0,1)点</li><li>a&gt;1,x变大，y也变大</li><li>0&lt;a&lt;1,x变大，y变小</li><li>第一象限内（右上），图像忘左上方向，a变大</li></ul><h2 id="指数函数举例"><a href="#指数函数举例" class="headerlink" title="指数函数举例"></a>指数函数举例</h2><p>求：</p><p>$$2^{-\frac{2}{3}}$$</p><p>得<br>$$\frac{1}{\sqrt[^3]{2^2}}$$</p><blockquote><p>提示:一个数的负次方即为这个数的正次方的倒数</p></blockquote><h1 id="对数"><a href="#对数" class="headerlink" title="对数"></a>对数</h1><p>如果:<br>$$a^b = N$$</p><p>那么<code>b叫做以a为底N的对数</code>,记为：</p><p>$$\log_a^N$$</p><h2 id="对数的性质"><a href="#对数的性质" class="headerlink" title="对数的性质"></a>对数的性质</h2><p>$$a^b=N \Longleftrightarrow b=\log_a^N$$<br>将<em>b</em>代入左边a的上面，可以推出来,<br>$$a^{log_a^N}=N$$</p><p>其实这里的意思就是<code>对n取对数再取指数</code></p><p>换底公式：</p><p>$$\log_a^b=\frac{log_c^b}{log_c^a}$$</p><p>和、差公式：</p><p>$$\log_a^{mn}=\log_a^m+\log_a^n$$<br>$$\log_a^{\frac{m}{n}}=\log_a^m-\log_a^n$$<br>幂公式，<br>$$\log_a^{(n^k)}=k\log_a^n$$<br>$$\log_{(a^k)}^n=\frac{1}{k}\log_a^n$$<br>倒数，<br>$$\log_a^b=\frac{1}{log_b^a}$$</p><h2 id="对数函数"><a href="#对数函数" class="headerlink" title="对数函数"></a>对数函数</h2><p>$$y=\log_{a}x(a&gt;0,a≠1)$$</p><ul><li>对数函数是指数函数的反函数，单调性相同</li><li>x可取全体正数</li><li>y可为所有值</li><li>永远经过(1,0)点</li><li>a&gt;1,x变大，y也变大，a&lt;1，x变大，y变小</li><li>在第一象限，图像往右下，a变大</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这篇文章包括指数与对数的性质，函数图像等基本知识。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="math" scheme="http://wittyfans.com/categories/math/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="math" scheme="http://wittyfans.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化基础与技术</title>
    <link href="http://wittyfans.com/coding/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9F%BA%E7%A1%80%E4%B8%8E%E6%8A%80%E6%9C%AF.html"/>
    <id>http://wittyfans.com/coding/数据可视化基础与技术.html</id>
    <published>2019-11-15T09:26:51.000Z</published>
    <updated>2019-11-15T14:24:41.818Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/15/kIahY4xBReDAVob.png" alt=""></p><blockquote><p>数据可视化的目的是准确而客观的讲述故事与展示数据，希望这篇文章可以对你的可视化工作有所帮助。</p></blockquote><a id="more"></a><h1 id="数据可视化基础与技术"><a href="#数据可视化基础与技术" class="headerlink" title="数据可视化基础与技术"></a>数据可视化基础与技术</h1><h1 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h1><ul><li>标度(scale)</li><li>坐标系(coord)</li><li>美学（aesthetic）</li><li>离散的（discrete）</li><li>将什么什么作为y轴，我们说, <em>mapping xx onto the y axis</em></li><li>我们感兴趣的关键变量，是key variable of interest</li><li>直角坐标系 Cartesian coordinate system</li><li>连续的位置标度：continuous position scales</li></ul><h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>aesthetic的一个关键的要素（critical component）是，元素的位置。</p><p>其次是，元素之间的颜色，字体，背景等等。</p><p>需要了解数据有两类，一类是连续型数据（continuous data），另一类是非连续型（discrete numerical values）。</p><p>比如时间间隔（time duration），是连续型的。<br>而一个房间里人员的数量则是非连续的，离散的。</p><p><strong>一般来说，位置、大小、颜色和线宽可以表示连续数据，但是形状和线型通常只能表示离散数据。</strong></p><p>数据是数字，我们称为定量。<br>数据是分类，我们称为定性。</p><p>常见的图都是三套scales,但你也可以定义更多的scale.</p><h1 id="坐标系和轴"><a href="#坐标系和轴" class="headerlink" title="坐标系和轴"></a>坐标系和轴</h1><p>坐标系和轴就不多介绍了。<br>在坐标系和轴上的数据，不仅仅是数字，也可以是任何单位，比如温度，距离。</p><h2 id="非线性轴"><a href="#非线性轴" class="headerlink" title="非线性轴"></a>非线性轴</h2><p>在轴上，均匀的间距与值的对应关系是有讲究的。特别是当你的值之间的间距是不一样而轴的标尺是一样的，或者相反的情况出现的时候。</p><p>你的轴和你的值之间的间距，必须有一个是固定的。</p><blockquote><p>When plotting log-transformed data, always specify the base in the labeling of the axis.</p></blockquote><p>如何让你的坐标是以倍数为轴来呈现？</p><h3 id="对数刻度"><a href="#对数刻度" class="headerlink" title="对数刻度"></a>对数刻度</h3><p>如果数据的最小值和最大值之间的数据很多且非常连续，那么使用对数刻度比较合适。</p><h3 id="平方根刻度"><a href="#平方根刻度" class="headerlink" title="平方根刻度"></a>平方根刻度</h3><p>对于对数刻度，如果有0怎么办？这时候可以考虑平方根刻度，它允许0的存在。在线性标度上，一个单位步长对应于一个常数值的加法或减法；在对数标度上，它对应于一个常数值的乘积或除，而平方根刻度则没有这种要求。</p><h3 id="极坐标轴"><a href="#极坐标轴" class="headerlink" title="极坐标轴"></a>极坐标轴</h3><p>极坐标类似太阳系的行星轨道。它对于一些重复的周期性数据很有用，比如一年的最后一天，也是第二年的第一天。</p><p>再比如，对于不同城市平均温度的绘制，使用极坐标的效果更好。</p><h1 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h1><p>颜色的用途主要是几种：</p><ul><li>区分数据组</li><li>展示数据</li><li>高亮特定的值</li></ul><h2 id="区分"><a href="#区分" class="headerlink" title="区分"></a>区分</h2><p>对于离散discrete数据或者不同组别的数据，使用颜色区分是很常见的。<br>这里有个定性色标的概念(qualitative color scale)，这些被选出来的颜色，可以比较清晰的区分彼此，并且不会让你产生颜色之间有顺序的感觉。</p><h2 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h2><p>颜色可以用来展示数据（主要指连续型的，比如速度，温度，收入等），这样的颜色组合我们叫做顺序色标（sequential color scale），但如果你要展示的值其中还包括负值，那就需要diverging color scale了，这种颜色的分布在中间是相对比较白或者透明的，两端则是对比明显颜色。</p><h2 id="标注"><a href="#标注" class="headerlink" title="标注"></a>标注</h2><p>对于某些carry key information的数据，我们需要将其标注出来，于是我们有了accent color scales，我们需要选择一个有表现力的颜色，同时一个不争夺我们注意力的颜色作为背景。</p><h1 id="选择合适的图"><a href="#选择合适的图" class="headerlink" title="选择合适的图"></a>选择合适的图</h1><h2 id="数量"><a href="#数量" class="headerlink" title="数量"></a>数量</h2><ul><li>bar(group,stack,vertically,horizontally)</li><li>dots</li><li>heatmap</li></ul><h2 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h2><ul><li>hist</li><li>density plots</li><li>cdf,累计密度图</li><li>qq plots</li><li>boxplots</li><li>violins</li><li>strip charts</li><li>sina plots</li><li>stacked hist</li><li>overlapping densities</li><li>ridgeline plot</li></ul><p>如果我们只对分布的移动趋势有兴趣，那么Boxplots, violins, strip charts, and sina plots是不错的选择。</p><p>如果你想对数据有更深层次的理解，同时你的数据集也比较小，可以使用stacked hist。</p><h2 id="比例"><a href="#比例" class="headerlink" title="比例"></a>比例</h2><ul><li>bar</li><li>pie</li><li>stacked bar</li><li>stacked densities</li><li>mosaic plot</li><li>treemap</li><li>parallel sets</li></ul><h1 id="展现数量的图"><a href="#展现数量的图" class="headerlink" title="展现数量的图"></a>展现数量的图</h1><h2 id="Bar-plots"><a href="#Bar-plots" class="headerlink" title="Bar plots"></a>Bar plots</h2><p>如果你的图竖着摆放，有时候会出现label的文字太长被遮挡的情况，这时候可以使用水平的bar图，不建议使用旋转的label。</p><p>应该根据bar的大小（即数值的大小）来排序，而不是lable的字母。</p><h2 id="dot-plots"><a href="#dot-plots" class="headerlink" title="dot plots"></a>dot plots</h2><p>bar图的长度取决于数值的大小，另外bar图的开始都是从0开始的，这意味着如果数值全部在某个大一点的区间，它们之间的对比可能会看起来比较小。</p><p>所以dot图是个更好的选择，但也要注意要根据数值来排列点，而不是label。</p><h2 id="heatmaps"><a href="#heatmaps" class="headerlink" title="heatmaps"></a>heatmaps</h2><p>使用bar图适用于那些数据组比较少的情况，你会发现如果你的数据有5，6组，整个图形已经显得有点拥挤了，如果你有超过20组数据需要展示，那么不妨选择热力图。</p><h1 id="展现分布"><a href="#展现分布" class="headerlink" title="展现分布"></a>展现分布</h1><h2 id="hist与density"><a href="#hist与density" class="headerlink" title="hist与density"></a>hist与density</h2><p>使用hist，一般程序都会给出默认的bin，但这也许不是不是合适的bin，需要你去调整与探索以找到最适合你的bin值。</p><p>直方图从18世纪开始就很流行，因为它很容易绘制，随着手机与电脑计算能力的改善，现在直方图开始被密度图所取代，密度图的曲线更平滑，同样在density图中，也有类似hist的bin参数，叫做bandwidth。</p><p>密度图的首尾需要特别注意，处理不得当，可能会出现负值。</p><p>如果有多个组的数据需要展示分布状态，不要使用stack hist，这样很不清晰，可以使用分开的density，或者是分开的hist。</p><blockquote><p>这两者都需要去选择合适的bin与bandwidth</p></blockquote><h2 id="ECDF"><a href="#ECDF" class="headerlink" title="ECDF"></a>ECDF</h2><p>在hist与densit图中，它们的局限性在于必须选择合适的bins值与带宽值，所以，bins与带宽值的选择，本身是一种主观性的行为，那么这只能说是一种对数据的解释，不是客观的（除非你选择的是公认的合适的值），所以我们还需要一种客观的对数据进行展示的方法。</p><p>将所有的数据描绘成点值，可以突出整个数据的分布，但是没办法突出具体某一个数据点的价值。</p><p>为了解决这个问题，统计学家发明了ecdf与qq图。</p><p>这些图不需要任何参数选择，可以一次性显示所有数据。</p><p>经验分布函数的思想是，x值为所有样本中排序后的值，y为所有的样本总数按均等分（根据个数），然后累加在一起的值。</p><p>这样通过观察，就可以看到所有的样本点的分布，如果某个值出现的比较多，即x往右变化时值没有变，而y则增加了一个均等分的值，此时图像上的表现就是在x处网上增加一个点，也就是斜率变得更为陡峭了。</p><h1 id="应用技巧"><a href="#应用技巧" class="headerlink" title="应用技巧"></a>应用技巧</h1><h2 id="不同的图表"><a href="#不同的图表" class="headerlink" title="不同的图表"></a>不同的图表</h2><p>you can do this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.plt.bar()</span><br></pre></td></tr></table></figure><p>and this is the diffrent types plot:</p><ul><li>‘bar’ or ‘barh’ for bar plots</li><li>‘hist’ for histogram</li><li>‘box’ for boxplot</li><li>‘kde’ or ‘density’ for density plots</li><li>‘area’ for area plots</li><li>‘scatter’ for scatter plots</li><li>‘hexbin’ for hexagonal bin plots</li><li>‘pie’ for pie plots</li></ul><p>and you can specific the kind in methond:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(kind=&apos;bar&apos;)</span><br></pre></td></tr></table></figure><p>Reference:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter</span><br><span class="line">df.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie</span><br></pre></td></tr></table></figure><h2 id="把列合并展示"><a href="#把列合并展示" class="headerlink" title="把列合并展示"></a>把列合并展示</h2><p>When you plotting, <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://pandas.pydata.org/pandas-docs/stable/_images/bar_plot_multi_ex.png)</span><br><span class="line"></span><br><span class="line">to this:</span><br><span class="line"></span><br><span class="line">![](https://pandas.pydata.org/pandas-docs/stable/_images/bar_plot_stacked_ex.png)</span><br><span class="line"></span><br><span class="line">plot.bar(stacked=True);</span><br><span class="line"></span><br><span class="line">If you want thehorizontal bar plots, use the barh method:</span><br><span class="line"></span><br><span class="line"> ```df2.plot.barh(stacked=True)</span><br></pre></td></tr></table></figure></p><h1 id="使用绘图对象"><a href="#使用绘图对象" class="headerlink" title="使用绘图对象"></a>使用绘图对象</h1><p>Plots in matplotlib is a Figure object, you can new a plot window:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 多个图一起绘制</span><br><span class="line">and you can add the sub plots to this object,</span><br></pre></td></tr></table></figure><p>fig = plt.figure()<br>ax1 = fig.add_subplot(2, 2, 1)<br>ax2 = fig.add_subplot(2, 2, 2)<br>ax3 = fig.add_subplot(2, 2, 3)<br>ax4 = fig.add_subplot(2, 2, 4)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">This will let you get four plot sub plots in one  window, and any command will show in the last plot.</span><br><span class="line"></span><br><span class="line">## 快速创建多个子图</span><br><span class="line"></span><br><span class="line">and you create subplots fast like this:</span><br></pre></td></tr></table></figure></p><p>fig,axes = plt.subplots(3,3)<br>axes[0,0].hist(np.random.randn(100),bins=20,color=’k’,alpha=0.3)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 控制子图之间的间隙</span><br><span class="line">subplots spacing is under control by ```plt.subplots_adjust(wspace=0,hspace=0)</span><br></pre></td></tr></table></figure></p><h2 id="个性化"><a href="#个性化" class="headerlink" title="个性化"></a>个性化</h2><h3 id="线的虚实、颜色"><a href="#线的虚实、颜色" class="headerlink" title="线的虚实、颜色"></a>线的虚实、颜色</h3><p>In the plot() method, it can accept a string to stylying, to plot x verus y with green dashes , you would execute:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">of course, the color and dash can setting seperate:</span><br><span class="line"></span><br><span class="line">```ax.plot(x, y, linestyle=&apos;--&apos;, color=&apos;g&apos;)</span><br></pre></td></tr></table></figure><p>linestyle:</p><ul><li>-</li><li>--</li></ul><p>Line plots can additionally have <em>markers</em> to highlight the actual data points.</p><figure class="highlight plain"><figcaption><span>or ```plot(marker</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 一份数据，多种展示方式</span><br><span class="line">我们可以将不同的drawstyle结合在一起，放在同一幅图中显示，比如这样</span><br></pre></td></tr></table></figure><p>ri.groupby(ri.stop_datetime.dt.hour).drugs_related_stop.mean().plot(linestyle=’–’,color=’g’,marker=’o’,label=’Default’)<br>ri.groupby(ri.stop_datetime.dt.hour).drugs_related_stop.mean().plot(linestyle=’-‘,color=’r’,marker=’o’,label=’steps-post’,drawstyle=’steps-post’)<br>plt.legend(loc=’best’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://xfjlcq.bn.files.1drv.com/y4mHSUL0Fe0J2icuEUtKoUHovl1jCzOHR3BSoy5LdPsDttVbG0OJYys58TNGyXOLLj0elzuvTe8dSPxanf28ZanIF5y-qdjw_BoG3hlVLh0HiLxXbQpTRX2Mdg_wqv7jHA-cWaQdKRwpT07LUWhQNNF97VE4rtUSwCGLslxsludS7d7FTp1X9_c9z1eMPE5Nj2-Z4FZYU6zSUZXSD_iLFspsw/legendPlot.png?psid=1)</span><br><span class="line"></span><br><span class="line">drawstyle的类型：</span><br><span class="line"></span><br><span class="line">```&#123;&apos;default&apos;, &apos;steps&apos;, &apos;steps-pre&apos;, &apos;steps-mid&apos;, &apos;steps-post&apos;&#125;</span><br></pre></td></tr></table></figure></p><h3 id="多份数据合并展示"><a href="#多份数据合并展示" class="headerlink" title="多份数据合并展示"></a>多份数据合并展示</h3><p>An easy way to adding legends:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from numpy.random import randn</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(1, 1, 1)</span><br><span class="line">ax.plot(randn(1000).cumsum(), &apos;k&apos;, label=&apos;one&apos;) ax.plot(randn(1000).cumsum(), &apos;k--&apos;, label=&apos;two&apos;)</span><br><span class="line">ax.plot(randn(1000).cumsum(), &apos;k.&apos;, label=&apos;three&apos;)</span><br><span class="line">ax.legend(loc=&apos;best&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>The loc tells matplotlib where to place the plot. If you aren’t picky, ‘best’ is a good option, as it will choose a location that is most out of the way. To exclude one or more elements from the legend, pass no label or label=’<em>nolegend</em>‘.</p></blockquote><h3 id="设置图的基本属性"><a href="#设置图的基本属性" class="headerlink" title="设置图的基本属性"></a>设置图的基本属性</h3><p>设置plot的标题:</p><figure class="highlight plain"><figcaption><span>Releted Stop Search Rate')```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">设置x轴的标题：</span><br><span class="line"></span><br><span class="line">```ax.set_xlabel(&apos;hours&apos;)</span><br></pre></td></tr></table></figure><p>下面的x轴的单位和标签也可以设置，注意个数必须统一:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">drm = ri.groupby(ri.stop_datetime.dt.hour).drugs_related_stop.mean()</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(1,1,1)</span><br><span class="line">ax.plot(drm)</span><br><span class="line"></span><br><span class="line">ticks = ax.set_xticks([0,6,12,18])</span><br><span class="line">labels = ax.set_xticklabels([&apos;Mid Night&apos;,&apos;Morning&apos;,&apos;Moon&apos;,&apos;AftenNoon&apos;])</span><br></pre></td></tr></table></figure><blockquote><p>Y-axis is same, substituting y for x in above.</p></blockquote><h3 id="如何根据GroupBy中多列的值绘制"><a href="#如何根据GroupBy中多列的值绘制" class="headerlink" title="如何根据GroupBy中多列的值绘制"></a>如何根据GroupBy中多列的值绘制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig,ax = plt.subplots(figsize=(16,7))</span><br><span class="line">data.groupby(&apos;A&apos;,&apos;B&apos;).count().unstack().plot(ax=ax)</span><br></pre></td></tr></table></figure><p>Use the unstack method to plotting.</p><blockquote><p>To be contiuned…</p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://serialmentor.com/dataviz/" target="_blank" rel="noopener">Fundamentals of Data Visualization - Claus O. Wilke</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/15/kIahY4xBReDAVob.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数据可视化的目的是准确而客观的讲述故事与展示数据，希望这篇文章可以对你的可视化工作有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="data visualizing" scheme="http://wittyfans.com/tags/data-visualizing/"/>
    
      <category term="matplotlib" scheme="http://wittyfans.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>监督学习实例：学校图书数据分类</title>
    <link href="http://wittyfans.com/coding/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E5%AD%A6%E6%A0%A1%E5%9B%BE%E4%B9%A6%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB.html"/>
    <id>http://wittyfans.com/coding/监督学习实例：学校图书数据分类.html</id>
    <published>2019-08-17T16:02:27.000Z</published>
    <updated>2019-11-18T03:11:14.378Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p><img src="https://i.loli.net/2019/11/18/SbPAWvxuiRELnO6.png" alt=""></p><blockquote><p>我们学校比旁边的学校在教科书上面花了更多的钱吗？这是否有用呢？</p></blockquote><a id="more"></a><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>学校想知道，我们比旁边的学校在教科书上面花了更多的钱吗？这是否有用？</p><p>要回答这个问题，首先我们需要有教科书的数据，并进行分类。然而分类是一个及其复杂的操作，学校每年都会花很多的时间去手动分类。</p><p>我们的目标是可以建立一个机器学习模型自动处理这个分类步骤。</p><p>比如《线性代数》，这本书我们会给他几个标签：</p><ul><li>数学</li><li>教科书</li><li>中学</li></ul><p>这些标签，就是我们的target variable。</p><p>这是一个典型的分类问题。</p><p>不过我们的预测应该由概率来定义，我们不会预测说，这就是本数学书，而是说,我有百分之60的把握认为这是一本数学书，如果不对，那我有百分之70的把握认为这是一本物理书。</p><h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><p>数据是csv格式的，我们使用<code>df = pd.read_csv(&#39;TrainingData.csv&#39;)</code>导入数据并保存到名为<code>df</code>的变量。</p><h2 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h2><p><strong>基本信息</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">In [6]: df.info()</span><br><span class="line">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">RangeIndex: 1560 entries, 0 to 1559</span><br><span class="line">Data columns (total 26 columns):</span><br><span class="line">Unnamed: 0                1560 non-null int64</span><br><span class="line">Function                  1560 non-null object</span><br><span class="line">Use                       1560 non-null object</span><br><span class="line">Sharing                   1560 non-null object</span><br><span class="line">Reporting                 1560 non-null object</span><br><span class="line">Student_Type              1560 non-null object</span><br><span class="line">Position_Type             1560 non-null object</span><br><span class="line">Object_Type               1560 non-null object</span><br><span class="line">Pre_K                     1560 non-null object</span><br><span class="line">Operating_Status          1560 non-null object</span><br><span class="line">Object_Description        1461 non-null object</span><br><span class="line">Text_2                    382 non-null object</span><br><span class="line">SubFund_Description       1183 non-null object</span><br><span class="line">Job_Title_Description     1131 non-null object</span><br><span class="line">Text_3                    677 non-null object</span><br><span class="line">Text_4                    193 non-null object</span><br><span class="line">Sub_Object_Description    364 non-null object</span><br><span class="line">Location_Description      874 non-null object</span><br><span class="line">FTE                       449 non-null float64</span><br><span class="line">Function_Description      1340 non-null object</span><br><span class="line">Facility_or_Department    252 non-null object</span><br><span class="line">Position_Extra            1026 non-null object</span><br><span class="line">Total                     1542 non-null float64</span><br><span class="line">Program_Description       1192 non-null object</span><br><span class="line">Fund_Description          819 non-null object</span><br><span class="line">Text_1                    1132 non-null object</span><br><span class="line">dtypes: float64(2), int64(1), object(23)</span><br><span class="line">memory usage: 317.0+ KB</span><br></pre></td></tr></table></figure><p><strong>简单描述:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [5]: df.describe()</span><br><span class="line">Out[5]:</span><br><span class="line">          Unnamed: 0         FTE         Total</span><br><span class="line">count    1560.000000  449.000000  1.542000e+03</span><br><span class="line">mean   227767.180128    0.493532  1.446867e+04</span><br><span class="line">std    130207.535688    0.452844  7.916752e+04</span><br><span class="line">min       198.000000   -0.002369 -1.044084e+06</span><br><span class="line">25%    113690.750000         NaN           NaN</span><br><span class="line">50%    226445.500000         NaN           NaN</span><br><span class="line">75%    340883.500000         NaN           NaN</span><br><span class="line">max    450277.000000    1.047222  1.367500e+06</span><br></pre></td></tr></table></figure><h2 id="FTE-全职员工数"><a href="#FTE-全职员工数" class="headerlink" title="FTE 全职员工数"></a>FTE 全职员工数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [4]: df.FTE.head()</span><br><span class="line">Out[4]:</span><br><span class="line">198     NaN</span><br><span class="line">209     NaN</span><br><span class="line">750     1.0</span><br><span class="line">931     NaN</span><br><span class="line">1524    NaN</span><br><span class="line">Name: FTE, dtype: float64</span><br></pre></td></tr></table></figure><p>数据里的FTE为(Full Time equivalent)全职员工的意思,在我们的数据中，如果一项预算与一个员工有关，这个值就反应了这个员工的全职工作的百分比。</p><ul><li>1，全职员工</li><li>0，兼职或者合同制员工</li></ul><p>这个值本身是有许多的数据缺失的，所以如果要使用的话，需要先将na值去掉。</p><p>将FTE绘图，可以看到，这所学校的兼职员工和全职员工的支出很高，而中间的数据则比较少。</p><p><img src="https://i.loli.net/2019/11/15/2cYvGT3VPQhU5dE.png" alt=""></p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>我们的数据中，有些列只有特定的值，比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.label.unique()</span><br><span class="line"></span><br><span class="line">[&apos;a&apos;,&apos;b&apos;]</span><br></pre></td></tr></table></figure><p>我们需要将其变成数字来处理，一是我们的模型只能计算数字，而是可以提升速度。</p><p>在pandas中，有一种 <code>category</code> 类型的数据，可以干这件事。</p><p>通过pandas的 <code>astype()</code>方法，可以将一列数据转化成 <code>category</code>，转化后，就可以使用 <code>get_dummies方法</code>来查看 dummy variables,这会让原来的每个值都用数字来替代,如：</p><p><em>原始数据：</em></p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">origin</th></tr></thead><tbody><tr><td style="text-align:left">0</td><td style="text-align:left">US</td></tr><tr><td style="text-align:left">1</td><td style="text-align:left">Europe</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">Asia</td></tr></tbody></table><p><em>get_dummies()后：</em></p><table><thead><tr><th style="text-align:left">origin_Asia</th><th style="text-align:left">origin_Europe</th><th style="text-align:left">origin_US</th></tr></thead><tbody><tr><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td></tr></tbody></table><p>这个步骤又叫做 <code>binary indicator</code> representation.</p><p>如果你有多列数据需要转化成 <code>category</code>类型，可以使用 lambda 函数加dataframe的apply方法，记得设置axis=0，也就是处理方式是按列，处理完成之后，你可以使用<code>df.dtypes.value_counts()</code>来查看你的数据里数据类型的分布情况。</p><h1 id="如何定义成功？"><a href="#如何定义成功？" class="headerlink" title="如何定义成功？"></a>如何定义成功？</h1><p>用accuracy，没办法解决垃圾邮件的问题，这个我们在机器学习那一章已经讲过，所以我们使用<code>log loss</code>，简单来说,accuracy是尽可能的提高正确率，而<code>log loss</code>则是尽可能的降低错误率。</p><p>下面是我们使用的<code>loss function</code>的定义：</p><p>$$logloss=-\frac{1}{N}\sum_{i=1}^{N}(y_ilogs(p_i))+(1-y_i)log(1-p_i)$$</p><ul><li>y：是否分类正确，1=yes,0=no</li><li>p：为1的概率</li></ul><p>复习一下， </p><p>$$\sum$$ </p><p>叫做求和符号，读作<em>segema</em>,它的意思就是连续的加法，比如:</p><p>$$\sum_{k=1}^{n}ak=a_1+a_2+a_3+..a_n$$</p><p>k是下标，它会从k变化到n，当k=1的时候，ak就是a1，当k=2的时候，ak就是a2，最后一项就是k=n,segima的意思呢就是把这些项全都加起来。</p><p>所以这里的segema的意思就是把数据中，每一行的数据都加起来，如果你不了解求和符号，可以参考我的这篇<a href="http://wittyfans.com/math/%E6%95%B0%E5%AD%A6%E5%A4%8D%E4%B9%A0%EF%BC%9A%E6%B1%82%E5%92%8C%E7%AC%A6%E5%8F%B7.html">文章</a></p><p>把一行的数据乘以 $$-\frac{1}{N}$$，得到这一行的logloss.</p><h2 id="log-loss函数示例"><a href="#log-loss函数示例" class="headerlink" title="log loss函数示例"></a>log loss函数示例</h2><h3 id="假设A"><a href="#假设A" class="headerlink" title="假设A"></a>假设A</h3><ul><li>true label=0</li><li>预测1的概率是p=0.9：</li></ul><p>带入公式计算可得：</p><p>$$log loss=(1-y)\log^{1-p}$$</p><p>$$=\log^{1-0.9}$$</p><p>$$=\log^{0.1}$$</p><p>$$=2.3$$</p><h3 id="假设B"><a href="#假设B" class="headerlink" title="假设B"></a>假设B</h3><ul><li>true label=1</li><li>预测0的概率是0.5</li></ul><p>则los函数表示为：</p><p>$$logloss=-\frac{1}{1}1\log^{0.5}+(1-1)\log^{1-0.5}$$</p><p>带入公式计算可得：</p><p>$$=-\log^{0.5}$$</p><p>$$=0.69$$</p><h3 id="log-loss-python实现"><a href="#log-loss-python实现" class="headerlink" title="log loss python实现"></a>log loss python实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def compute_log_loss(predicted, actual, eps=1e-14):</span><br><span class="line"> &quot;&quot;&quot; Computes the logarithmic loss between predicted and</span><br><span class="line"> actual when these are 1D arrays.</span><br><span class="line"></span><br><span class="line"> :param predicted: The predicted probabilities as floats between 0-1</span><br><span class="line"> :param actual: The actual binary labels. Either 0 or 1.</span><br><span class="line"> :param eps (optional): log(0) is inf, so we need to offset our</span><br><span class="line"> predicted values slightly by eps from 0 or 1.</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line"> predicted = np.clip(predicted, eps, 1 - eps)</span><br><span class="line"> loss = -1 * np.mean(actual * np.log(predicted)</span><br><span class="line"> + (1 - actual)</span><br><span class="line"> * np.log(1 - predicted))</span><br><span class="line"></span><br><span class="line"> return loss</span><br></pre></td></tr></table></figure><p>我们可以用这个函数取计算一些提供好的值的log loss值，下面是算好后的结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Log loss, correct and confident: 0.05129329438755058</span><br><span class="line">Log loss, correct and not confident: 0.4307829160924542</span><br><span class="line">Log loss, wrong and not confident: 1.049822124498678</span><br><span class="line">Log loss, wrong and confident: 2.9957322735539904</span><br><span class="line">Log loss, actual labels: 9.99200722162646e-15</span><br></pre></td></tr></table></figure><p>可以看到，模型的预测越准确，则logloss值越低，真实值的logloss是极低的。</p><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><p>我们建模往往都是一开始建立简单的模型，然后再慢慢的优化。</p><p>所以快速的建立模型，很有必要，我们从 <em>mutil-class logistic regression</em> 开始，简单的模型我们就只选择数字类型的了，其他的列都不要，然后将列分开建模，然后把整个数据按行拿进来预测，观察在这一列是否有出现。</p><p>但还有一个问题，在之前的课程中，我们将数据分割成train组与test组，但这个方案在这里是不行的，而且那中情况只适合单个 target 的情况。</p><p>这里我们会使用另外一个函数来分割数据，叫做 <em>mutil_label_train_test_split</em></p><p>随后我们找出所有数字类型的features作为trains set, 把我们感兴趣的 lebels 也拿出来（以get_dummies的形式），就可以利用这两组数据生成train与test数据了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create the new DataFrame: numeric_data_only</span><br><span class="line">numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)</span><br><span class="line"></span><br><span class="line"># Get labels and convert to dummy variables: label_dummies</span><br><span class="line">label_dummies = pd.get_dummies(df[LABELS])</span><br><span class="line"></span><br><span class="line"># Create training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2,seed=123)</span><br><span class="line"></span><br><span class="line"># Print the info</span><br><span class="line">print(&quot;X_train info:&quot;)</span><br><span class="line">print(X_train.info())</span><br><span class="line">print(&quot;\nX_test info:&quot;)  </span><br><span class="line">print(X_test.info())</span><br><span class="line">print(&quot;\ny_train info:&quot;)  </span><br><span class="line">print(y_train.info())</span><br><span class="line">print(&quot;\ny_test info:&quot;)  </span><br><span class="line">print(y_test.info())</span><br></pre></td></tr></table></figure><p>有了traning数据之后，就可以训练模型并计算我们的模型得分了，这里为了将我们的列分开计算，我们需要使用 <em>‌OneVsRestClassifier</em> 包，用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Import classifiers</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.multiclass import OneVsRestClassifier</span><br><span class="line"></span><br><span class="line"># Create the DataFrame: numeric_data_only</span><br><span class="line">numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)</span><br><span class="line"></span><br><span class="line"># Get labels and convert to dummy variables: label_dummies</span><br><span class="line">label_dummies = pd.get_dummies(df[LABELS])</span><br><span class="line"></span><br><span class="line"># Create training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2, seed=123)</span><br><span class="line"></span><br><span class="line"># Instantiate the classifier: clf</span><br><span class="line">clf = OneVsRestClassifier(LogisticRegression())</span><br><span class="line"></span><br><span class="line"># Fit the classifier to the training data</span><br><span class="line">clf.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># Print the accuracy</span><br><span class="line">print(&quot;Accuracy: &#123;&#125;&quot;.format(clf.score(X_test,y_test)))</span><br></pre></td></tr></table></figure><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>这一次我们利用真实的数据来预测，这些数据是模型从未见过的，我们通过pd.read_csv导入它</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Instantiate the classifier: clf</span><br><span class="line">clf = OneVsRestClassifier(LogisticRegression())</span><br><span class="line"></span><br><span class="line"># Fit it to the training data</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Load the holdout data: holdout</span><br><span class="line">holdout = pd.read_csv(&quot;HoldoutData.csv&quot;,index_col=0)</span><br><span class="line"></span><br><span class="line"># Generate predictions: predictions</span><br><span class="line">predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))</span><br></pre></td></tr></table></figure><h2 id="自然语言处理简要概述（NLP）"><a href="#自然语言处理简要概述（NLP）" class="headerlink" title="自然语言处理简要概述（NLP）"></a>自然语言处理简要概述（NLP）</h2><h3 id="Tokenizing-与-gram"><a href="#Tokenizing-与-gram" class="headerlink" title="Tokenizing 与 gram"></a>Tokenizing 与 gram</h3><p>Tokenizing即将文本变成词语，简单的直接按照空格或者是标点符号分割，复杂一点会有自此识别，如结巴分词中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我来到北京清华大学</span><br></pre></td></tr></table></figure><p>变成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</span><br></pre></td></tr></table></figure><p>在上面的例子中，gram的选择会有不一样的效果,如果gram=1，则结果变成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我/ 来/到/ 北/京/ 清/华/ 大/学</span><br></pre></td></tr></table></figure><p>如果gram=2,则：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我来/到北/京清/华大/学</span><br></pre></td></tr></table></figure></p><h2 id="文本到数字"><a href="#文本到数字" class="headerlink" title="文本到数字"></a>文本到数字</h2><p>上面的语句变成词语组，这些词语就叫做 <code>bag of words</code></p><p>在 sklearn中，有一个方法可以计算bag of words，<em>CountVectorizer</em>。</p><p>使用之前，你需要给CountVectorizer传入一个正则表达式，它才知道如何去分割字词，对了，不要忘记处理你数据中的missing value.</p><p>当你用正则创建好CountVectorizer后，就可以把语句传进来计算bag of words,一样它也是使用fit方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line"># Create the token pattern: TOKENS_ALPHANUMERIC</span><br><span class="line">TOKENS_ALPHANUMERIC = &apos;[A-Za-z0-9]+(?=\\s+)&apos;</span><br><span class="line"></span><br><span class="line"># Fill missing values in df.Position_Extra</span><br><span class="line">df.Position_Extra.fillna(&apos;&apos;,inplace=True)</span><br><span class="line"></span><br><span class="line"># Instantiate the CountVectorizer: vec_alphanumeric</span><br><span class="line">vec_alphanumeric = CountVectorizer(token_pattern=&apos;[A-Za-z0-9]+(?=\s+)&apos;)</span><br><span class="line"></span><br><span class="line"># Fit to the data</span><br><span class="line">vec_alphanumeric.fit(df.Position_Extra)</span><br><span class="line"></span><br><span class="line"># Print the number of tokens and first 15 tokens</span><br><span class="line">msg = &quot;There are &#123;&#125; tokens in Position_Extra if we split on non-alpha numeric&quot;</span><br><span class="line">print(msg.format(len(vec_alphanumeric.get_feature_names())))</span><br><span class="line">print(vec_alphanumeric.get_feature_names()[:15])</span><br></pre></td></tr></table></figure><h1 id="改善您的模型"><a href="#改善您的模型" class="headerlink" title="改善您的模型"></a>改善您的模型</h1><h2 id="Pipelines-feature-amp-text-preprocessing"><a href="#Pipelines-feature-amp-text-preprocessing" class="headerlink" title="Pipelines, feature &amp; text preprocessing"></a>Pipelines, feature &amp; text preprocessing</h2><p>我们已经接触过pipline了，它可以让我们处理数据的步骤变得更容易。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Import Pipeline</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"># Import other necessary modules</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.multiclass import OneVsRestClassifier</span><br><span class="line"></span><br><span class="line"># Split and select numeric data only, no nans</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(sample_df[[&apos;numeric&apos;]],pd.get_dummies(sample_df[&apos;label&apos;]),random_state=22)</span><br><span class="line"></span><br><span class="line"># Instantiate Pipeline object: pl</span><br><span class="line">pl = Pipeline([</span><br><span class="line">    (&apos;clf&apos;, OneVsRestClassifier(LogisticRegression()))</span><br><span class="line">])</span><br><span class="line"># Fit the pipeline to the training data</span><br><span class="line">pl.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Compute and print accuracyaccuracy = pl.score(X_test, y_test)</span><br><span class="line">print(&quot;\nAccuracy on sample data - numeric, no nans: &quot;, accuracy)</span><br></pre></td></tr></table></figure><p>对于pipline，它是按照顺序来执行里面的步骤的，所以您需要规划好顺序，例如对于你的数据，如果有缺失值，你必须在训练模型之前就将这些na值处理好:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pl = Pipeline([</span><br><span class="line">        (&apos;imp&apos;, Imputer()),</span><br><span class="line">        (&apos;clf&apos;, OneVsRestClassifier(LogisticRegression()))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><p>这里使用了inputer来处理缺失值。</p><p>pipline对象的使用一样也是调用fit方法，并且pipline对象还提供了打分的功能(默认是accuracy)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Fit the pipeline to the training data</span><br><span class="line">pl.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># Compute and print accuracy</span><br><span class="line">accuracy = pl.score(X_test,y_test)</span><br></pre></td></tr></table></figure><h2 id="Text-features-and-feature-unions"><a href="#Text-features-and-feature-unions" class="headerlink" title="Text features and feature unions"></a>Text features and feature unions</h2><p>对于我们的text值，不可以把它和数值型的数据一起处理，即不可以放在同一个pipline中。</p><p>怎么办呢？解决方案是使用Function Transformer()和FeatureUnion()</p><p>Function Transformer()做的事情很简单，就是接受一个python函数，把它转化成sklearn可以理解的对象，然后用它去处理数据。</p><p>我们可以写两个函数，都接受所有的dataframe，但是一个输出text的处理结果，另一个输出数值型数据的结果。</p><p>这样我们就可以对数值型数据与text型数据分别设置两套pipline。</p><p>在 Function Transformer() 参数重，我们将参数 validate设置为false，以让其不检查空值。</p><p>FeatureUnion是另一个我们需要用到的包，当我们用function transformer的时候，一个是数值型数据，另一个是文本型数据，FeatureUnion可以把这两个features联合到一起作为同一个数组，作为classifier的输入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># Import FeatureUnion</span><br><span class="line">from sklearn.pipeline import FeatureUnion</span><br><span class="line"></span><br><span class="line"># Split using ALL data in sample_df</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(sample_df[[&apos;numeric&apos;, &apos;with_missing&apos;, &apos;text&apos;]],pd.get_dummies(sample_df[&apos;label&apos;]), random_state=22)</span><br><span class="line"></span><br><span class="line"># Create a FeatureUnion with nested pipeline: process_and_join_features</span><br><span class="line">process_and_join_features = FeatureUnion(</span><br><span class="line">            transformer_list = [</span><br><span class="line">                (&apos;numeric_features&apos;, Pipeline([</span><br><span class="line">                    (&apos;selector&apos;, get_numeric_data),</span><br><span class="line">                    (&apos;imputer&apos;, Imputer())</span><br><span class="line">                ])),</span><br><span class="line">                (&apos;text_features&apos;, Pipeline([</span><br><span class="line">                    (&apos;selector&apos;, get_text_data),</span><br><span class="line">                    (&apos;vectorizer&apos;, CountVectorizer())</span><br><span class="line">                ]))</span><br><span class="line">             ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Instantiate nested pipeline: pl</span><br><span class="line">pl = Pipeline([</span><br><span class="line">        (&apos;union&apos;, process_and_join_features),</span><br><span class="line">        (&apos;clf&apos;, OneVsRestClassifier(LogisticRegression()))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Fit pl to the training data</span><br><span class="line">pl.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Compute and print accuracy</span><br><span class="line">accuracy = pl.score(X_test, y_test)</span><br><span class="line">print(&quot;\nAccuracy on sample data - all data: &quot;, accuracy)</span><br></pre></td></tr></table></figure><h2 id="回归学校数据"><a href="#回归学校数据" class="headerlink" title="回归学校数据"></a>回归学校数据</h2><p>对于之前的问题，我们的数据中只有一列是文本型数据，而学校数据中，则有14列。</p><p>我们需要将这些列都结合起来，这个函数已经写好了，叫做 <code>combine_text_columns</code>, 定义好之后，只需要在定义Function Transformer()的时候更改一下参数就好了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Import FunctionTransformer</span><br><span class="line">from sklearn.preprocessing import FunctionTransformer</span><br><span class="line"></span><br><span class="line"># Get the dummy encoding of the labels</span><br><span class="line">dummy_labels = pd.get_dummies(df[LABELS])</span><br><span class="line"></span><br><span class="line"># Get the columns that are features in the original df</span><br><span class="line">NON_LABELS = [c for c in df.columns if c not in LABELS]</span><br><span class="line"></span><br><span class="line"># Split into training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],dummy_labels,0.2, seed=123)</span><br><span class="line"></span><br><span class="line"># Preprocess the text data: get_text_data</span><br><span class="line">get_text_data = FunctionTransformer(combine_text_columns,validate=False)</span><br><span class="line"></span><br><span class="line"># Preprocess the numeric data: get_numeric_data</span><br><span class="line">get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)</span><br></pre></td></tr></table></figure><p>定义好处理文本和数字的函数之后，我们就可以建立模型了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Complete the pipeline: pl</span><br><span class="line">pl = Pipeline([</span><br><span class="line">        (&apos;union&apos;, FeatureUnion(</span><br><span class="line">            transformer_list = [</span><br><span class="line">                (&apos;numeric_features&apos;, Pipeline([</span><br><span class="line">                    (&apos;selector&apos;, get_numeric_data),</span><br><span class="line">                    (&apos;imputer&apos;, Imputer())</span><br><span class="line">                ])),</span><br><span class="line">                (&apos;text_features&apos;, Pipeline([</span><br><span class="line">                    (&apos;selector&apos;, get_text_data),</span><br><span class="line">                    (&apos;vectorizer&apos;, CountVectorizer())</span><br><span class="line">                ]))</span><br><span class="line">             ]</span><br><span class="line">        )),</span><br><span class="line">        (&apos;clf&apos;, OneVsRestClassifier(LogisticRegression()))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"># Fit to the training data</span><br><span class="line">pl.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># Compute and print accuracy</span><br><span class="line">accuracy = pl.score(X_test, y_test)</span><br><span class="line">print(&quot;\nAccuracy on budget dataset: &quot;, accuracy)</span><br></pre></td></tr></table></figure><p>可以看到pipline输出的分数。这个处理的步骤是不是很熟悉，这个步骤是通用的，而且如果你想要更换别的分类器，只需要将 <code>LogisticRegression</code> 改成别的就好了，例如在这个例子里，你把模型改成<em>RandomForestClassifier</em>，将会有0.2的提升，如果把<em>RandomForestClassifier</em>的参数<em>n_estimators</em>改成15，<em>accuracy</em> 还有有所增加。</p><h1 id="专家指点"><a href="#专家指点" class="headerlink" title="专家指点"></a>专家指点</h1><ul><li>tokenize text，不仅仅只是根据空格与标点来处理文字</li><li>n-gram statistics，文字的选择我们定义gram，我们也可以同时定义多个gram，如,<code>CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,ngram_range=(1,2)))</code></li><li>使用预知的alpha-numeric sequences,对于text，只接受字母数字序列</li></ul><h2 id="统计技巧-interaction-terms"><a href="#统计技巧-interaction-terms" class="headerlink" title="统计技巧,interaction terms"></a>统计技巧,interaction terms</h2><p>来看一组例子,这两组数据中，English teacher和2nd grade 都有出现</p><ul><li><em>English teacher for 2nd grade</em></li><li><em>2nd grade - budget for English teacher</em></li></ul><p>$$\beta_1x_1+\beta_2x_2+\beta_3(x_1x_2)$$</p><table><thead><tr><th style="text-align:left">x_1</th><th style="text-align:left">x_2</th><th style="text-align:left">x3</th></tr></thead><tbody><tr><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">x1*x2=0*1=0</td></tr><tr><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">x1*x2=1*1=1</td></tr></tbody></table><ul><li>x1,x2代表某个特定的token有出现</li><li>beta符号则代表其重要程度或者说相关系数</li><li>x3是x1、x2两者的乘积</li></ul><p>当然，sklearn提供了一种非常直接的方式使用interaction terms，即：<em>PolynomialFeatures</em>,</p><p>另外还有一个叫做SparseInteractions的包，也可以做同样的事情。</p><p>最后，我们不可能因为选一个不一样的模型就改善所有的情况，模型的优化是一个渐进的步骤，可能是你将模型的某个参数调整一下，可能是你在语义处理方面更换了一个更好的工具，这些东西都需要你去根据你的实际情况作出调整。</p><p>这篇文章就到这里结束了，下一篇将会是非监督学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/18/SbPAWvxuiRELnO6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们学校比旁边的学校在教科书上面花了更多的钱吗？这是否有用呢？&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="data science" scheme="http://wittyfans.com/tags/data-science/"/>
    
  </entry>
  
  <entry>
    <title>scikit-learn与监督学习</title>
    <link href="http://wittyfans.com/coding/scikit-learn%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.html"/>
    <id>http://wittyfans.com/coding/scikit-learn与监督学习.html</id>
    <published>2019-07-26T09:38:14.000Z</published>
    <updated>2019-11-15T14:18:26.118Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/15/AFHezumhwD5grWB.png" alt=""></p><blockquote><p>给机器学习的能力，让它可以根据<strong>数据</strong>自己做决定的一种技术。</p></blockquote><a id="more"></a><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h2><p>什么是机器学习？它是给机器学习的能力，让它可以根据<strong>数据</strong>自己做决定的一种技术。</p><p>如，电子邮件是否是垃圾邮件，根据数据自动分类。</p><p>如果您使用的<strong>数据</strong>，是labled的，这就叫监督学习，如果是不unlabeled，则是非监督学习。</p><p>非监督学习的例子，如企业根据用户的数据，对用户进行分类(clustering).</p><blockquote><p>clustering，非监督学习的一个分支。</p></blockquote><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><p>这是一种对系统进行奖赏与惩罚的训练技术，让机器可以根据环境的变化作出反应,典型的应用如阿法狗。</p><p>我们将首先focus在监督学习这部分。</p><p>我们将根据数据来训练，这些数据叫做 <code>prodictor varibles</code> 或者 <code>features</code>,<code>indenpendent varibles</code>，训练的目的是要做出预测，要预测的值叫做 <code>target varibles</code>,<code>denpendent varibles</code>或<code>response varible</code>, 如果预测的值是一组分类信息，如花的品种，人的性格，这种预测叫做<code>classfication</code>,如果是一些连续的值，如股票的价格，那就叫做<code>regression</code>.</p><p>现在我们先学习 <code>classfication</code>.</p><p>对于监督学习，首先需要数据，那么数据从哪里来？</p><ul><li>历史的数据，已经做好了分类</li><li>通过自己的试验获取，如A/B Test</li></ul><p>机器学习的工具有很多，我们主要是用 <code>scikit-learn</code>或者说<code>sklearn</code>.</p><p>其他的工具包括:</p><ul><li>TensorFlow</li><li>keras</li></ul><h1 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h1><h2 id="示例数据"><a href="#示例数据" class="headerlink" title="示例数据"></a>示例数据</h2><p>主要是用花瓣的数据，其中包括:</p><ul><li>petal lengh, 花瓣长度</li><li>petal width，花瓣宽度</li><li>sepal length，萼片 <code>èpiàn</code> 长度</li><li>sepal width，萼片 <code>èpiàn</code> 宽度</li></ul><blockquote><p>萼片，即花瓣下面，包住花的那部分。</p></blockquote><p>关于花，有不同的品种，在英文中叫做<code>species</code>，我们的数据中有三种，Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），以及Iris Virginica（维吉尼亚鸢尾）。</p><p>数据怎么导入呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">iris = datases.load_iris()</span><br></pre></td></tr></table></figure><p>这是bunch类型的数据，跟python内置的dict数据差不多。</p><blockquote><p>对于数据，记住,sample in rows, features are columns.</p></blockquote><h2 id="实例数据"><a href="#实例数据" class="headerlink" title="实例数据"></a>实例数据</h2><p>美国众议员议员数据,US House of Representatives Congressmen.</p><p>我们去预测它们的政党隶属关系(party affiliation),即：</p><ul><li>‘Democrat’,民主党</li><li>‘Republican’，共和党</li></ul><p>根据什么来预测呢？根据它们对特定问题的投票。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[&apos;party&apos;, &apos;infants&apos;, &apos;water&apos;, &apos;budget&apos;,</span><br><span class="line">&apos;physician&apos;, &apos;salvador&apos;,&apos;religious&apos;,</span><br><span class="line">&apos;satellite&apos;, &apos;aid&apos;, &apos;missile&apos;, &apos;immigration&apos;,</span><br><span class="line">&apos;synfuels&apos;,&apos;education&apos;, &apos;superfund&apos;,</span><br><span class="line">&apos;crime&apos;, &apos;duty_free_exports&apos;, &apos;eaa_rsa&apos;]</span><br></pre></td></tr></table></figure><p>上面是数据集的列名，即一些议题的投票情况。</p><p>我们使用countplot图来探索这些数据，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">sns.countplot(x=&apos;education&apos;, hue=&apos;party&apos;, data=df, palette=&apos;RdBu&apos;)</span><br><span class="line">plt.xticks([0,1], [&apos;No&apos;, &apos;Yes&apos;])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h1><p>我们手头有labled的数据，但我们还需要对那些没有标记的数据进行标记，我们需要对标记过数据学习，然后建立一个标记器。</p><p>标记过的数据叫做traning data.</p><h3 id="标记数据-KNN算法"><a href="#标记数据-KNN算法" class="headerlink" title="标记数据-KNN算法"></a>标记数据-KNN算法</h3><p>我们用knn算法来实现这个步骤，knn是根据数据的邻居来预测的一种算法，比如放眼望全世界，如果您的位置在中国，那么我可以预测您有很大的几率是中国人，因为您和很多的中国人在一起，对于knn算法，我们首先需要根据knn算法建立模型并训练，训练模型我们叫fiting modal，在sklearn中，我们使用<code>fit()</code>方法，而<code>predit()</code>用来预测。</p><p>如:<code>knn.fit(iris[&#39;data],iris[&#39;target&#39;])</code></p><p>使用knn fit方法的条件:</p><ol><li>np或者pd类型的数据</li><li>features是连续型的数据，不是分类数据</li><li>没有缺失值</li></ol><p>fit后，knn会返回fit后的分类器（classifier）本身，然后就可以拿来使用了。</p><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Import KNeighborsClassifier from sklearn.neighbors</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"># Create arrays for the features and the response variable</span><br><span class="line">y = df[&apos;party&apos;].values</span><br><span class="line">X = df.drop(&apos;party&apos;, axis=1).values</span><br><span class="line"></span><br><span class="line"># Create a k-NN classifier with 6 neighbors</span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=6)</span><br><span class="line"></span><br><span class="line"># Fit the classifier to the data</span><br><span class="line">knn.fit(X,y)</span><br></pre></td></tr></table></figure><p>neighbors设置为6，这是这个示例默认的值，您可以设置成其他的值，在这个示例中，6是最合适的。</p><h2 id="测试模型性能-Accuracy"><a href="#测试模型性能-Accuracy" class="headerlink" title="测试模型性能-Accuracy"></a>测试模型性能-Accuracy</h2><p>accuracy是测试一个模型的指标，计算方法是模型的<code>正确预测数/测试数据总数</code>，那么我们用什么数据来算accuracy呢？</p><p>我们肯定需要用从来没有给模型用过的数据来计算，也不能用那些训练过模型的数据。</p><p>所以，一般的做法是，将数据分成两组：</p><ol><li>traning set,用来训练模型</li><li>test set，用来预测，测试模型性能</li></ol><p>我们用<code>train_test_split</code> 方法来实现，这个方法的参数中，</p><ul><li>x，y就不多介绍</li><li>test_size为您要分隔的比例，一般来说traing占0.7，test占0.3</li><li><p>stratify设置为y（y包含lable），这个参数可以让您的lable（分类数据）比较均匀的分布在分隔后的数据中，不至于过于集中</p><p>  训练完成之后，可以预测，然后就可以使用<code>knn.score(x_test,y_testx)</code>查看模型的分数。</p></li></ul><p>另外对于n_neighbors，设置的值越大，则数据集之间的分界线（decision boundary）越不敏感，曲线变得更平滑，更少的波动，但太大会导致overfitting,而太小会导致underfiting。</p><h1 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h1><p>regression问题是连续型的值的预测，如gdp, 房价。</p><p>我们对波士顿房价的数据进行预测，x值为一个街区中的房子房间的平均数量，可以观察到，房间数越多，价格越高。</p><p>regression的预测使用的是 <code>linear_model</code>模块。</p><h2 id="线性回归基础"><a href="#线性回归基础" class="headerlink" title="线性回归基础"></a>线性回归基础</h2><p>线性回归的基础就是<code>y=a*x+b</code>类似的一次函数,y是我们需要预测的值，x是我们的features，我们做的就是去寻找最合适的a与b，确定最合适的那个函数。</p><p>这么一个函数，我们叫做loss或者cost函数，我们为这个函数找到最合适的a与b值，让它确定的这个线，跟所有图中的点的的 <strong>垂直</strong> 距离最近，这里所说的距离就叫做 <code>residual</code>。</p><p><img src="https://i.loli.net/2019/11/15/9sEMrji8LR5AXUt.png" alt="Residual"></p><p>我们可以尝试减少所有 <code>risidual</code> 相加的和，但是正负会抵消，因为点距离线的距离有正负，那么我们减少这个距离的平方的和会比较好，即将所有的点与线段的距离平方后，再相加。</p><p>使用这个函数，就叫做最小二乘法（ordinary least squares）或者说OLS.</p><p>当我们训练的模型只有一个feature的时候，我们会使用 <code>y=a*x+b</code>,当feature增加到两个，则使用<code>y=a1*x1+a2*x2+b</code>,以此类推。</p><p>所以如果有两个feature的情况，我们的任务就是确定函数中三个数的值，<code>a1,a2,b</code>.</p><h2 id="实例讲解"><a href="#实例讲解" class="headerlink" title="实例讲解"></a>实例讲解</h2><p>我们现在根据人的生育率来预测寿命，那么我们的x值就是生育率，我们会把它作为参数传给回归器，我们的生育率是有一个范围的，不可能为负数，也不可能是好几百，所以我们从收集到的真实数据中找到最小的和最大的作为生育率的范围。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 导入相关的包</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line"></span><br><span class="line"># 创建回归器</span><br><span class="line">reg = LinearRegression()</span><br><span class="line"></span><br><span class="line"># 确定生育率的范围，并reshape为回归器需要的shape（这里的数据是给预测器预测用的）</span><br><span class="line">prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)</span><br><span class="line"></span><br><span class="line"># 把收集到的数据x与y传给回归器，让它学会怎么回归</span><br><span class="line">reg.fit(X_fertility,y)</span><br><span class="line"></span><br><span class="line"># 学会回归之后，把我们确定好的生育率范围数据传给预测器</span><br><span class="line">y_pred = reg.predict(prediction_space)</span><br><span class="line"></span><br><span class="line"># 打印 R^2 ，评分</span><br><span class="line">print(reg.score(X_fertility, y))</span><br><span class="line"></span><br><span class="line"># 绘图</span><br><span class="line">plt.plot(prediction_space, y_pred, color=&apos;black&apos;, linewidth=3)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="分割数据"><a href="#分割数据" class="headerlink" title="分割数据"></a>分割数据</h2><p>我们提到过分割数据的概念，主要是使用<code>train_test_split</code>包方法。<br>分隔好数据后我们即可进行预测，预测后的数据使用回归器的score方法查看预测效果，另外这里还介绍一个对预测效果的打分工具，即<code>mean_squared_error</code>.</p><p>您可以通过 <code>from sklearn.metrics import mean_squared_error</code> 引入并使用它。</p><p>它又叫做 <em>Root Mean Squared Error (RMSE)</em>.</p><p>它使用的参数是预测的数据与真实的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># y_test已c好</span><br><span class="line"></span><br><span class="line">y_pred = reg.predict(X_test)</span><br><span class="line">rmse = np.sqrt(mean_squared_error(y_test,y_pred))</span><br></pre></td></tr></table></figure><h2 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross-validation"></a>Cross-validation</h2><p>RMSE的分数和您分割数据的方式是有关系的，我们需要一种科学的分割方式。<br>在test的数据中，可能会存在一种数据层面的 <em>peculiarities</em> 导致它不能代表模型对新数据的预测能力。<br>解决这个问题的技术就叫做 <em>cross-validation</em>,它将所有数据分组，并分别作为测试与训练组，这样便可以最大化训练的数据，同时也将所有的对所有的数据进行了预测。</p><p>先简单介绍下 <em>cross-validation</em> 的步骤：</p><ol><li>将数据分隔成5（或者别的）组，这里可以说组 group,也有人说fold</li><li>将第一组作为test组，剩下的4组作为training组，training完成后，预测test组的数据，计算出结果作为标准（Metric1）</li><li>将第二组作为test，重复直到最后一组(metric1,2,3,4,5)</li><li>对5组metric进行统计分析，如mean，median，置信区间等等</li></ol><blockquote><p>分割成5组，即5folds = 5-fold CV, 10 folds = 10 folds CV,组数越多，所需要的计算量就大。</p></blockquote><h2 id="Regularized-regression"><a href="#Regularized-regression" class="headerlink" title="Regularized regression"></a>Regularized regression</h2><p>回忆一下，我们对于线性回归的定义，我们找到与所有点的垂直距离最短的线,即确定这个一次函数的系数（coefficient），但如果我们让这些系数或者说参数很大，我们就会过度拟合（overfitting），所以我们需要控制这个系数（coefficient），如果coefficient变得很大，它就是不听话，那我们就对函数做出惩罚，我们知道regulation是合规的意思，这里的也就是说对regression的参数作出规范。</p><p>接下来我们会介绍一些常见的regulation。</p><h3 id="Ridge-regression-岭回归"><a href="#Ridge-regression-岭回归" class="headerlink" title="Ridge regression 岭回归"></a>Ridge regression 岭回归</h3><p>ridge regression就是一个加强版的OLS loss function，它多了一个<code>阿尔法</code>参数，当阿尔法为0的时候，ridge regression变成了普通的ols函数，当阿尔法变得很大，对overfitting的惩罚也变得更大，更敏感，这会让模型变得过于简单(underfitting).</p><p>对于ridge regression的使用，它多了个alpha参数。另外对于不同参数，我们需要将它 <em>归一化</em>，所以在初始化ridge实例的时候，需要指定 <code>normalize=true</code>.</p><h3 id="Lasso-regression-套索回归"><a href="#Lasso-regression-套索回归" class="headerlink" title="Lasso regression 套索回归"></a>Lasso regression 套索回归</h3><p>Lasson回归的特点是，它可以自己选择feature，这样就会把那些不重要features的系数降为或者说（shrink）为0.</p><p>当我们训练好模型后，我们可以通过 <code>.coef_</code> 来查看模型中各个features的权重，或者说偏好。</p><p>下面是关于 coef_ 的解释。：</p><blockquote><p><em>The coef_ contain the coefficients for the prediction of each of the targets. It is also the same as if you trained a model to predict each of the targets separately.</em></p></blockquote><p>我们可以将columns与conef_绘图，这就可以看到哪些featues对模型的影响是最大的，这在许多的bussniss与科学实验中都有很多应用。</p><p><strong>附：关于Lasso的Python实践</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Import Lasso</span><br><span class="line">from sklearn.linear_model import Lasso</span><br><span class="line"></span><br><span class="line"># Instantiate a lasso regressor: lasso</span><br><span class="line">lasso = Lasso(alpha=0.4,normalize=True)</span><br><span class="line"></span><br><span class="line"># Fit the regressor to the data</span><br><span class="line">lasso.fit(X,y)</span><br><span class="line"></span><br><span class="line"># Compute and print the coefficients</span><br><span class="line">lasso_coef = lasso.coef_</span><br><span class="line">print(lasso_coef)</span><br><span class="line"></span><br><span class="line"># Plot the coefficients</span><br><span class="line">plt.plot(range(len(df_columns)), lasso_coef)</span><br><span class="line">plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)</span><br><span class="line">plt.margins(0.02)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Lasso非常适合选择features，但是在构建回归模型时，Ridge回归应该是您的首选。</p><p>关于阿尔法参数对ridge的影响，可以多研究研究<a href="https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/regression-2?ex=13" target="_blank" rel="noopener">这个</a>章节。</p><h1 id="调整模型"><a href="#调整模型" class="headerlink" title="调整模型"></a>调整模型</h1><p>训练好模型之后，并不能完美的预测数据，就如打磨任何一件产品，粗加工后还需要细腻的打磨。这一节将会引入一些其他的metric来衡量您建立的模型，同时也会介绍优化模型的技术，即hyperparameter tuning.</p><p>在分类的问题中，accuracy是我们评价模型的标尺，但accuracy不总是唯一衡量模型的标准。</p><p>比如在垃圾邮件的检测问题中，如果我们把所有的邮件都作为正常邮件，因为我们只有1%的垃圾邮件。我们的模型准确度高达99%！听起来还不错，但这完全是个糟糕的模型</p><p>先记住，这种正常的数据远远多于异常数据的情况，叫做 <code>class imbalance</code>.</p><h2 id="评价模型的标尺"><a href="#评价模型的标尺" class="headerlink" title="评价模型的标尺"></a>评价模型的标尺</h2><p>给定一个两个结果的分类器，如垃圾邮件问题：</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">预测到的垃圾邮件</th><th style="text-align:left">预测到的真实邮件</th></tr></thead><tbody><tr><td style="text-align:left">实际为垃圾邮件</td><td style="text-align:left"><strong>True Positive</strong></td><td style="text-align:left">False Negative</td></tr><tr><td style="text-align:left">实际为真实邮件</td><td style="text-align:left">False Positive</td><td style="text-align:left"><strong>True Negative</strong></td></tr></tbody></table><p>上面的这个表，叫做confusion matrix</p><ul><li>左上和右下的为预测正确的情况</li><li>通常，感兴趣的组为positive，想要预测垃圾邮件，那么垃圾邮件即为positive的</li></ul><p>为什么需要confusion matrix?</p><ol><li>计算Accuracy,即：对角线（diagonal）格子数/格子总数, <code>tp+tn/(tp+tn+fp+fn)</code></li><li>计算其他的模型指标，如：<ol><li><strong>Precision</strong>，又叫positive predictive,PPV: <code>tp/(tp+fp)</code>,在垃圾邮件模型中为，<code>实际垃圾邮件数量/预测垃圾邮件数量</code>；这个指标值越高，意味着我们更少的真实邮件预测为垃圾邮件，提高precision则我们的预测更准确，它又叫做准确率，查准率</li><li><strong>Recall</strong>，<code>tp/(tp+fn)</code>,预测正确的垃圾邮件占所有实际垃圾邮件之比，提高recall即意味着尽可能多的预测所有垃圾邮件，所以它也叫做查全率或叫召回率</li><li><strong>F1score</strong>,<code>2*(precision*recall/precision+recall)</code>，Precision与recall分别对应查准问题与查全问题，然而常常二者不能同时提高，所以对于实际复杂问题处理很有偏见，于是我们引入F1-score来近似帮助我们解决实际问题。</li></ol></li></ol><p>可以直接使用confusion matrix函数来计算，它接受两个数据，一个是y_test,另一个是y_pred，即预测出来的数据。</p><p>对于 resulting matrix, 也是一样的，在所有的sklearn的预测函数中，第一个参数总是test数据，第二个一般都是预测出来的数据,另外还有一个support参数，它就是test数据中对于不同结果的响应情况，总数就是test数据的长度。</p><h2 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h2><p>不要被这个名字迷惑，logistic regression其实是用来处理分类问题的。</p><p>它返回的是概率，当预测的数据概率大于0.5，我们标记为1，如果小于0.5，则标记为0.</p><p>它有点像线性回归，会在图像上把两组数据区分开来,这个边界就叫做线性决策边界（linear decision boundary）</p><p>在使用logistic regression的时候，我们可以定义这个概率（默认是0.5）.</p><ul><li>如果定为0，那么所有进来的数据全部预测为1</li><li>如果定为1，所有进来的数据全部预测为0</li></ul><h3 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h3><p>根据我们之前定义的混淆矩阵，我们再来了解两个指标,方便我们引入ROC的概念：</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">预测到的垃圾邮件</th><th style="text-align:left">预测到的真实邮件</th></tr></thead><tbody><tr><td style="text-align:left">实际为垃圾邮件</td><td style="text-align:left"><strong>True Positive</strong></td><td style="text-align:left">False Negative</td></tr><tr><td style="text-align:left">实际为真实邮件</td><td style="text-align:left">False Positive</td><td style="text-align:left"><strong>True Negative</strong></td></tr></tbody></table><p>假设我们有如下的图：</p><p><img src="https://i.loli.net/2019/11/14/a9OnfKcCSb4tw1r.png" alt="ROC曲线"></p><ul><li><p>FPR, fpr = fp/(fp+tn),FPR表示，在所有的垃圾邮件中，被预测成正常邮件的比例。它告诉我们，随机拿一个垃圾邮件，有多大概率会将其预测成正常邮件。显然我们会希望FPR越小越好。</p></li><li><p>TPR，fpr=(tp/tp+fn), TPR表示，在所有的正常邮件中，被预测为正常邮件的比例，即随机拿一个正常邮件，有多大的概率会被预测称正常邮件，我们希望这个值越大越好。</p></li></ul><p>x轴是FPR，y轴是TPR，那么点（0，0），（1，1）意味着什么呢？</p><ul><li>（0，0）即FPR=0，TPR=0，也就是FP=0，TP=0，意味着，所有的邮件，我都把它预测为垃圾邮件</li><li>（1，1）意味着，所有邮件，我都预测为正常邮件</li><li>(1,0)，即FPR=1，TPR=0，这是最糟糕的情况。所有的预测都预测错了</li><li>点(0,1)，FPR=0说明FP=0，也就是说没有一个真实邮件被看作垃圾邮件，TPR=1，即所有正常邮件都是标记成正常邮件，这是最好的情况</li></ul><p>我们知道，在二分类（0，1）的模型中，一般我们最后的输出是一个概率值，表示结果是1的概率。那么我们最后怎么决定输入的x是属于0或1呢？我们需要一个阈值，超过这个阈值则归类为1，低于这个阈值就归类为0。所以，不同的阈值会导致分类的结果不同，也就是混淆矩阵不一样了，FPR和TPR也就不一样了。所以当阈值从0开始慢慢移动到1的过程，就会形成很多对(FPR, TPR)的值，将它们画在坐标系上，就是所谓的ROC曲线了。</p><blockquote><p>引用自CSDN博主「Webbley」的<a href="https://blog.csdn.net/liweibin1994/article/details/79462554" target="_blank" rel="noopener">原创文章</a>。</p></blockquote><h3 id="建立Logistic-modal"><a href="#建立Logistic-modal" class="headerlink" title="建立Logistic modal"></a>建立Logistic modal</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Import the necessary modules</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.metrics import confusion_matrix,classification_report</span><br><span class="line"></span><br><span class="line"># Create training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)</span><br><span class="line"></span><br><span class="line"># Create the classifier: logreg</span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"># Fit the classifier to the training data</span><br><span class="line">logreg.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># Predict the labels of the test set: y_pred</span><br><span class="line">y_pred = logreg.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Compute and print the confusion matrix and classification report</span><br><span class="line">print(confusion_matrix(y_test, y_pred))</span><br><span class="line">print(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h3 id="绘制ROC曲线"><a href="#绘制ROC曲线" class="headerlink" title="绘制ROC曲线"></a>绘制ROC曲线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 引入roc的包</span><br><span class="line">from sklearn.metrics import roc_curve</span><br><span class="line"></span><br><span class="line">y_pred_prob = logreg.predict_proba(X_test)[:,1]</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [0, 1], &apos;k--&apos;)</span><br><span class="line">plt.plot(fpr, tpr, label=&apos;Logistic Regression&apos;)</span><br><span class="line">plt.xlabel(&apos;False Positive Rate’)</span><br><span class="line">plt.ylabel(&apos;True Positive Rate&apos;)</span><br><span class="line">plt.title(&apos;Logistic Regression ROC Curve&apos;)</span><br><span class="line">plt.show();</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/15/kQdZoCr5EAwiF8q.png" alt="ROC曲线下面的面积"></p><p>当我们的FPR与TRP位于（0，1）的时候，预测到的结果是最好的，观察这个ROC图像，即ROC曲线下面的面积最大的时候，所以我们可以得出:</p><blockquote><p>ROC曲线下面的面积越大，模型越好！</p></blockquote><p>这个面积就叫做AUC，同样也是一个使用广泛的分类问题评价标尺。</p><h3 id="计算AUC"><a href="#计算AUC" class="headerlink" title="计算AUC"></a>计算AUC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Import necessary modules</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line"># Compute predicted probabilities: y_pred_prob</span><br><span class="line">y_pred_prob = logreg.predict_proba(X_test)[:,1]</span><br><span class="line"></span><br><span class="line"># Compute and print AUC score</span><br><span class="line">print(&quot;AUC: &#123;&#125;&quot;.format(roc_auc_score(y_test, y_pred_prob)))</span><br><span class="line"></span><br><span class="line"># Compute cross-validated AUC scores: cv_auc</span><br><span class="line">cv_auc = cross_val_score(logreg,X,y,cv=5,scoring=&apos;roc_auc&apos;)</span><br><span class="line"></span><br><span class="line"># Print list of AUC scores</span><br><span class="line">print(&quot;AUC scores computed using 5-fold cross-validation: &#123;&#125;&quot;.format(cv_auc))</span><br></pre></td></tr></table></figure><h2 id="Hyperparameter-tuning"><a href="#Hyperparameter-tuning" class="headerlink" title="Hyperparameter tuning"></a>Hyperparameter tuning</h2><ul><li>线性回归：选择参数</li><li>ridge/lasso: 选择alpha</li><li>KNN：选择neighbors</li></ul><p>上面这些选择的参数，就叫做Hyperparameter tuning。</p><p>所以一个好的模型，就在于选择一个好的参数，这个参数如何决定呢？</p><p>我们可以定义一个grid表格，将参数填进去，一个一个的试验，直到找到最好的那个。</p><p><img src="https://i.loli.net/2019/11/15/5VwtUYGKu8gH6ey.png" alt="Grid search cross-validation"></p><h3 id="GridSearchCV"><a href="#GridSearchCV" class="headerlink" title="GridSearchCV"></a>GridSearchCV</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Import necessary modules</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line"># Setup the hyperparameter grid</span><br><span class="line">c_space = np.logspace(-5, 8, 15)</span><br><span class="line">param_grid = &#123;&apos;C&apos;: c_space&#125;</span><br><span class="line"></span><br><span class="line"># Instantiate a logistic regression classifier: logreg</span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"># Instantiate the GridSearchCV object: logreg_cv</span><br><span class="line">logreg_cv = GridSearchCV(logreg, param_grid, cv=5)</span><br><span class="line"></span><br><span class="line"># Fit it to the data</span><br><span class="line">logreg_cv.fit(X,y)</span><br><span class="line"></span><br><span class="line"># Print the tuned parameters and score</span><br><span class="line">print(&quot;Tuned Logistic Regression Parameters: &#123;&#125;&quot;.format(logreg_cv.best_params_))</span><br><span class="line">print(&quot;Best score is &#123;&#125;&quot;.format(logreg_cv.best_score_))</span><br></pre></td></tr></table></figure><h3 id="RandomizedSearchCV"><a href="#RandomizedSearchCV" class="headerlink" title="RandomizedSearchCV"></a>RandomizedSearchCV</h3><p>上面的searchCV是按照一定的规则来测试参数，而randomizedSearchCV则是随机选择数据作为测试的参数，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Import necessary modules</span><br><span class="line">from scipy.stats import randint</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.model_selection import RandomizedSearchCV</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Setup the parameters and distributions to sample from: param_dist</span><br><span class="line">param_dist = &#123;&quot;max_depth&quot;: [3, None],</span><br><span class="line">              &quot;max_features&quot;: randint(1, 9),</span><br><span class="line">              &quot;min_samples_leaf&quot;: randint(1, 9),</span><br><span class="line">              &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># Instantiate a Decision Tree classifier: tree</span><br><span class="line">tree = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"># Instantiate the RandomizedSearchCV object: tree_cv</span><br><span class="line">tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)</span><br><span class="line"></span><br><span class="line"># Fit it to the data</span><br><span class="line">tree_cv.fit(X,y)</span><br><span class="line"></span><br><span class="line"># Print the tuned parameters and score</span><br><span class="line">print(&quot;Tuned Decision Tree Parameters: &#123;&#125;&quot;.format(tree_cv.best_params_))</span><br><span class="line">print(&quot;Best score is &#123;&#125;&quot;.format(tree_cv.best_score_))</span><br></pre></td></tr></table></figure><h1 id="数据预处理与管道"><a href="#数据预处理与管道" class="headerlink" title="数据预处理与管道"></a>数据预处理与管道</h1><p>现实世界的数据是复杂的，我们目前使用的都是标记过的数据，如果是没有标记过的数据，我们就需要将其打上标记，我们可以使用pandas的get_dummies()方法或者是sklearn的OneHotEncoder()，它会完成数据之间的转换，如下所示:</p><p><em>原始数据：</em></p><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">origin</th></tr></thead><tbody><tr><td style="text-align:left">0</td><td style="text-align:left">US</td></tr><tr><td style="text-align:left">1</td><td style="text-align:left">Europe</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">Asia</td></tr></tbody></table><p><em>get_dummies()后：</em></p><table><thead><tr><th style="text-align:left">origin_Asia</th><th style="text-align:left">origin_Europe</th><th style="text-align:left">origin_US</th></tr></thead><tbody><tr><td style="text-align:left">0</td><td style="text-align:left">0</td><td style="text-align:left">1</td></tr><tr><td style="text-align:left">0</td><td style="text-align:left">1</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">1</td><td style="text-align:left">0</td><td style="text-align:left">0</td></tr></tbody></table><p>对于dummies后的数据，如果我们已经有了前两列（origin_Asia,origin_Rurope）的数据，就可以推测剩下的就是US的数据了，所以可以把US这里列删掉。</p><h2 id="处理missing-data"><a href="#处理missing-data" class="headerlink" title="处理missing data"></a>处理missing data</h2><p>您收集到的数据，不会是完美的，肯定有些值是缺失的，它们可能为0，为nan，或者以其他形式记录的空值。</p><p>你可以使用np.dropna来把所有为空值的行删除，但这样很可能会让你的数据总量变少。</p><p>你也可以自动将所有的空值，以该列的平均值替代。</p><p>可以利用imputer对象来处理，如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">imp = impImputer(missing_values=&apos;NaN&apos;,strategy=&apos;mean&apos;,axix=0)</span><br><span class="line"></span><br><span class="line">imp.fit(X)</span><br><span class="line">imp.transform(X)</span><br></pre></td></tr></table></figure><p>用imp fit X后，便可对数据进行transform，因此imputer对象也叫transformer.</p><h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><p>将你的操作打包成一个数组，就变成了一个pipline，你可以把你的操作（如处理missing value，大小写转换等等）都写进管道，一次性处理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Import necessary modules</span><br><span class="line">from sklearn.preprocessing import Imputer</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line"># Setup the pipeline steps: steps</span><br><span class="line">steps = [(&apos;imputation&apos;, Imputer(missing_values=&apos;NaN&apos;, strategy=&apos;most_frequent&apos;, axis=0)),</span><br><span class="line">        (&apos;SVM&apos;, SVC())]</span><br><span class="line"></span><br><span class="line"># Create the pipeline: pipeline</span><br><span class="line">pipeline = Pipeline(steps)</span><br><span class="line"></span><br><span class="line"># Create training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)</span><br><span class="line"></span><br><span class="line"># Fit the pipeline to the train set</span><br><span class="line">pipeline.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Predict the labels of the test set</span><br><span class="line">y_pred = pipeline.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Compute metrics</span><br><span class="line">print(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h2 id="Centering-and-scaling"><a href="#Centering-and-scaling" class="headerlink" title="Centering and scaling"></a>Centering and scaling</h2><p>模型的featuers的数据值，它的取值范围可能是非常大的，比如年龄的区间是0-100，而体重的区间则不一样。对模型来说，我们最好将这些数据进行缩放（scaling），全部放到同一个区间中，这样它们对于模型的重要程度才比较好估计。</p><p>缩放的范围可以是0-1，也可以是-1到1.</p><p>可以查看sklearn的文档，了解更多关于scaling的细节。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/15/AFHezumhwD5grWB.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给机器学习的能力，让它可以根据&lt;strong&gt;数据&lt;/strong&gt;自己做决定的一种技术。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="machine learning" scheme="http://wittyfans.com/tags/machine-learning/"/>
    
      <category term="Python" scheme="http://wittyfans.com/tags/Python/"/>
    
      <category term="sklearn" scheme="http://wittyfans.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>Numpy ndarray 基本操作</title>
    <link href="http://wittyfans.com/coding/NumPy-ndarray-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html"/>
    <id>http://wittyfans.com/coding/NumPy-ndarray-基本操作.html</id>
    <published>2019-07-01T02:45:05.000Z</published>
    <updated>2019-11-15T14:36:42.035Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2019/11/15/qUBCxdZk7VmIFe1.png" alt=""></p><blockquote><p>为什么要使用Numpy？给你两组数据运算，然后对比一下性能就知道了.</p></blockquote><a id="more"></a><h1 id="Why-Numpy"><a href="#Why-Numpy" class="headerlink" title="Why Numpy?"></a>Why Numpy?</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">my_arr = np.arange(1000000)</span><br><span class="line"></span><br><span class="line">my_list = list(range(1000000))</span><br></pre></td></tr></table></figure><p>现在对两组数乘以2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%time for _ in range(10): my_arr2 = my_arr * 2</span><br><span class="line"></span><br><span class="line">CPU times: user 20 ms, sys: 50 ms, total: 70 ms Wall time: 72.4 ms</span><br><span class="line"></span><br><span class="line">%time for _ in range(10): my_list2 = [x * 2 for x in my_list]</span><br><span class="line"></span><br><span class="line">CPU times: user 760 ms, sys: 290 ms, total: 1.05 s</span><br></pre></td></tr></table></figure><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><h3 id="生成随机数"><a href="#生成随机数" class="headerlink" title="生成随机数"></a>生成随机数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">data = np.random.randn(2,3)</span><br></pre></td></tr></table></figure><p>数据长这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.05094946, -1.54555805, -1.19695135],</span><br><span class="line">       [-1.06169454,  1.13763682,  0.57538678]])</span><br></pre></td></tr></table></figure><p>将它们乘10:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[  9.41882893,   3.20674452,  18.05866858],</span><br><span class="line">       [ -7.97835594,  -9.56449228,  -0.83342424]])</span><br></pre></td></tr></table></figure><p>两份数据相加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[-2.37617968,  3.45388874, -0.64218591],</span><br><span class="line">       [-2.99399147, -1.11118452, -1.11992404]])</span><br></pre></td></tr></table></figure><p>对于Numpy的数据：</p><blockquote><p>An ndarray is a generic multidimensional container for homogeneous data; that is, all of the elements must be the same type. Every array has a shape, a tuple indicating the size of each dimension, and a dtype, an object describing the data type of the array:</p></blockquote><ul><li>所有的数据必须是同样的类型</li><li>每个数组都有一个元组类型的shape属性，表示这个数组的维度信息</li><li>每个数组都有一个dtype属性用来描述它其中的数据类型</li></ul><p>如上面的data：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.shape ---&gt; (2, 3)</span><br><span class="line">data.dtype --&gt; dtype(&apos;float64&apos;)</span><br></pre></td></tr></table></figure><blockquote><p>While it’s not necessary to have a deep understanding of NumPy for many data analytical applications, becoming proficient in array-oriented programming and thinking is a key step along the way to becoming a scientific Python guru.</p></blockquote><h2 id="创建-NDarrays"><a href="#创建-NDarrays" class="headerlink" title="创建 NDarrays"></a>创建 NDarrays</h2><p>直接从数组创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data1 = [6,7.5,8,0,1]</span><br><span class="line">arr1 = np.array(data1)</span><br><span class="line">arr1</span><br></pre></td></tr></table></figure><p>也可以从多维数组创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]</span><br><span class="line">arr2 = np.array(data2)</span><br><span class="line"></span><br><span class="line">arr2.shape --&gt; (2,4)</span><br><span class="line">arr2.dim --&gt;2</span><br></pre></td></tr></table></figure><p>可以用 <em>ndim</em> 属性来看数组的维度信息。</p><p>Numpy还有一些有趣的方法，可以直接创建0和1，或者空值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.zeros(10) --&gt; 创建10个0的数组</span><br><span class="line">np.zeros((3,6))</span><br><span class="line">np.empty((2,3,2)) --&gt; 创建两个两列三行的数组</span><br></pre></td></tr></table></figure><p>还有一些创建<em>ndarrays</em>的方法：</p><ul><li>array: Convert input data (list, tuple, array, or other sequence type) to an ndarray either by inferring a dtype</li><li>asarray: Convert input to ndarray, but do not copy if the input is already an ndarray</li><li>arange: Like the built-in range but returns an ndarray instead of a list</li></ul><p>更多的方法可以参考：<em>Python for data analyse, Table 4-1</em></p><h2 id="NDarrays的数据类型"><a href="#NDarrays的数据类型" class="headerlink" title="NDarrays的数据类型"></a>NDarrays的数据类型</h2><p>ndarrays 的data type或者是dtype包含了一些基本的信息(meta),array在定义的时候是可以指定数据类型的，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr1 = np.array([1,2,3],dtype=np.float64)</span><br><span class="line">arr2 = np.array([1,2,3],dtype=np.int32)</span><br></pre></td></tr></table></figure><p>数据类型可以相互转化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([1,2,3,4])</span><br><span class="line">arr.dtype</span><br><span class="line"># 输出 dtype(&apos;int64&apos;)</span><br><span class="line"></span><br><span class="line">arr = np.array([1,2,3,4])</span><br><span class="line">floatarr = arr.astype(np.float64)</span><br><span class="line">floatarr.dtype</span><br><span class="line"># 输出 dtype(&apos;float64&apos;)</span><br></pre></td></tr></table></figure><p>相反的<em>float</em>也可以转化成 <em>int</em>,十进制多出来的部分会被四舍五入。</p><h2 id="数组运算"><a href="#数组运算" class="headerlink" title="数组运算"></a>数组运算</h2><p>下面是基本的运算：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[1., 2., 3.], [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>乘：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr*arr</span><br><span class="line"># out</span><br><span class="line">array([[  1.,   4.,   9.],</span><br><span class="line">       [ 16.,  25.,  36.]])</span><br></pre></td></tr></table></figure><p>减:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr-arr</span><br><span class="line"># out</span><br><span class="line">array([[ 0.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  0.]])</span><br></pre></td></tr></table></figure><p>所有的运算都是基于相对关系的，记住这一点即可。除此之外，np还支持比较，假设两个arr对比，返回结果会是一个包含true或false的数组。</p><h2 id="切片和索引"><a href="#切片和索引" class="headerlink" title="切片和索引"></a>切片和索引</h2><p>Numpy的切片和索引和数组的差不多，切片就是按照坐标或者坐标范围来找出对应，或对应范围内的值，根据坐标来理解就很简单</p><p><img src="https://i.loli.net/2019/02/08/5c5d05c2d4a0f.jpeg" alt=""></p><p>你可以对一个切片范围内的值重新赋值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(10)</span><br><span class="line">arr[5] --&gt; 5</span><br><span class="line">arr[5:8] --&gt; [5,6,7]</span><br><span class="line">arr[5:8] = 12</span><br><span class="line">arr --&gt; array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])</span><br></pre></td></tr></table></figure><p>np设计需要处理大量的数据，所以对于数组的操作，都是在原来的数据上改动，不会copy。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">arr_slice = arr[5:8]</span><br><span class="line">arr_slice</span><br><span class="line"># out: array([5, 6, 7])</span><br><span class="line"></span><br><span class="line">arr_slice[:] = 9 # [:]是应用在数组中的所有元素</span><br><span class="line">arr</span><br><span class="line"># out: array([0, 1, 2, 3, 4, 9, 9, 9, 8, 9])</span><br></pre></td></tr></table></figure><p>如果你要copy，np提供了一个copy函数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])</span><br><span class="line">copyed = arr3d.copy()</span><br><span class="line">copyed</span><br><span class="line"># out:</span><br><span class="line">array([[[ 1,  2,  3],</span><br><span class="line">        [ 4,  5,  6]],</span><br><span class="line"></span><br><span class="line">       [[ 7,  8,  9],</span><br><span class="line">        [10, 11, 12]]])</span><br></pre></td></tr></table></figure><p>可以在两个维度上切片：</p><p><em>arr2d[:]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[1, 2, 3],</span><br><span class="line">       [4, 5, 6],</span><br><span class="line">       [7, 8, 9]])</span><br></pre></td></tr></table></figure><p><em>arr2d[:2]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[1, 2, 3],</span><br><span class="line">       [4, 5, 6]])</span><br></pre></td></tr></table></figure><p><em>arr2d[:2,1:]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[2, 3],</span><br><span class="line">       [5, 6]])</span><br></pre></td></tr></table></figure><p><em>arr2d[:,:0]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[1],</span><br><span class="line">       [4],</span><br><span class="line">       [7]])</span><br></pre></td></tr></table></figure><p>参照下图，动手实践几次，就会懂其中的套路了。</p><p><img src="https://i.loli.net/2019/02/08/5c5d054478e7b.jpeg" alt=""></p><h2 id="Boolean-Indexing"><a href="#Boolean-Indexing" class="headerlink" title="Boolean Indexing"></a>Boolean Indexing</h2><p>我们有一批名字:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">names = np.array([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;, &apos;Bob&apos;, &apos;Will&apos;, &apos;Joe&apos;, &apos;Joe&apos;])</span><br></pre></td></tr></table></figure><p>我们可以直接通过 <em>names == ‘Bob’</em> 来返回一个检查结果，这个结果包含的是一个 <em>bollean</em> 的 <em>list</em>.</p><p><em>names == ‘Bob’</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ True, False, False,  True, False, False, False], dtype=bool)</span><br></pre></td></tr></table></figure><p>如果我们有一份数据，也是7行，那么我们可以吧这个包含 <em>True</em> 和 <em>False</em> 的l <em>List</em> 传进去，这样 <em>Numpy</em> 会选出那些对应 <em>True</em> 的行。</p><p><em>data = np.random.randn(7,4)</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.71030181, -0.14900916, -1.15238417, -0.49395683],</span><br><span class="line">       [-0.92601472,  0.88452947, -0.9206763 , -0.43338155],</span><br><span class="line">       [-0.68093622,  0.93612942,  0.03261537,  1.44615091],</span><br><span class="line">       [ 1.40919226, -0.07214425, -0.07973205, -1.01432059],</span><br><span class="line">       [-0.4042085 ,  0.66812768,  0.4715137 ,  0.34981598],</span><br><span class="line">       [ 0.89631112, -0.70534677,  0.44560626,  0.6133761 ],</span><br><span class="line">       [-0.28979691,  0.58481489, -0.06945283, -0.99545537]])</span><br></pre></td></tr></table></figure><p><em>data[names==’Bob’]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.71030181, -0.14900916, -1.15238417, -0.49395683],</span><br><span class="line">       [ 1.40919226, -0.07214425, -0.07973205, -1.01432059]])</span><br></pre></td></tr></table></figure><blockquote><p> Boolean selection will not fail if the boolean array is not the correct length, so I recommend care when using this feature.</p></blockquote><p>上面的选择，也可以配合切片：</p><p><em>data[names==’Bob’,:1]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.71030181],</span><br><span class="line">       [ 1.40919226]])</span><br></pre></td></tr></table></figure><p>选择除了 <em>Bob</em> 之外的名字：</p><ul><li><em>names != ‘Bob’</em></li><li><em>~(names == ‘Bob’)</em></li></ul><p>对于 <em>names</em> 的过滤，可以用组合条件：</p><ul><li><em>cond = names == ‘Bob’</em></li><li><em>cond = (names==’Bob’) | (names == ‘will’)</em></li></ul><p>对于 <em>data</em> 也一样：</p><p><em>data[data &lt; 0] = 0</em></p><p>设置整行的值也非常简单：</p><p><em>data[names != ‘Joe’] = 7</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[ 7.        ,  7.        ,  7.        ,  7.        ],</span><br><span class="line">       [-0.92601472,  0.88452947, -0.9206763 , -0.43338155],</span><br><span class="line">       [ 7.        ,  7.        ,  7.        ,  7.        ],</span><br><span class="line">       [ 7.        ,  7.        ,  7.        ,  7.        ],</span><br><span class="line">       [ 7.        ,  7.        ,  7.        ,  7.        ],</span><br><span class="line">       [ 0.89631112, -0.70534677,  0.44560626,  0.6133761 ],</span><br><span class="line">       [-0.28979691,  0.58481489, -0.06945283, -0.99545537]])</span><br></pre></td></tr></table></figure><h2 id="Fancy-Indexing"><a href="#Fancy-Indexing" class="headerlink" title="Fancy Indexing"></a>Fancy Indexing</h2><p>Fancy indexing is a term adopted by NumPy to describe indexing using integer arrays. Suppose we had an 8 × 4 array:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.empty((8,4),dtype=np.int)</span><br><span class="line">for i in range(8):</span><br><span class="line">    arr[i] = i</span><br><span class="line">arr</span><br></pre></td></tr></table></figure><p>选择单个值：</p><p><em>arr[3,0]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out: 3</span><br></pre></td></tr></table></figure><p>选择多行：</p><p><em>arr[[3,0]]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[3, 3, 3, 3],</span><br><span class="line">       [0, 0, 0, 0]])</span><br></pre></td></tr></table></figure><p>让我们构建一个按顺序排列的 <em>8x4</em> 的数组：</p><p><em>arr = np.arange(32).reshape((8,4))</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  1,  2,  3],</span><br><span class="line">       [ 4,  5,  6,  7],</span><br><span class="line">       [ 8,  9, 10, 11],</span><br><span class="line">       [12, 13, 14, 15],</span><br><span class="line">       [16, 17, 18, 19],</span><br><span class="line">       [20, 21, 22, 23],</span><br><span class="line">       [24, 25, 26, 27],</span><br><span class="line">       [28, 29, 30, 31]])</span><br></pre></td></tr></table></figure><p>按选择前两个子数组的第一个数：</p><p><em>arr[[1,2],[0,0]]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4, 8])</span><br></pre></td></tr></table></figure><p>你也可以对选择出来的数组，进行排序：</p><p><em>arr[[1,2]][:]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4,  5,  6,  7],</span><br><span class="line">       [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><p><em>arr[[1,2]][:,[0]]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[4],</span><br><span class="line">       [8]])</span><br></pre></td></tr></table></figure><p><em>arr[[1,2]][:,[0,3,2,1]]</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 4,  7,  6,  5],</span><br><span class="line">       [ 8, 11, 10,  9]])</span><br></pre></td></tr></table></figure><blockquote><p>Fancy indexing always copies the data into a new array.</p></blockquote><h2 id="Transposing-Arrays-and-Swapping-Axes"><a href="#Transposing-Arrays-and-Swapping-Axes" class="headerlink" title="Transposing Arrays and Swapping Axes"></a>Transposing Arrays and Swapping Axes</h2><h3 id="Shape"><a href="#Shape" class="headerlink" title="Shape"></a>Shape</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">value = 18</span><br><span class="line">x = 2</span><br><span class="line">y = 9</span><br><span class="line">arr = np.arange(value).reshape((x,y))</span><br><span class="line">arr</span><br><span class="line"># 输出</span><br><span class="line">array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],</span><br><span class="line">       [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">value = 18</span><br><span class="line">x = 3</span><br><span class="line">y = 6</span><br><span class="line">arr = np.arange(value).reshape((x,y))</span><br><span class="line"># 输出</span><br><span class="line">array([[ 0,  1,  2,  3,  4,  5],</span><br><span class="line">       [ 6,  7,  8,  9, 10, 11],</span><br><span class="line">       [12, 13, 14, 15, 16, 17]])</span><br></pre></td></tr></table></figure><p>只需要确保 Value = X <em> Y 就可以任意 </em>shape* 了。</p><h3 id="Transposing"><a href="#Transposing" class="headerlink" title="Transposing"></a>Transposing</h3><p>看例子：</p><p><em>arr = np.arange(18).reshape((3,6))</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  1,  2,  3,  4,  5],</span><br><span class="line">       [ 6,  7,  8,  9, 10, 11],</span><br><span class="line">       [12, 13, 14, 15, 16, 17]])</span><br></pre></td></tr></table></figure><p><em>arr.T</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  6, 12],</span><br><span class="line">       [ 1,  7, 13],</span><br><span class="line">       [ 2,  8, 14],</span><br><span class="line">       [ 3,  9, 15],</span><br><span class="line">       [ 4, 10, 16],</span><br><span class="line">       [ 5, 11, 17]])</span><br></pre></td></tr></table></figure><p><em>Transposing</em> 在矩阵计算中用的非常多，比如用 <em>np.dot</em> 方法计算矩阵的内积:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arra = np.array([2,3,0])</span><br><span class="line">arrb = np.array([2,-1,1])</span><br><span class="line">np.dot(arra,arrb)</span><br><span class="line"># 输出 1</span><br></pre></td></tr></table></figure><blockquote><p>怎么计算内积？看下图就明白了</p></blockquote><p><img src="https://i.loli.net/2019/02/08/5c5d41ff41077.png" alt=""></p><p><img src="https://i.loli.net/2019/02/08/5c5d4246ea899.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/11/15/qUBCxdZk7VmIFe1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么要使用Numpy？给你两组数据运算，然后对比一下性能就知道了.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="python" scheme="http://wittyfans.com/tags/python/"/>
    
      <category term="data analysis" scheme="http://wittyfans.com/tags/data-analysis/"/>
    
      <category term="NumPy" scheme="http://wittyfans.com/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>Linux 常用命令整理(eng)</title>
    <link href="http://wittyfans.com/writing/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86-eng.html"/>
    <id>http://wittyfans.com/writing/Linux-常用命令整理-eng.html</id>
    <published>2019-06-07T13:56:19.000Z</published>
    <updated>2019-06-07T13:59:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>常用命令的cheatsheet，如果想了解更多理论部分，请参考之前的文章<a href="http://wittyfans.com/coding/Linux阅读笔记.html">Linux阅读笔记</a>。</p></blockquote><a id="more"></a><h1 id="Linux-Mac-Commands"><a href="#Linux-Mac-Commands" class="headerlink" title="Linux / Mac Commands"></a>Linux / Mac Commands</h1><h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><ul><li>cd</li><li>pwd</li><li>ls</li><li>ls -a</li><li>ls -l </li><li>ls -la</li><li>cd .. </li><li>ls sub_dir/</li><li>cd ~</li></ul><h1 id="Files"><a href="#Files" class="headerlink" title="Files"></a>Files</h1><ul><li>mkdir</li><li>touch</li><li>cp, copy a file</li><li>cp -r, copy a directory</li><li>mv, rename or moving a file</li><li>open</li><li>rm</li><li>rm -rf, force removing a directory</li><li>man(-r), show some info</li><li>about, info</li></ul><h1 id="Find"><a href="#Find" class="headerlink" title="Find"></a>Find</h1><h2 id="help"><a href="#help" class="headerlink" title="help"></a>help</h2><ul><li>man find</li></ul><h2 id="by-name"><a href="#by-name" class="headerlink" title="by name"></a>by name</h2><ul><li>find ., list all file and folder below current</li><li>find folder</li><li>find . -type d, find all foldre, no file</li><li>find . -type f, find all file , no foldre</li><li>find . -type f -name “test.txt”, name as text txt file</li><li>find . -type f -name “text*”, name as txt all file</li><li>find . -type f -iname “text*”, 不区分大小写</li><li>find . -type f -name “*.py”</li></ul><h2 id="by-time"><a href="#by-time" class="headerlink" title="by time"></a>by time</h2><ul><li>find . -type f -mmin -10,过去十分钟修改过的文件</li><li>find . -type f -mmin +10</li><li>find . -type f -mmin +1 -mmin -5</li><li>find . -type f -mtime -20</li></ul><blockquote><p>amin,atime: access min and access day; cmin,ctime: change min and change day; mmin,mtime: modify;</p></blockquote><h2 id="by-size"><a href="#by-size" class="headerlink" title="by size"></a>by size</h2><ul><li><p>find . -size +5m</p><blockquote><p>k,g is work too</p></blockquote></li><li><p>ls -lah ./folders, info about sub folder and files,including size</p></li><li>find . -empty</li></ul><h2 id="permission"><a href="#permission" class="headerlink" title="permission"></a>permission</h2><ul><li>find. -perm 777, read, write, and excute</li><li>find folder -exec chown coreschafer:www-data {} +</li></ul><blockquote><p>find folder, will return all folder, -exec will run the command in that results, {} palceholder, + end of the command.</p></blockquote><ul><li>find folder -type f -exec chmod 664 {} +</li><li>find folder -perm 664</li><li>find . -type f -name “*.jpg”</li><li>find . -type f -name “*.jpg” -maxdepth 1, searched 1 level down</li><li>find . -type f -name “*.jpg” -maxdepth 1 -exec rm {} +, delete serched files</li></ul><h1 id="Grep"><a href="#Grep" class="headerlink" title="Grep"></a>Grep</h1><h2 id="Grep-single-file"><a href="#Grep-single-file" class="headerlink" title="Grep single file"></a>Grep single file</h2><blockquote><p>searched text</p></blockquote><ul><li>grep “text_you_want_search” filename.txt</li><li>grep -w “text_you_want_search” filename.txt, have to match all words</li><li>grep -wi “text_you_want_search” filename.txt, igore the lowcase and uppearcse.</li><li>grep -win “text_you_want_search” filename.txt, get info about the line number</li><li>grep -win -B 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, behind</li><li>grep -win -A 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, ahead</li><li>grep -win -C 4 “text_you_want_search” filename.txt, return the context about the searched words, 4 line, two line before and two behind.</li></ul><h2 id="Grep-multi-file"><a href="#Grep-multi-file" class="headerlink" title="Grep multi file"></a>Grep multi file</h2><ul><li>grep -win “text_” ./*, all file</li><li>grep -win “text_” ./*.txt, txt file</li><li>grep -winr “text” ./ , search all subdir</li><li>grep -wirl “text” ./ , no need match info, just file list</li><li>grep -wirc “text” ./ , show matched number in eatch file</li></ul><h2 id="Grep-command-history"><a href="#Grep-command-history" class="headerlink" title="Grep command history"></a>Grep command history</h2><ul><li>history | grep “git commit”</li><li>history | grep “git commit” | grep “dotfile”</li></ul><h2 id="Grep-rgx"><a href="#Grep-rgx" class="headerlink" title="Grep rgx"></a>Grep rgx</h2><ul><li>grep -P “\d{3}-\d{3}-\d{4}” file.txt, work well in linux, mac need to config, I configed</li></ul><h1 id="cURL"><a href="#cURL" class="headerlink" title="cURL"></a>cURL</h1><h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><ul><li>curl url</li><li>curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a></li><li>curl http:***/json_file<ul><li>curl -i http:***/json_file, details info about the get</li></ul></li><li>curl http:***/method<ul><li>curl -d “first=name&amp;last=lastname” http:***/method, d for data, Post request</li><li>curl -X PUT -d “first=name&amp;last=lastname” http:***/method, d for data, Pust request</li><li>curl -X DELETE http:***/method, delete request</li></ul></li></ul><h2 id="Verify"><a href="#Verify" class="headerlink" title="Verify"></a>Verify</h2><blockquote><p>could not verify your access ?</p></blockquote><ul><li>curl -u username:password http://***, Auth</li></ul><h2 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h2><blockquote><p>Download file</p></blockquote><ul><li>curl http://***/folder, return binary file , error</li><li>curl -o filename.jpg http://***/folder, sucess</li></ul><blockquote><p>Saving large json file</p></blockquote><ul><li>curl -o file_name.json http:/.api.the_url*</li></ul><h1 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a>rsync</h1><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><blockquote><p>aviable in Mac, debian-based linux need to install</p></blockquote><ul><li>apt-get install rsync</li><li>yum install rsync</li></ul><h2 id="Use"><a href="#Use" class="headerlink" title="Use"></a>Use</h2><ul><li>rsync folder1/* backup/ , sync fils to backup folder,will skping the subfolder’s file, but affected subfolder</li><li>rsync -r folder1/* backup/ , including subfolder’s file</li><li>rsync -r folder1 backup/, sync folder, not content in it</li></ul><h2 id="Check-chage-before-run"><a href="#Check-chage-before-run" class="headerlink" title="Check chage before run"></a>Check chage before run</h2><ul><li>rsync -a –dry-run folder1/* backup/, check before the command run, now view showed<ul><li>rsync -av –dry-run folder1/* backup/, auto view</li></ul></li></ul><h2 id="Source-folder-has-new-file"><a href="#Source-folder-has-new-file" class="headerlink" title="Source_folder has new file"></a>Source_folder has new file</h2><ul><li>rsync -av –delete –dry-run original/ backup/, check, be careful !</li></ul><h2 id="Do-it-in-local-and-host"><a href="#Do-it-in-local-and-host" class="headerlink" title="Do it in local and host"></a>Do it in local and host</h2><ul><li>rsync -zaP -p local_folder username@ip:~/public/, z for compress, a for all, P for tarnsfer in internet</li><li>rsync 0zaP username@ip:~/public/file ~/Downloads/, revers</li></ul><h1 id="Cron"><a href="#Cron" class="headerlink" title="Cron"></a>Cron</h1><ul><li>crontab -l, list the crons</li></ul><blockquote><p>set your editor to nano, default vim</p></blockquote><ul><li><p>export EDITOR=/user/bin/nano</p></li><li><p>crontab -e, open editor</p><ul><li>press i to input</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Algorhythm for schuel you task with time</span><br><span class="line"></span><br><span class="line"># ┌───────────── minute (0 - 59)</span><br><span class="line"># │ ┌───────────── hour (0 - 23)</span><br><span class="line"># │ │ ┌───────────── day of month (1 - 31)</span><br><span class="line"># │ │ │ ┌───────────── month (1 - 12)</span><br><span class="line"># │ │ │ │ ┌───────────── day of week (0 - 6) (Sunday to Saturday;</span><br><span class="line"># │ │ │ │ │                                       7 is also Sunday on some systems)</span><br><span class="line"># │ │ │ │ │</span><br><span class="line"># │ │ │ │ │</span><br><span class="line"># * * * * *  command_to_execute</span><br></pre></td></tr></table></figure><ul><li><a href="https://youtu.be/QZJ1drMQz1A" target="_blank" rel="noopener">More in</a></li><li><a href="https://github.com/CoreyMSchafer/code_snippets/blob/master/Cron-Tasks/snippets.txt" target="_blank" rel="noopener">snippets</a></li><li><a href="https://crontab.guru/" target="_blank" rel="noopener">tools to make a cron job</a></li></ul><h1 id="Profile"><a href="#Profile" class="headerlink" title="Profile"></a>Profile</h1><ul><li>PS1=”customer_namae”; , change the name, temporary</li><li>ls -la, show dot file, find .bash_profile</li></ul><h2 id="backup"><a href="#backup" class="headerlink" title="backup"></a>backup</h2><ol><li>mv .bash_profile backup_folder</li><li>mv .bashrc back_folder</li></ol><h2 id="created"><a href="#created" class="headerlink" title="created"></a>created</h2><ol><li>cd !</li><li>touch .bash_profile</li><li>touch .bashrc</li><li>subl .bash_profile -&gt; echo “Hello from bash_profile”, subl for sublime</li><li>subl .bashrc -&gt; echo “Hello from bashrc”</li></ol><blockquote><p>bash_profile, run every time you open terminal, or ssh login to some machine. when you input bash, you will get the print from bashrc.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [-f ~/.bashrc]; then</span><br><span class="line">source ~/.bashrc</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="config-terminal-view"><a href="#config-terminal-view" class="headerlink" title="config terminal view"></a>config terminal view</h2><p>You can add the keywords in below list to the ps1, to show some extra info.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># config display name</span><br><span class="line">ps1 = &quot;-&gt;&quot;; export PS1</span><br><span class="line"></span><br><span class="line"># if you config command is too long, you can += it</span><br><span class="line">ps1 += &quot;some_other_config&quot;</span><br><span class="line"></span><br><span class="line"># add the infomation of you computer, or time</span><br><span class="line"></span><br><span class="line">\u: usernmae</span><br><span class="line">\h: hostname</span><br><span class="line">\n: new line</span><br><span class="line">\t: the current time</span><br><span class="line">\w: current working directory</span><br><span class="line">\W: base name of current working directory</span><br></pre></td></tr></table></figure><p>In shell, you can print with {}, like :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># This will print the pwd&apos;s output in shell.</span><br><span class="line">You current diretory is $(pwd)</span><br><span class="line"></span><br><span class="line"># change color, no stop, all font will set to orange</span><br><span class="line">echo &quot;$(tput setaf 166) This is orange&quot;</span><br><span class="line"></span><br><span class="line"># set a stop of the change color command</span><br><span class="line">echo &quot;$(tput setaf 166) This is orange $(tput sgr0)&quot;</span><br></pre></td></tr></table></figure><h1 id="Mac-OS-Keyboard-Shortcuts"><a href="#Mac-OS-Keyboard-Shortcuts" class="headerlink" title="Mac OS Keyboard Shortcuts"></a>Mac OS Keyboard Shortcuts</h1><ul><li>using arrow key to view the history command</li><li>ctrl + a, jump to the begining of line</li><li>ctrl + e, jump to the end</li><li>option + left/rithg, jump by words</li><li>holding option key, move mourse to the point you want to go</li><li>ctrl + u, delete code before the point</li><li>ctrl + k, delete everything after the cursor</li><li>!fi, re run the command in history that begins with fi</li><li>ctrl+ r, search the history command that you inputed</li><li>ctrl + l, clean the screen, without clean the history output</li><li>ctrl + k, clearn the screen ,including the history output</li></ul><h1 id="Aliases"><a href="#Aliases" class="headerlink" title="Aliases"></a>Aliases</h1><ol><li>sudo vim .bash_profile</li><li>alias dt = ‘cd !/Desktop/‘</li><li>killall Finder</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bash_profile</span><br><span class="line">alias testhost=&apos;ssh root@8.8.8.8 -p 22&apos;</span><br></pre></td></tr></table></figure><h1 id="generate-key"><a href="#generate-key" class="headerlink" title="generate key"></a>generate key</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 生成本地秘钥，一路回车即可</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"># 上传生成的秘钥到服务器</span><br><span class="line">scp ~/.ssh/id_rsa.pub testhost:~/.ssh/</span><br><span class="line"># 使用以下命令将 id_rsa.pub 更名为 authorized_keys</span><br><span class="line">mv id_rsa.pub authorized_keys</span><br><span class="line"># 修改权限</span><br><span class="line">chmod 700 ~/.ssh/</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;常用命令的cheatsheet，如果想了解更多理论部分，请参考之前的文章&lt;a href=&quot;http://wittyfans.com/coding/Linux阅读笔记.html&quot;&gt;Linux阅读笔记&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Excel笔记P1</title>
    <link href="http://wittyfans.com/coding/Excel%E7%AC%94%E8%AE%B0P1.html"/>
    <id>http://wittyfans.com/coding/Excel笔记P1.html</id>
    <published>2019-06-07T13:25:55.000Z</published>
    <updated>2019-06-07T13:31:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>主要记录我在YouTube上学习到的Excel知识，没有整理所以顺序比较乱。</p></blockquote><a id="more"></a><h1 id="1-省时的快捷键"><a href="#1-省时的快捷键" class="headerlink" title="1. 省时的快捷键"></a>1. 省时的快捷键</h1><ul><li>alt + pageup/pagedown, 左右移动光标</li><li>ctrl + 右/左, 将光标移动到列尾/列首</li><li>ctrl + 上/下，将光标移动到行首/行尾</li><li>如何根据列名快速找到列？<ol><li>选择cell</li><li>shift + space 选择整行</li><li>ctrl + f </li><li>输入名字搜索</li></ol></li></ul><blockquote><p>我电脑按shift + space 没反应，有冲突？ </p></blockquote><ul><li>F2，快速编辑，在excel中，点击cell不会自动编辑，需要双击，你也可以通过按f2开始快速编辑<ul><li>开始快速编辑后，在右下角会显示edit模式，这时候如果你编辑公式，左右光标会在公式之间移动，如果你想要选择cell作为公式的参数呢？再按一次将edit模式变成enter模式即可</li></ul></li></ul><h1 id="2-高级复制粘贴"><a href="#2-高级复制粘贴" class="headerlink" title="2. 高级复制粘贴"></a>2. 高级复制粘贴</h1><h2 id="拖动粘贴"><a href="#拖动粘贴" class="headerlink" title="拖动粘贴"></a>拖动粘贴</h2><ol><li>选中值</li><li>移动光标至框边缘出现四向箭头</li><li>右键鼠标按住不放</li><li>拖动到其它位置</li><li>松开选择粘贴类型</li></ol><h2 id="粘贴选择多格式"><a href="#粘贴选择多格式" class="headerlink" title="粘贴选择多格式"></a>粘贴选择多格式</h2><ol><li>正常复制</li><li>按 ALT+E+S,打开粘贴格式面板（Mac使用contrl+command+v）<ol><li>自定义粘贴</li><li>比如按v -&gt; 回车，只粘贴值</li></ol></li></ol><p>如果你的键盘有menu键，可以之间使用memu+v粘贴值。</p><h1 id="3-在两个表中使用VLookup-Hlookup函数"><a href="#3-在两个表中使用VLookup-Hlookup函数" class="headerlink" title="3. 在两个表中使用VLookup/Hlookup函数"></a>3. 在两个表中使用VLookup/Hlookup函数</h1><p>如果你表的header在上面，使用vlookup函数，如果你的header在左边，那么hlookup也许更适合你。</p><h2 id="Vertical-Look-UP"><a href="#Vertical-Look-UP" class="headerlink" title="Vertical Look UP"></a>Vertical Look UP</h2><blockquote><p>V stands for Vertical look up, not value, as there is a Hlookup = horzizontal；大家可能把vlookup函数理解为value look up，其实它是 vertical lookup</p></blockquote><p>你只能根据左边的列找出右边列的值，而不可以通过根据右边的列来找出左边列的值。</p><p>先将两个表摊开放到一起看，选择review -&gt; Arrange all -&gt; vertical</p><p>使用vlookup函数，它需要四个参数：</p><ol><li>查什么？选中那个cell</li><li>去哪里查，选中整个范围</li><li>查所选数据中的第几列，需要注意，excel从1开始编号</li><li>精确匹配还是近似匹配（近似匹配：查a，那么abc也会被返回）</li></ol><p>按回车，就可以成功的找出你需要的值。</p><h2 id="Horzizontal-Look-UP"><a href="#Horzizontal-Look-UP" class="headerlink" title="Horzizontal Look UP"></a>Horzizontal Look UP</h2><p>使用hlookup函数，它需要四个参数：</p><ol><li>查什么？选中那个cell</li><li>去哪里查，选中整个范围</li><li>查所选数据中的第几行，需要注意，excel从1开始编号</li><li>精确匹配还是近似匹配（近似匹配：查a，那么abc也会被返回）</li></ol><h1 id="4-如何把两列值相加"><a href="#4-如何把两列值相加" class="headerlink" title="4. 如何把两列值相加"></a>4. 如何把两列值相加</h1><p>假设两个cell的值类型为文本(Text)，也就是字符串，可以直接将两个值使用 &amp; 结合，如果不是文本，你可以使用Text函数完成，即:Text(值的坐标,0).</p><h1 id="5-透视表"><a href="#5-透视表" class="headerlink" title="5. 透视表"></a>5. 透视表</h1><p>我是在Python中使用过 pivot_table 才知道 pivot 操作的意思，接触 excel 后才知道这叫透视表，有些人也叫这个数据剖析.</p><p>anyway, 今天来记录一下透视表的使用。</p><p>我使用的是 excel 2016 英文版，微软的 excel 界面不会差太多，打开excel你的界面应该和我差不多。</p><p>首先你需要打开你的表，然后选择你需要透视的数据：</p><p><img src="https://i.loli.net/2019/06/04/5cf65f311a5c943124.png" alt="pivot_table0.png"></p><h1 id="6-我只要透视后的排名前十的数据"><a href="#6-我只要透视后的排名前十的数据" class="headerlink" title="6. 我只要透视后的排名前十的数据"></a>6. 我只要透视后的排名前十的数据</h1><p>点击 cell 旁边的 sort 按钮，点击 value filter, 选择最下面的 top 10， 当然你也可以在这里做其他的filter操作。</p><h1 id="7-安装-excel-插件"><a href="#7-安装-excel-插件" class="headerlink" title="7 安装 excel 插件"></a>7 安装 excel 插件</h1><p> excel的插件是 xlam 格式的文件，你不可以直接双击打开，需要打开excel.</p><p> file -&gt; add-ins -&gt; manage -&gt; go</p><p> 这时候可以看到你安装好了的和没安装的插件，然后点击 browser，找到你的插件，点击 ok 即可。</p><p> 插件会每次都随着excel的打开而启动，也许有时候你并不需要这样，你可以在 Developer, add-ins 里面 把这个插件的选项 unchec。</p><h1 id="8-完全卸载excel插件"><a href="#8-完全卸载excel插件" class="headerlink" title="8. 完全卸载excel插件"></a>8. 完全卸载excel插件</h1><ol><li>取消选择插件。</li><li>file -&gt; options -&gt; add-ins</li><li>选择需要卸载的插件，找到路径</li><li>打开该路径，删除该插件</li><li>打开选择插件按钮，重新选择，excel会提示无法找到，是否删除，点击确定</li></ol><h1 id="9-插件不见了？重新加载插件"><a href="#9-插件不见了？重新加载插件" class="headerlink" title="9. 插件不见了？重新加载插件"></a>9. 插件不见了？重新加载插件</h1><ol><li>file - options - add-ins</li><li>manage -&gt; disabled items -&gt; go</li></ol><h1 id="10-做一个drop-down-memu（下拉菜单）"><a href="#10-做一个drop-down-memu（下拉菜单）" class="headerlink" title="10. 做一个drop down memu（下拉菜单）"></a>10. 做一个drop down memu（下拉菜单）</h1><p>下拉菜单可以让你在填写cell中的值的时候，有一些选择，不用手动填写也避免了错误的输入，发现一个插件 List Search（免费），可以很简单的创建下拉菜单。</p><h1 id="11-做一个-Table-of-contents-Gallery"><a href="#11-做一个-Table-of-contents-Gallery" class="headerlink" title="11. 做一个 Table of contents Gallery"></a>11. 做一个 Table of contents Gallery</h1><p>我也不知道如何翻译这个词，反正就是把你所有做好的图，整齐的放在第一页做一个预览图，点击后就可以跳转到子页面，类似这样：</p><p><img src="https://i.loli.net/2019/06/04/5cf6780d097a640476.jpeg" alt="56D77E73-AE1E-4F47-853D-55DEFD291D38.jpeg"></p><p>Tab Hound, 需要收费，有兴趣的可以找到这个文章研究一下宏的用法。</p><h1 id="12-过滤器"><a href="#12-过滤器" class="headerlink" title="12. 过滤器"></a>12. 过滤器</h1><p>点击任意的cell，ctrl+shift+L, 启动/关闭过滤器。</p><p>在过滤器中，你可以按照一列中的值去过滤整个表（不是删除，只是隐藏），你也可以在里面排序。</p><p>推荐数据转换成 Table 分析，这样配合过滤器使用，在你插入行和列的时候，exel会自动将你的操作与filter关联起来，利用公式求值的时候，table会自动填充表中其他row的值。</p><p>在table中，你也可以轻松的添加 total 的行，excel会自动帮你计算（不包括隐藏的值），可以选择求和，求平均等操作。</p><p>table还可以对同一个sheet中的两个table一起过滤。</p><p>一些常用的filter快捷键：</p><ul><li>alt + 下箭头，就可以快速调出filter的菜单</li><li>alt + 下 + e, 调出filter菜单然后开始输入文字搜索</li><li>alt + 下 + c, 清除当前列的 filter条件</li><li>shfit + alt + 下，在任意cell调出filter菜单</li><li>alt + 下 + f + e + enter, 快速找出为空的行</li></ul><p>最舒服的是根据当前选择的值进行筛选:</p><ul><li>右键-&gt; filter -&gt; filter by selected cell’s value</li><li>快捷键：menu key + E + V (Windows)</li></ul><h2 id="多列过滤"><a href="#多列过滤" class="headerlink" title="多列过滤"></a>多列过滤</h2><p>过滤器默认是只针对这些看得见的数据的，所以可以很简单的进行多列过滤。你可想象，每次进行一个过滤器的应用，都是给你看得见的结果加了一个条件，在编程的世界里，也就是进行了and操作。</p><p>但如果你想要or操作呢？</p><p>你可以借助or公式来达到这个目的：</p><p><img src="https://i.loli.net/2019/06/04/5cf68eafbf80710954.jpeg" alt="or"></p><p>你可以直接将鼠标移动到filter button上，查看当前的条件，而不需要进去菜单。</p><h2 id="重复值过滤"><a href="#重复值过滤" class="headerlink" title="重复值过滤"></a>重复值过滤</h2><p>你想要找某一列中重复的值，然后把他们放在一起对比。</p><ol><li>选择列</li><li>conditional formating, highlight cell</li><li>筛选，sort</li></ol><h2 id="根据已有的list过滤"><a href="#根据已有的list过滤" class="headerlink" title="根据已有的list过滤"></a>根据已有的list过滤</h2><ol><li>选择列</li><li>打开filter菜单</li><li>搜索到你需要的关键字</li><li>选择第二个，同时加上“添加”选项</li></ol><h1 id="13-Power-Query"><a href="#13-Power-Query" class="headerlink" title="13. Power Query"></a>13. Power Query</h1><h2 id="Tidy-Data"><a href="#Tidy-Data" class="headerlink" title="Tidy Data"></a>Tidy Data</h2><p>在利用Python Pandas分析数据的时候，我们最开始一般都需要将数据转化成容易分析的格式，也就是大家所说的Tiday Data, 如果你不知tidy data的定义，那么可以参考<a href="https://vita.had.co.nz/papers/tidy-data.pdf" target="_blank" rel="noopener">这篇</a>文章。</p><p>那么在Exce中如何进行这样的操作呢？今天发现可以利用 Power Query 实现这个需求。</p><p>如果你的Excel版本是2016，那么Power Query就已经内置在其中了，如果是比较早的版本，那么可以在<a href="https://www.microsoft.com/en-us/download/confirmation.aspx?id=39379" target="_blank" rel="noopener">这个</a>页面下载到Power Query。</p><p>同时微软也提供了 Power Query 的<a href="https://support.office.com/en-us/article/power-query-overview-and-learning-ed614c81-4b00-4291-bd3a-55d80767f81d" target="_blank" rel="noopener">教程</a>.</p><p>要 unpivolt 你的数据，</p><ol><li>选择你的数据</li><li>Insert -&gt; Table, 将你的数据转化成Table</li><li>再点击Data -&gt; From Table，进入 Power Query 的编辑界面</li><li>选择你需要操作的列</li><li>点击 Transform -&gt; Unpivot Columns (如果你没看到这个选项，将excel拖宽一点)</li></ol><p>这样就完成了你数据的转化，你还可以对 unpivot 后的数据进行过滤操作。</p><h2 id="Unpivot-Columns"><a href="#Unpivot-Columns" class="headerlink" title="Unpivot Columns"></a>Unpivot Columns</h2><p>假设你的数据是这样的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 a,b</span><br><span class="line">2 b,c</span><br></pre></td></tr></table></figure><p>然后你想要把它们分开，就像这样:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 a</span><br><span class="line">1 b</span><br><span class="line">2 b</span><br><span class="line">2 c</span><br></pre></td></tr></table></figure></p><p>基本操作和上面的是一样的，</p><ol><li>选择data -&gt; from table</li><li>home-&gt; spilt columns -&gt; by delimiter -&gt; ok</li><li>删除多余的列</li><li>transform -&gt; unpivot columns</li></ol><h2 id="根据已有的数据计算百分比"><a href="#根据已有的数据计算百分比" class="headerlink" title="根据已有的数据计算百分比"></a>根据已有的数据计算百分比</h2><ol><li>输入总数</li><li>输入函数开始计算，在这一步需要注意关闭 Generate GetPirovtData选项，方法是:<ol><li>点击需要计算的值</li><li>analyse-&gt; options 去掉Generate GetPirovtData的钩</li><li>输入函数计算值</li><li><strong>control + shift 5</strong> 快速转换成百分比</li></ol></li></ol><h1 id="14-如何根据现有数据绘图"><a href="#14-如何根据现有数据绘图" class="headerlink" title="14. 如何根据现有数据绘图"></a>14. 如何根据现有数据绘图</h1><ol><li>选择你要绘制图的数据</li><li>insert table，选择图类型</li><li>选择labels<ol><li>select data</li><li>edite labels</li><li>选择label的区间</li></ol></li></ol><h1 id="15-如何制作excel选择器"><a href="#15-如何制作excel选择器" class="headerlink" title="15. 如何制作excel选择器"></a>15. 如何制作excel选择器</h1><p>我们在预览数据的时候，可以通过过滤器(Filter)来过滤我们看到的数据，这时候可以在过滤器标签上看到它显示为 Multiple Items, 但是我们并不能很直观的看到它到底选择了什么数据项。</p><p>为了让你所选的项目一目了然，我们可以制作一个多选菜单：</p><ol><li>Add a Slicer</li></ol><p>首先，先将你的数据透视好，然后在你透视操作的基列上面点击，再打开analyse选项卡，插入 Slicer, 选择你需要选择的列，然后你就可以看到 slicer 出现，你可以在这里选择过滤项目。</p><p>但是如何知道你选择了哪些项目呢？也就是说如果你需要在另一个表中使用你选过哪些选项要怎么实现呢？</p><p>你可以copy一份你已经透视过的表（需要注意保留filter），然后在 pivot 选项中，将除了filte的其他值全部删掉，这样你在 slicer中选择的时候，这个list也会实时更新。</p><h1 id="16-excel编程"><a href="#16-excel编程" class="headerlink" title="16. excel编程"></a>16. excel编程</h1><h2 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">function func_name(arg as arg_type) as returned_type</span><br><span class="line">    func_name = caculated_value</span><br><span class="line">end function</span><br><span class="line"></span><br><span class="line">用中文解释一遍：</span><br><span class="line"></span><br><span class="line">function 函数名(参数 as 参数数据类型) as 返回值类型</span><br><span class="line">    函数名= 计算的结果</span><br><span class="line">function</span><br></pre></td></tr></table></figure><ul><li>function 是关键字，用来定义函数</li><li>end function 用来标记函数的结束</li><li>excel 的函数没有类似编程语言中的return关键字，只需要把计算结果赋值给函数名即可完成结果的返回</li></ul><h2 id="参考python语法对比"><a href="#参考python语法对比" class="headerlink" title="参考python语法对比"></a>参考python语法对比</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># python</span><br><span class="line">a = 10</span><br><span class="line"></span><br><span class="line"># excel</span><br><span class="line">dim a as 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># if 语句</span><br><span class="line"># python</span><br><span class="line">if a == b:</span><br><span class="line">    ...</span><br><span class="line">else:</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"># excel</span><br><span class="line"></span><br><span class="line">if a==b then</span><br><span class="line">    ...</span><br><span class="line">else</span><br><span class="line">    ....</span><br><span class="line">end if</span><br></pre></td></tr></table></figure><h1 id="17-锁定某个Cell"><a href="#17-锁定某个Cell" class="headerlink" title="17. 锁定某个Cell"></a>17. 锁定某个Cell</h1><ol><li>选中所有其他的cell，formate -&gt; proctet -&gt; 取消选中</li><li>在sheet name右键，保护该表</li><li>设置密码</li></ol><p>锁定某个cell的好处：</p><p>假设你按照行统计，用扫码枪去扫描自动输入，果excel会自动换行，某一行你想要直接跳过，就可以设置锁定不允许选择。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;主要记录我在YouTube上学习到的Excel知识，没有整理所以顺序比较乱。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="data analyse" scheme="http://wittyfans.com/tags/data-analyse/"/>
    
      <category term="数据分析" scheme="http://wittyfans.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>PowerShell通过http, https, 和ftp下载文件</title>
    <link href="http://wittyfans.com/coding/PowerShell%E9%80%9A%E8%BF%87Http-HTTPS-%E5%92%8CFTp%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6.html"/>
    <id>http://wittyfans.com/coding/PowerShell通过Http-HTTPS-和FTp下载文件.html</id>
    <published>2019-04-08T14:13:44.000Z</published>
    <updated>2019-04-08T14:29:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>利用 <em>PowerShell</em> 发起请求，不仅可以下载文件，还可以利用管道对文件进行解析，这一点比 <em>CMD</em> 命令行和 <em>Linux</em> 下的 <em>wget</em> 还要好用。</p></blockquote><a id="more"></a><h1 id="内网环境下"><a href="#内网环境下" class="headerlink" title="内网环境下"></a>内网环境下</h1><p>如果你工作的环境主要是通过 <em>Server Message Block (SMB)</em> 协议来传输文件，那么可以直接在 <em>powershell</em> 中使用 <em>copy-item</em> 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Copy-Item -Source \\server\share\file -Destination C:\path\</span><br></pre></td></tr></table></figure><p>如果你在公司的内网（域环境）下，那么这个命令挺适合你，如果你是要下载外网或者 <em>Internet</em> 上面的数据，事情就变得复杂一点了。</p><h1 id="Internet"><a href="#Internet" class="headerlink" title="Internet"></a>Internet</h1><h2 id="任意版本-powershell"><a href="#任意版本-powershell" class="headerlink" title="任意版本 powershell"></a>任意版本 <em>powershell</em></h2><p>如果你用的power shell 2.x 的版本，你需要使用 <em>new-object</em> 配合 <em>System.Net.WebClient</em> 来实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$WebClient = New-Object System.Net.WebClient</span><br><span class="line">$WebClient.DownloadFile(&quot;https://www.wittyfans.com/file&quot;,&quot;C:\path\file&quot;)</span><br></pre></td></tr></table></figure><h2 id="powershell-版本3-x-以上"><a href="#powershell-版本3-x-以上" class="headerlink" title="powershell 版本3.x 以上"></a><em>powershell</em> 版本3.x 以上</h2><p>如果是 <em>powershell 3.x</em>, 可以用 <em>Invoke-WebRequest</em>命令。Invoke-WebRequest 和Linux其实还有一些关系。它比 <em>wget</em> 还要好用一些，因为它不仅可以下载，而且可以对文件进行解析。</p><p>使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-WebRequest -Uri &quot;http://www.wittyfans.com&quot; -OutFile &quot;C:\path\file&quot;</span><br></pre></td></tr></table></figure><ul><li>默认会下载这个网页，所以如果你指定的下载路径只是一个文件夹，<em>powershell</em>会提示找不到路径，这时候你需要指定路径加文件名.</li><li>如果你省略本地路径，则<em>powershell</em>会默认使用脚本所在目录的路径</li></ul><p>举例：</p><p>下载<em>sublime</em>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-WebRequest -uri &quot;https://download.sublimetext.com/Sublime%20Text%20Build%203207%20x64%20Setup.exe&quot; -OutFile &quot;C:\Users\wittyfans\Desktop\sublime.exe&quot;</span><br></pre></td></tr></table></figure><p><em>Invoke-WebRequest</em> 默认把下载的东西传输给管道，如果你需要保存文件，必须要指定 <em>outfile</em> 参数。而且你还可以在管道中后面去分析这个文件，如果你传输的是二进制文件，<em>power shell</em>会默认以文本的方式处理，这时候你就没办法分析了，不过你可以增加一个参数，只分析文本内容，你需要这样使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-WebRequest &quot;http://www.wittyfans.com&quot; | Select-Object -ExpandProperty Content | Out-File &quot;file&quot;</span><br></pre></td></tr></table></figure><p>如果你想要保存所有管道中的文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-WebRequest &quot;http://www.wittyfans.com&quot; -OutFile &quot;file&quot; -PassThru | Select-Object -ExpandProperty Content</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>如果你的下载需要验证身份，<em>powershell</em>不会提示你，除非你指定了用户名，此时<em>powershell</em>会提示你输入密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 1</span><br><span class="line">Invoke-WebRequest -Uri https://www.wittyfans.com/ -OutFile C:&quot;\path\file&quot; -Credential &quot;yourUserName&quot;</span><br><span class="line"></span><br><span class="line"># 2</span><br><span class="line">$Credentials = Get-Credential</span><br><span class="line">Invoke-WebRequest -Uri &quot;https://www.wittyfans.com&quot; -OutFile &quot;C:\path\file&quot; -Credential $Credentials</span><br></pre></td></tr></table></figure><p>你可以使用<em>-UseDefaultCredentials</em> 参数来使用当前用户的凭据，这样就可以省略 <em>Credential</em> 参数。</p><p>也可以使用弹窗来要求输入密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$client = new-object System.Net.WebClient</span><br><span class="line">$client.Credentials = Get-Credential</span><br><span class="line">$client.DownloadFile(&quot;http://i.imgur.com/JnphmRt.jpg&quot;,&quot;C:\Users\Fatima Wahab\Desktop\cat.jpg&quot;)</span><br></pre></td></tr></table></figure><p>注意检查资源的路径是对的，如果你的路径是该网站的首页，那么就会出错。</p><p>为了确保安全，建议你使用 <em>https</em> 验证，如果只是基本的验证方式，你的密码可能会被抓包分析出来。</p><p>这种下载的验证方式之适用于那些服务器自己管理凭据的情况，现今很多的公司都是用 <em>content management system (CMS)</em> 来验证用户，这时候你就需要使用<em>powershell</em>填写一些表单再提交,用我写的一个函数来举个例子,这个函数只是验证身份，没有下载的动作:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Function login-in($userName,$userPassWord)&#123;</span><br><span class="line">    # 请求并保存session</span><br><span class="line">    $R=Invoke-WebRequest &quot;the_url&quot; -SessionVariable fb</span><br><span class="line"></span><br><span class="line">    # 填写表单信息</span><br><span class="line">    $Form = $R.Forms[0]</span><br><span class="line">    $Form.Fields[&quot;account&quot;]=$userName</span><br><span class="line">    $Form.Fields[&quot;password&quot;] = $userPassWord</span><br><span class="line">    $Form.Fields[&quot;signIn&quot;] = &quot;Sign+in&quot;</span><br><span class="line"></span><br><span class="line">    # 提交 </span><br><span class="line">    $R=Invoke-WebRequest -Uri (&quot;the_url&quot; + $Form.Action) -WebSession $FB -Method POST -Body $Form.Fields</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果想要安全一些，最好不要使用<em>FTP</em>的方式，建议用<em>SFTP</em> 或者 <em>SCP</em>，但 <em>Invoke-WebRequest</em> 不支持这些协议，你可以安装一些第三方的库来实现，现在已经有相关的库实现了。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.addictivetips.com/windows-tips/how-to-download-a-file-with-a-powershell-command-in-windows-10/" target="_blank" rel="noopener">How To Download A File With A PowerShell Command In Windows 10</a></li><li><a href="https://stackoverflow.com/questions/51225598/downloading-a-file-with-powershell" target="_blank" rel="noopener">StackOverFlow上关于 Downloading a file with powershell 的回答</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;利用 &lt;em&gt;PowerShell&lt;/em&gt; 发起请求，不仅可以下载文件，还可以利用管道对文件进行解析，这一点比 &lt;em&gt;CMD&lt;/em&gt; 命令行和 &lt;em&gt;Linux&lt;/em&gt; 下的 &lt;em&gt;wget&lt;/em&gt; 还要好用。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="coding" scheme="http://wittyfans.com/categories/coding/"/>
    
    
      <category term="helpdesk" scheme="http://wittyfans.com/tags/helpdesk/"/>
    
      <category term="powershell" scheme="http://wittyfans.com/tags/powershell/"/>
    
      <category term="windows" scheme="http://wittyfans.com/tags/windows/"/>
    
  </entry>
  
</feed>
